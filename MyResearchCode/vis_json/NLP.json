{
    "BFSinput": {
        "0": {
            "9": [
                0
            ]
        },
        "1": {
            "0": [
                1
            ],
            "9": [
                0,
                1
            ]
        },
        "2": {
            "0": [
                2,
                4,
                15,
                23
            ],
            "4": [
                2
            ],
            "9": [
                0,
                4,
                15,
                23
            ],
            "15": [
                23
            ],
            "23": [
                2,
                4
            ]
        },
        "3": {},
        "4": {
            "0": [
                4,
                15,
                23
            ],
            "9": [
                0,
                4,
                15,
                23
            ],
            "15": [
                23
            ],
            "23": [
                4
            ]
        },
        "5": {
            "19": [
                5
            ]
        },
        "6": {},
        "7": {},
        "8": {
            "0": [
                8,
                15,
                21,
                23
            ],
            "9": [
                0,
                8,
                15,
                23
            ],
            "15": [
                21,
                23
            ],
            "21": [
                8
            ],
            "23": [
                8,
                21
            ]
        },
        "9": {},
        "10": {
            "0": [
                10,
                15,
                21,
                23
            ],
            "9": [
                0,
                15,
                23
            ],
            "15": [
                21,
                23
            ],
            "21": [
                10
            ],
            "23": [
                21
            ]
        },
        "11": {
            "0": [
                2,
                4,
                8,
                10,
                11,
                12,
                15,
                21,
                23
            ],
            "2": [
                12
            ],
            "4": [
                2
            ],
            "8": [
                12
            ],
            "9": [
                0,
                4,
                8,
                11,
                12,
                15,
                23
            ],
            "10": [
                12
            ],
            "12": [
                11
            ],
            "15": [
                21,
                23
            ],
            "21": [
                8,
                10,
                11,
                12
            ],
            "23": [
                2,
                4,
                8,
                12,
                21
            ]
        },
        "12": {
            "0": [
                2,
                4,
                8,
                10,
                12,
                15,
                21,
                23
            ],
            "2": [
                12
            ],
            "4": [
                2
            ],
            "8": [
                12
            ],
            "9": [
                0,
                4,
                8,
                12,
                15,
                23
            ],
            "10": [
                12
            ],
            "15": [
                21,
                23
            ],
            "21": [
                8,
                10,
                12
            ],
            "23": [
                2,
                4,
                8,
                12,
                21
            ]
        },
        "13": {
            "0": [
                2,
                4,
                8,
                10,
                12,
                15,
                21,
                23
            ],
            "2": [
                12
            ],
            "4": [
                2
            ],
            "8": [
                12
            ],
            "9": [
                0,
                4,
                8,
                12,
                15,
                23
            ],
            "10": [
                12,
                13
            ],
            "12": [
                13
            ],
            "15": [
                21,
                23
            ],
            "21": [
                8,
                10,
                12,
                13
            ],
            "23": [
                2,
                4,
                8,
                12,
                13,
                21
            ]
        },
        "14": {
            "0": [
                2,
                4,
                15,
                23
            ],
            "2": [
                14
            ],
            "4": [
                2
            ],
            "6": [
                14
            ],
            "9": [
                0,
                4,
                15,
                23
            ],
            "15": [
                23
            ],
            "23": [
                2,
                4
            ]
        },
        "15": {
            "0": [
                15
            ],
            "9": [
                0,
                15
            ]
        },
        "16": {},
        "17": {
            "3": [
                17
            ]
        },
        "18": {
            "0": [
                10,
                15,
                18,
                21,
                23
            ],
            "9": [
                0,
                15,
                18,
                23
            ],
            "10": [
                18
            ],
            "15": [
                21,
                23
            ],
            "21": [
                10
            ],
            "23": [
                21
            ],
            "27": [
                18
            ]
        },
        "19": {},
        "20": {
            "0": [
                2,
                4,
                8,
                10,
                12,
                15,
                21,
                23
            ],
            "2": [
                12
            ],
            "4": [
                2
            ],
            "8": [
                12,
                20
            ],
            "9": [
                0,
                4,
                8,
                12,
                15,
                23
            ],
            "10": [
                12
            ],
            "12": [
                20
            ],
            "15": [
                21,
                23
            ],
            "21": [
                8,
                10,
                12,
                20
            ],
            "23": [
                2,
                4,
                8,
                12,
                20,
                21
            ]
        },
        "21": {
            "0": [
                15,
                21,
                23
            ],
            "9": [
                0,
                15,
                23
            ],
            "15": [
                21,
                23
            ],
            "23": [
                21
            ]
        },
        "22": {
            "0": [
                15
            ],
            "9": [
                0,
                15
            ],
            "15": [
                22
            ],
            "19": [
                22
            ]
        },
        "23": {
            "0": [
                15,
                23
            ],
            "9": [
                0,
                15,
                23
            ],
            "15": [
                23
            ]
        },
        "24": {},
        "25": {
            "0": [
                2,
                4,
                8,
                10,
                12,
                15,
                18,
                21,
                23,
                25
            ],
            "2": [
                12
            ],
            "4": [
                2,
                25
            ],
            "8": [
                12
            ],
            "9": [
                0,
                4,
                8,
                12,
                15,
                18,
                23,
                25
            ],
            "10": [
                12,
                13,
                18,
                25
            ],
            "12": [
                13
            ],
            "13": [
                25
            ],
            "15": [
                21,
                23
            ],
            "18": [
                25
            ],
            "21": [
                8,
                10,
                12,
                13,
                25
            ],
            "23": [
                2,
                4,
                8,
                12,
                13,
                21
            ],
            "27": [
                18
            ]
        },
        "26": {
            "0": [
                1,
                26
            ],
            "1": [
                26
            ],
            "9": [
                0,
                1
            ]
        },
        "27": {},
        "28": {},
        "29": {
            "28": [
                29
            ]
        }
    },
    "RemovedCycleLinks": {
        "0": {},
        "1": {},
        "2": {},
        "3": {},
        "4": {},
        "5": {},
        "6": {},
        "7": {},
        "8": {},
        "9": {},
        "10": {},
        "11": {},
        "12": {},
        "13": {},
        "14": {},
        "15": {},
        "16": {},
        "17": {},
        "18": {},
        "19": {},
        "20": {},
        "21": {},
        "22": {},
        "23": {},
        "24": {},
        "25": {},
        "26": {},
        "27": {},
        "28": {},
        "29": {}
    },
    "VideoSequence_ConceptInfo": {
        "0": {
            "5ctbvkAMQO4": [
                9,
                0
            ],
            "BYdtJs6wU5o": [
                9,
                0
            ]
        },
        "1": {
            "5ctbvkAMQO4": [
                9,
                0,
                1
            ],
            "BYdtJs6wU5o": [
                9,
                1,
                0
            ],
            "imPpT2Qo2sk": [
                9,
                1,
                0
            ]
        },
        "2": {
            "5ctbvkAMQO4": [
                9,
                0,
                23,
                2,
                4
            ],
            "8FGHDBwoPVo": [
                9,
                23,
                0,
                2
            ],
            "BYdtJs6wU5o": [
                9,
                15,
                0,
                4
            ],
            "J5IlKj7H8T8": [
                2,
                0,
                23,
                4
            ],
            "fOvTtapxa9c": [
                9,
                15,
                4,
                0
            ],
            "ohM7D21C_8Q": [
                15,
                9
            ]
        },
        "3": {},
        "4": {
            "5ctbvkAMQO4": [
                9,
                0,
                23,
                4
            ],
            "8FGHDBwoPVo": [
                9,
                23,
                0
            ],
            "BYdtJs6wU5o": [
                9,
                15,
                0,
                4
            ],
            "fOvTtapxa9c": [
                9,
                15,
                4,
                0
            ],
            "ohM7D21C_8Q": [
                15,
                9
            ]
        },
        "5": {
            "ReakZVh2Xwk": [
                5,
                19
            ],
            "yGKTphqxR9Q": [
                19
            ]
        },
        "6": {},
        "7": {},
        "8": {
            "5ctbvkAMQO4": [
                9,
                0,
                23,
                8
            ],
            "8FGHDBwoPVo": [
                9,
                23,
                0,
                21
            ],
            "AJVP96tAWxw": [
                9,
                23,
                8,
                21,
                0
            ],
            "BYdtJs6wU5o": [
                9,
                15,
                0
            ],
            "MNvT5JekDpg": [
                15,
                21,
                9,
                0
            ],
            "ohM7D21C_8Q": [
                15,
                9
            ]
        },
        "9": {},
        "10": {
            "5ctbvkAMQO4": [
                9,
                0,
                23
            ],
            "8FGHDBwoPVo": [
                9,
                23,
                0,
                21
            ],
            "BYdtJs6wU5o": [
                9,
                15,
                0
            ],
            "MNvT5JekDpg": [
                10,
                15,
                21,
                9,
                0
            ],
            "ohM7D21C_8Q": [
                15,
                9
            ]
        },
        "11": {
            "5ctbvkAMQO4": [
                9,
                0,
                23,
                11,
                2,
                8,
                4,
                12
            ],
            "8FGHDBwoPVo": [
                9,
                23,
                0,
                2,
                11,
                12,
                21
            ],
            "AJVP96tAWxw": [
                9,
                23,
                8,
                21,
                0,
                4
            ],
            "BYdtJs6wU5o": [
                9,
                15,
                0,
                4
            ],
            "J5IlKj7H8T8": [
                2,
                0,
                23,
                4
            ],
            "MNvT5JekDpg": [
                11,
                10,
                15,
                21,
                9,
                0
            ],
            "fOvTtapxa9c": [
                9,
                15,
                21,
                4,
                0,
                10
            ],
            "ohM7D21C_8Q": [
                15,
                9
            ]
        },
        "12": {
            "5ctbvkAMQO4": [
                9,
                0,
                23,
                2,
                8,
                4,
                12
            ],
            "8FGHDBwoPVo": [
                9,
                23,
                0,
                2,
                12,
                21
            ],
            "AJVP96tAWxw": [
                9,
                23,
                8,
                21,
                0,
                4
            ],
            "BYdtJs6wU5o": [
                9,
                15,
                0,
                4
            ],
            "J5IlKj7H8T8": [
                2,
                0,
                23,
                4
            ],
            "MNvT5JekDpg": [
                10,
                15,
                21,
                9,
                0
            ],
            "fOvTtapxa9c": [
                9,
                15,
                21,
                4,
                0,
                10
            ],
            "ohM7D21C_8Q": [
                15,
                9
            ]
        },
        "13": {
            "5ctbvkAMQO4": [
                9,
                0,
                23,
                2,
                8,
                4,
                12
            ],
            "8FGHDBwoPVo": [
                9,
                23,
                0,
                2,
                12,
                21
            ],
            "AJVP96tAWxw": [
                9,
                23,
                8,
                21,
                0,
                4
            ],
            "BYdtJs6wU5o": [
                9,
                15,
                0,
                4
            ],
            "J5IlKj7H8T8": [
                2,
                0,
                23,
                4
            ],
            "MNvT5JekDpg": [
                10,
                15,
                21,
                9,
                0
            ],
            "fOvTtapxa9c": [
                9,
                13,
                15,
                21,
                4,
                0,
                10
            ],
            "ohM7D21C_8Q": [
                15,
                9
            ]
        },
        "14": {
            "5ctbvkAMQO4": [
                9,
                0,
                23,
                2,
                4
            ],
            "8FGHDBwoPVo": [
                9,
                23,
                0,
                2
            ],
            "BYdtJs6wU5o": [
                9,
                15,
                0,
                4
            ],
            "J5IlKj7H8T8": [
                2,
                6,
                14,
                0,
                23,
                4
            ],
            "fOvTtapxa9c": [
                9,
                15,
                4,
                0
            ],
            "ohM7D21C_8Q": [
                15,
                9
            ]
        },
        "15": {
            "5ctbvkAMQO4": [
                9,
                0
            ],
            "BYdtJs6wU5o": [
                9,
                15,
                0
            ],
            "ohM7D21C_8Q": [
                15,
                9
            ]
        },
        "16": {},
        "17": {
            "UeiUiCRchiU": [
                3,
                17
            ]
        },
        "18": {
            "5ctbvkAMQO4": [
                9,
                0,
                23
            ],
            "8FGHDBwoPVo": [
                9,
                23,
                0,
                21
            ],
            "BYdtJs6wU5o": [
                9,
                15,
                0
            ],
            "MNvT5JekDpg": [
                10,
                15,
                21,
                9,
                0
            ],
            "Sx3Fpw0XCXk": [
                9,
                18,
                0
            ],
            "iGmHnICXDss": [
                9,
                18,
                27,
                0,
                10
            ],
            "ohM7D21C_8Q": [
                15,
                9
            ]
        },
        "19": {},
        "20": {
            "5ctbvkAMQO4": [
                9,
                0,
                23,
                2,
                8,
                4,
                12
            ],
            "8FGHDBwoPVo": [
                9,
                23,
                0,
                2,
                12,
                21
            ],
            "AJVP96tAWxw": [
                9,
                23,
                8,
                20,
                21,
                0,
                4
            ],
            "BYdtJs6wU5o": [
                9,
                15,
                0,
                4
            ],
            "J5IlKj7H8T8": [
                2,
                0,
                23,
                4
            ],
            "MNvT5JekDpg": [
                10,
                15,
                21,
                9,
                0
            ],
            "fOvTtapxa9c": [
                9,
                15,
                21,
                4,
                0,
                20,
                10
            ],
            "ohM7D21C_8Q": [
                15,
                9
            ]
        },
        "21": {
            "5ctbvkAMQO4": [
                9,
                0,
                23
            ],
            "8FGHDBwoPVo": [
                9,
                23,
                0,
                21
            ],
            "BYdtJs6wU5o": [
                9,
                15,
                0
            ],
            "MNvT5JekDpg": [
                15,
                21,
                9,
                0
            ],
            "ohM7D21C_8Q": [
                15,
                9
            ]
        },
        "22": {
            "5ctbvkAMQO4": [
                9,
                0,
                19
            ],
            "BYdtJs6wU5o": [
                9,
                15,
                0
            ],
            "imPpT2Qo2sk": [
                22,
                15,
                9,
                19,
                0
            ],
            "ohM7D21C_8Q": [
                15,
                9
            ],
            "yGKTphqxR9Q": [
                9,
                19,
                15
            ]
        },
        "23": {
            "5ctbvkAMQO4": [
                9,
                0,
                23
            ],
            "8FGHDBwoPVo": [
                9,
                23,
                0
            ],
            "BYdtJs6wU5o": [
                9,
                15,
                0
            ],
            "ohM7D21C_8Q": [
                15,
                9
            ]
        },
        "24": {},
        "25": {
            "5ctbvkAMQO4": [
                9,
                0,
                23,
                2,
                8,
                4,
                25,
                12
            ],
            "8FGHDBwoPVo": [
                9,
                23,
                0,
                2,
                12,
                21
            ],
            "AJVP96tAWxw": [
                9,
                23,
                8,
                21,
                0,
                4
            ],
            "BYdtJs6wU5o": [
                9,
                15,
                0,
                4
            ],
            "J5IlKj7H8T8": [
                2,
                0,
                23,
                4
            ],
            "MNvT5JekDpg": [
                10,
                15,
                21,
                9,
                0
            ],
            "Sx3Fpw0XCXk": [
                9,
                18,
                0
            ],
            "fOvTtapxa9c": [
                9,
                25,
                13,
                15,
                21,
                18,
                4,
                0,
                10
            ],
            "iGmHnICXDss": [
                9,
                18,
                27,
                0,
                10,
                25
            ],
            "ohM7D21C_8Q": [
                15,
                9
            ]
        },
        "26": {
            "5ctbvkAMQO4": [
                9,
                0,
                1
            ],
            "BYdtJs6wU5o": [
                9,
                1,
                26,
                0
            ],
            "imPpT2Qo2sk": [
                9,
                1,
                26,
                0
            ]
        },
        "27": {},
        "28": {},
        "29": {
            "d4gGtcobq8M": [
                28,
                29
            ]
        }
    },
    "concept_relationship": {
        "links": [
            {
                "prerequisite": null,
                "similarity": 0.9999999999999997,
                "source": 0,
                "target": 0
            },
            {
                "prerequisite": 2.5845345787052327,
                "similarity": 0.510132990857236,
                "source": 0,
                "target": 1
            },
            {
                "prerequisite": 4.530354428415314,
                "similarity": 0.7294157338705618,
                "source": 0,
                "target": 2
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 0,
                "target": 3
            },
            {
                "prerequisite": 2.9786536135107093,
                "similarity": 0.5744428422615251,
                "source": 0,
                "target": 4
            },
            {
                "prerequisite": null,
                "similarity": 0.11470786693528087,
                "source": 0,
                "target": 5
            },
            {
                "prerequisite": null,
                "similarity": 0.16222142113076252,
                "source": 0,
                "target": 6
            },
            {
                "prerequisite": null,
                "similarity": 0.11470786693528087,
                "source": 0,
                "target": 7
            },
            {
                "prerequisite": 3.995925833648708,
                "similarity": 0.7564945880212885,
                "source": 0,
                "target": 8
            },
            {
                "prerequisite": 0.6421153248986271,
                "similarity": 0.6447368421052629,
                "source": 9,
                "target": 0
            },
            {
                "prerequisite": 3.9923622943619295,
                "similarity": 0.47941573387056174,
                "source": 0,
                "target": 10
            },
            {
                "prerequisite": 3.7310303123159767,
                "similarity": 0.4486798535597566,
                "source": 0,
                "target": 11
            },
            {
                "prerequisite": 3.9792901469632636,
                "similarity": 0.7294157338705618,
                "source": 0,
                "target": 12
            },
            {
                "prerequisite": null,
                "similarity": 0.41222142113076254,
                "source": 0,
                "target": 13
            },
            {
                "prerequisite": null,
                "similarity": 0.16222142113076252,
                "source": 0,
                "target": 14
            },
            {
                "prerequisite": 0.8987177263343581,
                "similarity": 0.5401905000440046,
                "source": 0,
                "target": 15
            },
            {
                "prerequisite": null,
                "similarity": 0.16222142113076252,
                "source": 0,
                "target": 16
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 0,
                "target": 17
            },
            {
                "prerequisite": 5.435177065412278,
                "similarity": 0.6986798535597566,
                "source": 0,
                "target": 18
            },
            {
                "prerequisite": null,
                "similarity": 0.2838874869788344,
                "source": 0,
                "target": 19
            },
            {
                "prerequisite": null,
                "similarity": 0.22941573387056174,
                "source": 0,
                "target": 20
            },
            {
                "prerequisite": 2.8329647802253928,
                "similarity": 0.510132990857236,
                "source": 0,
                "target": 21
            },
            {
                "prerequisite": null,
                "similarity": 0.16222142113076252,
                "source": 0,
                "target": 22
            },
            {
                "prerequisite": 0.6859831288329209,
                "similarity": 0.6317708577854666,
                "source": 0,
                "target": 23
            },
            {
                "prerequisite": null,
                "similarity": 0.11470786693528087,
                "source": 0,
                "target": 24
            },
            {
                "prerequisite": 4.306814239809057,
                "similarity": 0.7294157338705618,
                "source": 0,
                "target": 25
            },
            {
                "prerequisite": 3.773062156762901,
                "similarity": 0.47941573387056174,
                "source": 0,
                "target": 26
            },
            {
                "prerequisite": null,
                "similarity": 0.16222142113076252,
                "source": 0,
                "target": 27
            },
            {
                "prerequisite": null,
                "similarity": 0.11470786693528087,
                "source": 0,
                "target": 28
            },
            {
                "prerequisite": null,
                "similarity": 0.11470786693528087,
                "source": 0,
                "target": 29
            },
            {
                "prerequisite": null,
                "similarity": 0.510132990857236,
                "source": 1,
                "target": 0
            },
            {
                "prerequisite": null,
                "similarity": 0.9999999999999998,
                "source": 1,
                "target": 1
            },
            {
                "prerequisite": null,
                "similarity": 0.0944911182523068,
                "source": 1,
                "target": 2
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 1,
                "target": 3
            },
            {
                "prerequisite": null,
                "similarity": 0.20044593143431827,
                "source": 1,
                "target": 4
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 1,
                "target": 5
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 1,
                "target": 6
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 1,
                "target": 7
            },
            {
                "prerequisite": null,
                "similarity": 0.1690308509457033,
                "source": 1,
                "target": 8
            },
            {
                "prerequisite": 3.7632756749073644,
                "similarity": 0.553488489333442,
                "source": 9,
                "target": 1
            },
            {
                "prerequisite": null,
                "similarity": 0.0944911182523068,
                "source": 1,
                "target": 10
            },
            {
                "prerequisite": null,
                "similarity": 0.1091089451179962,
                "source": 1,
                "target": 11
            },
            {
                "prerequisite": null,
                "similarity": 0.0944911182523068,
                "source": 1,
                "target": 12
            },
            {
                "prerequisite": null,
                "similarity": 0.13363062095621217,
                "source": 1,
                "target": 13
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 1,
                "target": 14
            },
            {
                "prerequisite": null,
                "similarity": 0.2988071523335984,
                "source": 1,
                "target": 15
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 1,
                "target": 16
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 1,
                "target": 17
            },
            {
                "prerequisite": null,
                "similarity": 0.1091089451179962,
                "source": 1,
                "target": 18
            },
            {
                "prerequisite": null,
                "similarity": 0.20044593143431827,
                "source": 1,
                "target": 19
            },
            {
                "prerequisite": null,
                "similarity": 0.0944911182523068,
                "source": 1,
                "target": 20
            },
            {
                "prerequisite": null,
                "similarity": 0.14285714285714282,
                "source": 1,
                "target": 21
            },
            {
                "prerequisite": null,
                "similarity": 0.26726124191242434,
                "source": 1,
                "target": 22
            },
            {
                "prerequisite": null,
                "similarity": 0.4072427255082878,
                "source": 1,
                "target": 23
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 1,
                "target": 24
            },
            {
                "prerequisite": null,
                "similarity": 0.1889822365046136,
                "source": 1,
                "target": 25
            },
            {
                "prerequisite": 0.2937502191239658,
                "similarity": 0.6279644730092272,
                "source": 1,
                "target": 26
            },
            {
                "prerequisite": null,
                "similarity": 0.13363062095621217,
                "source": 1,
                "target": 27
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 1,
                "target": 28
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 1,
                "target": 29
            },
            {
                "prerequisite": null,
                "similarity": 0.7294157338705618,
                "source": 2,
                "target": 0
            },
            {
                "prerequisite": null,
                "similarity": 0.0944911182523068,
                "source": 2,
                "target": 1
            },
            {
                "prerequisite": null,
                "similarity": 1.0,
                "source": 2,
                "target": 2
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 2,
                "target": 3
            },
            {
                "prerequisite": 2.035494295540477,
                "similarity": 0.5151650429449552,
                "source": 4,
                "target": 2
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 2,
                "target": 5
            },
            {
                "prerequisite": null,
                "similarity": 0.35355339059327373,
                "source": 2,
                "target": 6
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 2,
                "target": 7
            },
            {
                "prerequisite": null,
                "similarity": 0.11180339887498948,
                "source": 2,
                "target": 8
            },
            {
                "prerequisite": null,
                "similarity": 0.3647078669352809,
                "source": 2,
                "target": 9
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 2,
                "target": 10
            },
            {
                "prerequisite": null,
                "similarity": 0.2886751345948129,
                "source": 2,
                "target": 11
            },
            {
                "prerequisite": 0.18120626918623062,
                "similarity": 0.5,
                "source": 2,
                "target": 12
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 2,
                "target": 13
            },
            {
                "prerequisite": 0.26866908610144263,
                "similarity": 0.6035533905932737,
                "source": 2,
                "target": 14
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 2,
                "target": 15
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 2,
                "target": 16
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 2,
                "target": 17
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 2,
                "target": 18
            },
            {
                "prerequisite": null,
                "similarity": 0.17677669529663687,
                "source": 2,
                "target": 19
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 2,
                "target": 20
            },
            {
                "prerequisite": null,
                "similarity": 0.3444911182523068,
                "source": 2,
                "target": 21
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 2,
                "target": 22
            },
            {
                "prerequisite": 3.627214114414439,
                "similarity": 0.5273500981126146,
                "source": 23,
                "target": 2
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 2,
                "target": 24
            },
            {
                "prerequisite": null,
                "similarity": 0.375,
                "source": 2,
                "target": 25
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 2,
                "target": 26
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 2,
                "target": 27
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 2,
                "target": 28
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 2,
                "target": 29
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 3,
                "target": 0
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 3,
                "target": 1
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 3,
                "target": 2
            },
            {
                "prerequisite": null,
                "similarity": 1.0,
                "source": 3,
                "target": 3
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 3,
                "target": 4
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 3,
                "target": 5
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 3,
                "target": 6
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 3,
                "target": 7
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 3,
                "target": 8
            },
            {
                "prerequisite": null,
                "similarity": 0.11470786693528087,
                "source": 3,
                "target": 9
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 3,
                "target": 10
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 3,
                "target": 11
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 3,
                "target": 12
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 3,
                "target": 13
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 3,
                "target": 14
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 3,
                "target": 15
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 3,
                "target": 16
            },
            {
                "prerequisite": 0.16480810787145406,
                "similarity": 0.5,
                "source": 3,
                "target": 17
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 3,
                "target": 18
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 3,
                "target": 19
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 3,
                "target": 20
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 3,
                "target": 21
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 3,
                "target": 22
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 3,
                "target": 23
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 3,
                "target": 24
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 3,
                "target": 25
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 3,
                "target": 26
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 3,
                "target": 27
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 3,
                "target": 28
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 3,
                "target": 29
            },
            {
                "prerequisite": null,
                "similarity": 0.5744428422615251,
                "source": 4,
                "target": 0
            },
            {
                "prerequisite": null,
                "similarity": 0.20044593143431827,
                "source": 4,
                "target": 1
            },
            {
                "prerequisite": null,
                "similarity": 0.5151650429449552,
                "source": 4,
                "target": 2
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 4,
                "target": 3
            },
            {
                "prerequisite": null,
                "similarity": 1.0,
                "source": 4,
                "target": 4
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 4,
                "target": 5
            },
            {
                "prerequisite": null,
                "similarity": 0.24999999999999994,
                "source": 4,
                "target": 6
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 4,
                "target": 7
            },
            {
                "prerequisite": null,
                "similarity": 0.23717082451262841,
                "source": 4,
                "target": 8
            },
            {
                "prerequisite": 2.728326216078308,
                "similarity": 0.45277677641345315,
                "source": 9,
                "target": 4
            },
            {
                "prerequisite": null,
                "similarity": 0.17677669529663687,
                "source": 4,
                "target": 10
            },
            {
                "prerequisite": null,
                "similarity": 0.10206207261596575,
                "source": 4,
                "target": 11
            },
            {
                "prerequisite": null,
                "similarity": 0.33838834764831843,
                "source": 4,
                "target": 12
            },
            {
                "prerequisite": null,
                "similarity": 0.12499999999999997,
                "source": 4,
                "target": 13
            },
            {
                "prerequisite": null,
                "similarity": 0.24999999999999994,
                "source": 4,
                "target": 14
            },
            {
                "prerequisite": null,
                "similarity": 0.36180339887498947,
                "source": 4,
                "target": 15
            },
            {
                "prerequisite": null,
                "similarity": 0.375,
                "source": 4,
                "target": 16
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 4,
                "target": 17
            },
            {
                "prerequisite": null,
                "similarity": 0.10206207261596575,
                "source": 4,
                "target": 18
            },
            {
                "prerequisite": null,
                "similarity": 0.12499999999999997,
                "source": 4,
                "target": 19
            },
            {
                "prerequisite": null,
                "similarity": 0.17677669529663687,
                "source": 4,
                "target": 20
            },
            {
                "prerequisite": null,
                "similarity": 0.38363062095621214,
                "source": 4,
                "target": 21
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 4,
                "target": 22
            },
            {
                "prerequisite": 1.583633236308767,
                "similarity": 0.49514516892273003,
                "source": 23,
                "target": 4
            },
            {
                "prerequisite": null,
                "similarity": 0.17677669529663687,
                "source": 4,
                "target": 24
            },
            {
                "prerequisite": 3.2800945021899746,
                "similarity": 0.5151650429449552,
                "source": 4,
                "target": 25
            },
            {
                "prerequisite": null,
                "similarity": 0.08838834764831843,
                "source": 4,
                "target": 26
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 4,
                "target": 27
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 4,
                "target": 28
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 4,
                "target": 29
            },
            {
                "prerequisite": null,
                "similarity": 0.11470786693528087,
                "source": 5,
                "target": 0
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 5,
                "target": 1
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 5,
                "target": 2
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 5,
                "target": 3
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 5,
                "target": 4
            },
            {
                "prerequisite": null,
                "similarity": 1.0,
                "source": 5,
                "target": 5
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 5,
                "target": 6
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 5,
                "target": 7
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 5,
                "target": 8
            },
            {
                "prerequisite": null,
                "similarity": 0.11470786693528087,
                "source": 5,
                "target": 9
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 5,
                "target": 10
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 5,
                "target": 11
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 5,
                "target": 12
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 5,
                "target": 13
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 5,
                "target": 14
            },
            {
                "prerequisite": null,
                "similarity": 0.15811388300841897,
                "source": 5,
                "target": 15
            },
            {
                "prerequisite": null,
                "similarity": 0.35355339059327373,
                "source": 5,
                "target": 16
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 5,
                "target": 17
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 5,
                "target": 18
            },
            {
                "prerequisite": 4.16519087135005,
                "similarity": 0.42677669529663687,
                "source": 19,
                "target": 5
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 5,
                "target": 20
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 5,
                "target": 21
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 5,
                "target": 22
            },
            {
                "prerequisite": null,
                "similarity": 0.1386750490563073,
                "source": 5,
                "target": 23
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 5,
                "target": 24
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 5,
                "target": 25
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 5,
                "target": 26
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 5,
                "target": 27
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 5,
                "target": 28
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 5,
                "target": 29
            },
            {
                "prerequisite": null,
                "similarity": 0.16222142113076252,
                "source": 6,
                "target": 0
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 6,
                "target": 1
            },
            {
                "prerequisite": null,
                "similarity": 0.35355339059327373,
                "source": 6,
                "target": 2
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 6,
                "target": 3
            },
            {
                "prerequisite": null,
                "similarity": 0.24999999999999994,
                "source": 6,
                "target": 4
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 6,
                "target": 5
            },
            {
                "prerequisite": null,
                "similarity": 0.9999999999999999,
                "source": 6,
                "target": 6
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 6,
                "target": 7
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 6,
                "target": 8
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 6,
                "target": 9
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 6,
                "target": 10
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 6,
                "target": 11
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 6,
                "target": 12
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 6,
                "target": 13
            },
            {
                "prerequisite": 0.1385359020901007,
                "similarity": 0.4999999999999999,
                "source": 6,
                "target": 14
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 6,
                "target": 15
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 6,
                "target": 16
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 6,
                "target": 17
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 6,
                "target": 18
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 6,
                "target": 19
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 6,
                "target": 20
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 6,
                "target": 21
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 6,
                "target": 22
            },
            {
                "prerequisite": null,
                "similarity": 0.19611613513818402,
                "source": 6,
                "target": 23
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 6,
                "target": 24
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 6,
                "target": 25
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 6,
                "target": 26
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 6,
                "target": 27
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 6,
                "target": 28
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 6,
                "target": 29
            },
            {
                "prerequisite": null,
                "similarity": 0.11470786693528087,
                "source": 7,
                "target": 0
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 7,
                "target": 1
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 7,
                "target": 2
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 7,
                "target": 3
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 7,
                "target": 4
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 7,
                "target": 5
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 7,
                "target": 6
            },
            {
                "prerequisite": null,
                "similarity": 1.0,
                "source": 7,
                "target": 7
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 7,
                "target": 8
            },
            {
                "prerequisite": null,
                "similarity": 0.3647078669352809,
                "source": 7,
                "target": 9
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 7,
                "target": 10
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 7,
                "target": 11
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 7,
                "target": 12
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 7,
                "target": 13
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 7,
                "target": 14
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 7,
                "target": 15
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 7,
                "target": 16
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 7,
                "target": 17
            },
            {
                "prerequisite": null,
                "similarity": 0.2886751345948129,
                "source": 7,
                "target": 18
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 7,
                "target": 19
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 7,
                "target": 20
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 7,
                "target": 21
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 7,
                "target": 22
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 7,
                "target": 23
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 7,
                "target": 24
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 7,
                "target": 25
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 7,
                "target": 26
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 7,
                "target": 27
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 7,
                "target": 28
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 7,
                "target": 29
            },
            {
                "prerequisite": null,
                "similarity": 0.7564945880212885,
                "source": 8,
                "target": 0
            },
            {
                "prerequisite": null,
                "similarity": 0.1690308509457033,
                "source": 8,
                "target": 1
            },
            {
                "prerequisite": null,
                "similarity": 0.11180339887498948,
                "source": 8,
                "target": 2
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 8,
                "target": 3
            },
            {
                "prerequisite": null,
                "similarity": 0.23717082451262841,
                "source": 8,
                "target": 4
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 8,
                "target": 5
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 8,
                "target": 6
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 8,
                "target": 7
            },
            {
                "prerequisite": null,
                "similarity": 1.0,
                "source": 8,
                "target": 8
            },
            {
                "prerequisite": 4.445051732647988,
                "similarity": 0.4551956704170308,
                "source": 9,
                "target": 8
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 8,
                "target": 10
            },
            {
                "prerequisite": null,
                "similarity": 0.12909944487358058,
                "source": 8,
                "target": 11
            },
            {
                "prerequisite": 1.7054069295756382,
                "similarity": 0.47360679774997894,
                "source": 8,
                "target": 12
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 8,
                "target": 13
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 8,
                "target": 14
            },
            {
                "prerequisite": null,
                "similarity": 0.3914213562373095,
                "source": 8,
                "target": 15
            },
            {
                "prerequisite": null,
                "similarity": 0.40811388300841894,
                "source": 8,
                "target": 16
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 8,
                "target": 17
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 8,
                "target": 18
            },
            {
                "prerequisite": null,
                "similarity": 0.3162277660168379,
                "source": 8,
                "target": 19
            },
            {
                "prerequisite": 0.4080150430586501,
                "similarity": 0.47360679774997894,
                "source": 8,
                "target": 20
            },
            {
                "prerequisite": 2.3148049618032203,
                "similarity": 0.503546276418555,
                "source": 21,
                "target": 8
            },
            {
                "prerequisite": null,
                "similarity": 0.15811388300841894,
                "source": 8,
                "target": 22
            },
            {
                "prerequisite": 2.5730258115216653,
                "similarity": 0.5600868364730212,
                "source": 23,
                "target": 8
            },
            {
                "prerequisite": null,
                "similarity": 0.22360679774997896,
                "source": 8,
                "target": 24
            },
            {
                "prerequisite": null,
                "similarity": 0.11180339887498948,
                "source": 8,
                "target": 25
            },
            {
                "prerequisite": null,
                "similarity": 0.11180339887498948,
                "source": 8,
                "target": 26
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 8,
                "target": 27
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 8,
                "target": 28
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 8,
                "target": 29
            },
            {
                "prerequisite": null,
                "similarity": 0.6447368421052629,
                "source": 9,
                "target": 0
            },
            {
                "prerequisite": null,
                "similarity": 0.553488489333442,
                "source": 9,
                "target": 1
            },
            {
                "prerequisite": null,
                "similarity": 0.3647078669352809,
                "source": 9,
                "target": 2
            },
            {
                "prerequisite": null,
                "similarity": 0.11470786693528087,
                "source": 9,
                "target": 3
            },
            {
                "prerequisite": null,
                "similarity": 0.45277677641345315,
                "source": 9,
                "target": 4
            },
            {
                "prerequisite": null,
                "similarity": 0.11470786693528087,
                "source": 9,
                "target": 5
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 9,
                "target": 6
            },
            {
                "prerequisite": null,
                "similarity": 0.3647078669352809,
                "source": 9,
                "target": 7
            },
            {
                "prerequisite": null,
                "similarity": 0.4551956704170308,
                "source": 9,
                "target": 8
            },
            {
                "prerequisite": null,
                "similarity": 0.9999999999999997,
                "source": 9,
                "target": 9
            },
            {
                "prerequisite": null,
                "similarity": 0.1720618004029213,
                "source": 9,
                "target": 10
            },
            {
                "prerequisite": 5.624014436987874,
                "similarity": 0.4486798535597566,
                "source": 9,
                "target": 11
            },
            {
                "prerequisite": 4.914597442903333,
                "similarity": 0.4220618004029213,
                "source": 9,
                "target": 12
            },
            {
                "prerequisite": null,
                "similarity": 0.41222142113076254,
                "source": 9,
                "target": 13
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 9,
                "target": 14
            },
            {
                "prerequisite": 3.153119440455104,
                "similarity": 0.8264643125495053,
                "source": 9,
                "target": 15
            },
            {
                "prerequisite": null,
                "similarity": 0.41222142113076254,
                "source": 9,
                "target": 16
            },
            {
                "prerequisite": null,
                "similarity": 0.11470786693528087,
                "source": 9,
                "target": 17
            },
            {
                "prerequisite": 5.865625907131379,
                "similarity": 0.4486798535597566,
                "source": 9,
                "target": 18
            },
            {
                "prerequisite": null,
                "similarity": 0.2838874869788344,
                "source": 9,
                "target": 19
            },
            {
                "prerequisite": null,
                "similarity": 0.1720618004029213,
                "source": 9,
                "target": 20
            },
            {
                "prerequisite": null,
                "similarity": 0.26013299085723596,
                "source": 9,
                "target": 21
            },
            {
                "prerequisite": null,
                "similarity": 0.41222142113076254,
                "source": 9,
                "target": 22
            },
            {
                "prerequisite": 2.066385997572084,
                "similarity": 0.5681423814878888,
                "source": 9,
                "target": 23
            },
            {
                "prerequisite": null,
                "similarity": 0.11470786693528087,
                "source": 9,
                "target": 24
            },
            {
                "prerequisite": 5.030527776289496,
                "similarity": 0.4220618004029213,
                "source": 9,
                "target": 25
            },
            {
                "prerequisite": null,
                "similarity": 0.22941573387056174,
                "source": 9,
                "target": 26
            },
            {
                "prerequisite": null,
                "similarity": 0.16222142113076252,
                "source": 9,
                "target": 27
            },
            {
                "prerequisite": null,
                "similarity": 0.11470786693528087,
                "source": 9,
                "target": 28
            },
            {
                "prerequisite": null,
                "similarity": 0.11470786693528087,
                "source": 9,
                "target": 29
            },
            {
                "prerequisite": null,
                "similarity": 0.47941573387056174,
                "source": 10,
                "target": 0
            },
            {
                "prerequisite": null,
                "similarity": 0.0944911182523068,
                "source": 10,
                "target": 1
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 10,
                "target": 2
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 10,
                "target": 3
            },
            {
                "prerequisite": null,
                "similarity": 0.17677669529663687,
                "source": 10,
                "target": 4
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 10,
                "target": 5
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 10,
                "target": 6
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 10,
                "target": 7
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 10,
                "target": 8
            },
            {
                "prerequisite": null,
                "similarity": 0.1720618004029213,
                "source": 10,
                "target": 9
            },
            {
                "prerequisite": null,
                "similarity": 1.0,
                "source": 10,
                "target": 10
            },
            {
                "prerequisite": null,
                "similarity": 0.14433756729740646,
                "source": 10,
                "target": 11
            },
            {
                "prerequisite": 0.4616689530097897,
                "similarity": 0.5,
                "source": 10,
                "target": 12
            },
            {
                "prerequisite": 0.9383567574425937,
                "similarity": 0.42677669529663687,
                "source": 10,
                "target": 13
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 10,
                "target": 14
            },
            {
                "prerequisite": null,
                "similarity": 0.15811388300841897,
                "source": 10,
                "target": 15
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 10,
                "target": 16
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 10,
                "target": 17
            },
            {
                "prerequisite": 3.449045592269958,
                "similarity": 0.5386751345948129,
                "source": 10,
                "target": 18
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 10,
                "target": 19
            },
            {
                "prerequisite": null,
                "similarity": 0.125,
                "source": 10,
                "target": 20
            },
            {
                "prerequisite": 1.005921864256417,
                "similarity": 0.6889822365046137,
                "source": 21,
                "target": 10
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 10,
                "target": 22
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 10,
                "target": 23
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 10,
                "target": 24
            },
            {
                "prerequisite": 2.0342727305440405,
                "similarity": 0.875,
                "source": 10,
                "target": 25
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 10,
                "target": 26
            },
            {
                "prerequisite": null,
                "similarity": 0.17677669529663687,
                "source": 10,
                "target": 27
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 10,
                "target": 28
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 10,
                "target": 29
            },
            {
                "prerequisite": null,
                "similarity": 0.4486798535597566,
                "source": 11,
                "target": 0
            },
            {
                "prerequisite": null,
                "similarity": 0.1091089451179962,
                "source": 11,
                "target": 1
            },
            {
                "prerequisite": null,
                "similarity": 0.2886751345948129,
                "source": 11,
                "target": 2
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 11,
                "target": 3
            },
            {
                "prerequisite": null,
                "similarity": 0.10206207261596575,
                "source": 11,
                "target": 4
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 11,
                "target": 5
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 11,
                "target": 6
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 11,
                "target": 7
            },
            {
                "prerequisite": null,
                "similarity": 0.12909944487358058,
                "source": 11,
                "target": 8
            },
            {
                "prerequisite": null,
                "similarity": 0.4486798535597566,
                "source": 11,
                "target": 9
            },
            {
                "prerequisite": null,
                "similarity": 0.14433756729740646,
                "source": 11,
                "target": 10
            },
            {
                "prerequisite": null,
                "similarity": 1.0,
                "source": 11,
                "target": 11
            },
            {
                "prerequisite": 0.2473328105872413,
                "similarity": 0.5386751345948129,
                "source": 12,
                "target": 11
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 11,
                "target": 13
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 11,
                "target": 14
            },
            {
                "prerequisite": null,
                "similarity": 0.0912870929175277,
                "source": 11,
                "target": 15
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 11,
                "target": 16
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 11,
                "target": 17
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 11,
                "target": 18
            },
            {
                "prerequisite": null,
                "similarity": 0.2041241452319315,
                "source": 11,
                "target": 19
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 11,
                "target": 20
            },
            {
                "prerequisite": 2.4962286406341017,
                "similarity": 0.46821789023599236,
                "source": 21,
                "target": 11
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 11,
                "target": 22
            },
            {
                "prerequisite": null,
                "similarity": 0.41012815380508716,
                "source": 11,
                "target": 23
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 11,
                "target": 24
            },
            {
                "prerequisite": null,
                "similarity": 0.14433756729740646,
                "source": 11,
                "target": 25
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 11,
                "target": 26
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 11,
                "target": 27
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 11,
                "target": 28
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 11,
                "target": 29
            },
            {
                "prerequisite": null,
                "similarity": 0.7294157338705618,
                "source": 12,
                "target": 0
            },
            {
                "prerequisite": null,
                "similarity": 0.0944911182523068,
                "source": 12,
                "target": 1
            },
            {
                "prerequisite": null,
                "similarity": 0.5,
                "source": 12,
                "target": 2
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 12,
                "target": 3
            },
            {
                "prerequisite": null,
                "similarity": 0.33838834764831843,
                "source": 12,
                "target": 4
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 12,
                "target": 5
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 12,
                "target": 6
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 12,
                "target": 7
            },
            {
                "prerequisite": null,
                "similarity": 0.47360679774997894,
                "source": 12,
                "target": 8
            },
            {
                "prerequisite": null,
                "similarity": 0.4220618004029213,
                "source": 12,
                "target": 9
            },
            {
                "prerequisite": null,
                "similarity": 0.5,
                "source": 12,
                "target": 10
            },
            {
                "prerequisite": null,
                "similarity": 0.5386751345948129,
                "source": 12,
                "target": 11
            },
            {
                "prerequisite": null,
                "similarity": 1.0,
                "source": 12,
                "target": 12
            },
            {
                "prerequisite": 1.39281403736373,
                "similarity": 0.6767766952966369,
                "source": 12,
                "target": 13
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 12,
                "target": 14
            },
            {
                "prerequisite": null,
                "similarity": 0.3290569415042095,
                "source": 12,
                "target": 15
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 12,
                "target": 16
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 12,
                "target": 17
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 12,
                "target": 18
            },
            {
                "prerequisite": null,
                "similarity": 0.2651650429449553,
                "source": 12,
                "target": 19
            },
            {
                "prerequisite": 0.11491365199997183,
                "similarity": 0.5,
                "source": 12,
                "target": 20
            },
            {
                "prerequisite": 1.3002016077688026,
                "similarity": 0.6889822365046137,
                "source": 21,
                "target": 12
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 12,
                "target": 22
            },
            {
                "prerequisite": 3.554472570417399,
                "similarity": 0.5273500981126146,
                "source": 23,
                "target": 12
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 12,
                "target": 24
            },
            {
                "prerequisite": null,
                "similarity": 0.375,
                "source": 12,
                "target": 25
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 12,
                "target": 26
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 12,
                "target": 27
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 12,
                "target": 28
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 12,
                "target": 29
            },
            {
                "prerequisite": null,
                "similarity": 0.41222142113076254,
                "source": 13,
                "target": 0
            },
            {
                "prerequisite": null,
                "similarity": 0.13363062095621217,
                "source": 13,
                "target": 1
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 13,
                "target": 2
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 13,
                "target": 3
            },
            {
                "prerequisite": null,
                "similarity": 0.12499999999999997,
                "source": 13,
                "target": 4
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 13,
                "target": 5
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 13,
                "target": 6
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 13,
                "target": 7
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 13,
                "target": 8
            },
            {
                "prerequisite": null,
                "similarity": 0.41222142113076254,
                "source": 13,
                "target": 9
            },
            {
                "prerequisite": null,
                "similarity": 0.42677669529663687,
                "source": 13,
                "target": 10
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 13,
                "target": 11
            },
            {
                "prerequisite": null,
                "similarity": 0.6767766952966369,
                "source": 13,
                "target": 12
            },
            {
                "prerequisite": null,
                "similarity": 0.9999999999999999,
                "source": 13,
                "target": 13
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 13,
                "target": 14
            },
            {
                "prerequisite": null,
                "similarity": 0.36180339887498947,
                "source": 13,
                "target": 15
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 13,
                "target": 16
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 13,
                "target": 17
            },
            {
                "prerequisite": null,
                "similarity": 0.2041241452319315,
                "source": 13,
                "target": 18
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 13,
                "target": 19
            },
            {
                "prerequisite": null,
                "similarity": 0.35355339059327373,
                "source": 13,
                "target": 20
            },
            {
                "prerequisite": 1.1949478900233572,
                "similarity": 0.6336306209562121,
                "source": 21,
                "target": 13
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 13,
                "target": 22
            },
            {
                "prerequisite": 2.759826997732789,
                "similarity": 0.598058067569092,
                "source": 23,
                "target": 13
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 13,
                "target": 24
            },
            {
                "prerequisite": 1.5444797559483259,
                "similarity": 0.42677669529663687,
                "source": 13,
                "target": 25
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 13,
                "target": 26
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 13,
                "target": 27
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 13,
                "target": 28
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 13,
                "target": 29
            },
            {
                "prerequisite": null,
                "similarity": 0.16222142113076252,
                "source": 14,
                "target": 0
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 14,
                "target": 1
            },
            {
                "prerequisite": null,
                "similarity": 0.6035533905932737,
                "source": 14,
                "target": 2
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 14,
                "target": 3
            },
            {
                "prerequisite": null,
                "similarity": 0.24999999999999994,
                "source": 14,
                "target": 4
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 14,
                "target": 5
            },
            {
                "prerequisite": null,
                "similarity": 0.4999999999999999,
                "source": 14,
                "target": 6
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 14,
                "target": 7
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 14,
                "target": 8
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 14,
                "target": 9
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 14,
                "target": 10
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 14,
                "target": 11
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 14,
                "target": 12
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 14,
                "target": 13
            },
            {
                "prerequisite": null,
                "similarity": 0.9999999999999999,
                "source": 14,
                "target": 14
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 14,
                "target": 15
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 14,
                "target": 16
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 14,
                "target": 17
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 14,
                "target": 18
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 14,
                "target": 19
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 14,
                "target": 20
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 14,
                "target": 21
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 14,
                "target": 22
            },
            {
                "prerequisite": null,
                "similarity": 0.19611613513818402,
                "source": 14,
                "target": 23
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 14,
                "target": 24
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 14,
                "target": 25
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 14,
                "target": 26
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 14,
                "target": 27
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 14,
                "target": 28
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 14,
                "target": 29
            },
            {
                "prerequisite": null,
                "similarity": 0.5401905000440046,
                "source": 15,
                "target": 0
            },
            {
                "prerequisite": null,
                "similarity": 0.2988071523335984,
                "source": 15,
                "target": 1
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 15,
                "target": 2
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 15,
                "target": 3
            },
            {
                "prerequisite": null,
                "similarity": 0.36180339887498947,
                "source": 15,
                "target": 4
            },
            {
                "prerequisite": null,
                "similarity": 0.15811388300841897,
                "source": 15,
                "target": 5
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 15,
                "target": 6
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 15,
                "target": 7
            },
            {
                "prerequisite": null,
                "similarity": 0.3914213562373095,
                "source": 15,
                "target": 8
            },
            {
                "prerequisite": null,
                "similarity": 0.8264643125495053,
                "source": 15,
                "target": 9
            },
            {
                "prerequisite": null,
                "similarity": 0.15811388300841897,
                "source": 15,
                "target": 10
            },
            {
                "prerequisite": null,
                "similarity": 0.0912870929175277,
                "source": 15,
                "target": 11
            },
            {
                "prerequisite": null,
                "similarity": 0.3290569415042095,
                "source": 15,
                "target": 12
            },
            {
                "prerequisite": null,
                "similarity": 0.36180339887498947,
                "source": 15,
                "target": 13
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 15,
                "target": 14
            },
            {
                "prerequisite": null,
                "similarity": 1.0,
                "source": 15,
                "target": 15
            },
            {
                "prerequisite": null,
                "similarity": 0.11180339887498948,
                "source": 15,
                "target": 16
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 15,
                "target": 17
            },
            {
                "prerequisite": null,
                "similarity": 0.0912870929175277,
                "source": 15,
                "target": 18
            },
            {
                "prerequisite": null,
                "similarity": 0.2795084971874737,
                "source": 15,
                "target": 19
            },
            {
                "prerequisite": null,
                "similarity": 0.408113883008419,
                "source": 15,
                "target": 20
            },
            {
                "prerequisite": 2.757421171196894,
                "similarity": 0.5488071523335984,
                "source": 15,
                "target": 21
            },
            {
                "prerequisite": 4.787948391434577,
                "similarity": 0.47360679774997894,
                "source": 15,
                "target": 22
            },
            {
                "prerequisite": 0.507131619581576,
                "similarity": 0.7192645048267573,
                "source": 15,
                "target": 23
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 15,
                "target": 24
            },
            {
                "prerequisite": null,
                "similarity": 0.3290569415042095,
                "source": 15,
                "target": 25
            },
            {
                "prerequisite": null,
                "similarity": 0.23717082451262844,
                "source": 15,
                "target": 26
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 15,
                "target": 27
            },
            {
                "prerequisite": null,
                "similarity": 0.15811388300841897,
                "source": 15,
                "target": 28
            },
            {
                "prerequisite": null,
                "similarity": 0.15811388300841897,
                "source": 15,
                "target": 29
            },
            {
                "prerequisite": null,
                "similarity": 0.16222142113076252,
                "source": 16,
                "target": 0
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 16,
                "target": 1
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 16,
                "target": 2
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 16,
                "target": 3
            },
            {
                "prerequisite": null,
                "similarity": 0.375,
                "source": 16,
                "target": 4
            },
            {
                "prerequisite": null,
                "similarity": 0.35355339059327373,
                "source": 16,
                "target": 5
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 16,
                "target": 6
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 16,
                "target": 7
            },
            {
                "prerequisite": null,
                "similarity": 0.40811388300841894,
                "source": 16,
                "target": 8
            },
            {
                "prerequisite": null,
                "similarity": 0.41222142113076254,
                "source": 16,
                "target": 9
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 16,
                "target": 10
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 16,
                "target": 11
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 16,
                "target": 12
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 16,
                "target": 13
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 16,
                "target": 14
            },
            {
                "prerequisite": null,
                "similarity": 0.11180339887498948,
                "source": 16,
                "target": 15
            },
            {
                "prerequisite": null,
                "similarity": 0.9999999999999999,
                "source": 16,
                "target": 16
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 16,
                "target": 17
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 16,
                "target": 18
            },
            {
                "prerequisite": null,
                "similarity": 0.24999999999999994,
                "source": 16,
                "target": 19
            },
            {
                "prerequisite": null,
                "similarity": 0.17677669529663687,
                "source": 16,
                "target": 20
            },
            {
                "prerequisite": null,
                "similarity": 0.38363062095621214,
                "source": 16,
                "target": 21
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 16,
                "target": 22
            },
            {
                "prerequisite": null,
                "similarity": 0.19611613513818402,
                "source": 16,
                "target": 23
            },
            {
                "prerequisite": null,
                "similarity": 0.35355339059327373,
                "source": 16,
                "target": 24
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 16,
                "target": 25
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 16,
                "target": 26
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 16,
                "target": 27
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 16,
                "target": 28
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 16,
                "target": 29
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 17,
                "target": 0
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 17,
                "target": 1
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 17,
                "target": 2
            },
            {
                "prerequisite": null,
                "similarity": 0.5,
                "source": 17,
                "target": 3
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 17,
                "target": 4
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 17,
                "target": 5
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 17,
                "target": 6
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 17,
                "target": 7
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 17,
                "target": 8
            },
            {
                "prerequisite": null,
                "similarity": 0.11470786693528087,
                "source": 17,
                "target": 9
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 17,
                "target": 10
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 17,
                "target": 11
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 17,
                "target": 12
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 17,
                "target": 13
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 17,
                "target": 14
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 17,
                "target": 15
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 17,
                "target": 16
            },
            {
                "prerequisite": null,
                "similarity": 1.0,
                "source": 17,
                "target": 17
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 17,
                "target": 18
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 17,
                "target": 19
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 17,
                "target": 20
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 17,
                "target": 21
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 17,
                "target": 22
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 17,
                "target": 23
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 17,
                "target": 24
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 17,
                "target": 25
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 17,
                "target": 26
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 17,
                "target": 27
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 17,
                "target": 28
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 17,
                "target": 29
            },
            {
                "prerequisite": null,
                "similarity": 0.6986798535597566,
                "source": 18,
                "target": 0
            },
            {
                "prerequisite": null,
                "similarity": 0.1091089451179962,
                "source": 18,
                "target": 1
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 18,
                "target": 2
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 18,
                "target": 3
            },
            {
                "prerequisite": null,
                "similarity": 0.10206207261596575,
                "source": 18,
                "target": 4
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 18,
                "target": 5
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 18,
                "target": 6
            },
            {
                "prerequisite": null,
                "similarity": 0.2886751345948129,
                "source": 18,
                "target": 7
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 18,
                "target": 8
            },
            {
                "prerequisite": null,
                "similarity": 0.4486798535597566,
                "source": 18,
                "target": 9
            },
            {
                "prerequisite": null,
                "similarity": 0.5386751345948129,
                "source": 18,
                "target": 10
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 18,
                "target": 11
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 18,
                "target": 12
            },
            {
                "prerequisite": null,
                "similarity": 0.2041241452319315,
                "source": 18,
                "target": 13
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 18,
                "target": 14
            },
            {
                "prerequisite": null,
                "similarity": 0.0912870929175277,
                "source": 18,
                "target": 15
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 18,
                "target": 16
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 18,
                "target": 17
            },
            {
                "prerequisite": null,
                "similarity": 1.0,
                "source": 18,
                "target": 18
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 18,
                "target": 19
            },
            {
                "prerequisite": null,
                "similarity": 0.14433756729740646,
                "source": 18,
                "target": 20
            },
            {
                "prerequisite": null,
                "similarity": 0.3591089451179962,
                "source": 18,
                "target": 21
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 18,
                "target": 22
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 18,
                "target": 23
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 18,
                "target": 24
            },
            {
                "prerequisite": 0.11433207657851832,
                "similarity": 0.7886751345948129,
                "source": 18,
                "target": 25
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 18,
                "target": 26
            },
            {
                "prerequisite": 0.0020817056881679286,
                "similarity": 0.4541241452319315,
                "source": 27,
                "target": 18
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 18,
                "target": 28
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 18,
                "target": 29
            },
            {
                "prerequisite": null,
                "similarity": 0.2838874869788344,
                "source": 19,
                "target": 0
            },
            {
                "prerequisite": null,
                "similarity": 0.20044593143431827,
                "source": 19,
                "target": 1
            },
            {
                "prerequisite": null,
                "similarity": 0.17677669529663687,
                "source": 19,
                "target": 2
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 19,
                "target": 3
            },
            {
                "prerequisite": null,
                "similarity": 0.12499999999999997,
                "source": 19,
                "target": 4
            },
            {
                "prerequisite": null,
                "similarity": 0.42677669529663687,
                "source": 19,
                "target": 5
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 19,
                "target": 6
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 19,
                "target": 7
            },
            {
                "prerequisite": null,
                "similarity": 0.3162277660168379,
                "source": 19,
                "target": 8
            },
            {
                "prerequisite": null,
                "similarity": 0.2838874869788344,
                "source": 19,
                "target": 9
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 19,
                "target": 10
            },
            {
                "prerequisite": null,
                "similarity": 0.2041241452319315,
                "source": 19,
                "target": 11
            },
            {
                "prerequisite": null,
                "similarity": 0.2651650429449553,
                "source": 19,
                "target": 12
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 19,
                "target": 13
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 19,
                "target": 14
            },
            {
                "prerequisite": null,
                "similarity": 0.2795084971874737,
                "source": 19,
                "target": 15
            },
            {
                "prerequisite": null,
                "similarity": 0.24999999999999994,
                "source": 19,
                "target": 16
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 19,
                "target": 17
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 19,
                "target": 18
            },
            {
                "prerequisite": null,
                "similarity": 1.0,
                "source": 19,
                "target": 19
            },
            {
                "prerequisite": null,
                "similarity": 0.17677669529663687,
                "source": 19,
                "target": 20
            },
            {
                "prerequisite": null,
                "similarity": 0.3340765523905304,
                "source": 19,
                "target": 21
            },
            {
                "prerequisite": 3.8197543107810916,
                "similarity": 0.49999999999999994,
                "source": 19,
                "target": 22
            },
            {
                "prerequisite": null,
                "similarity": 0.343203236491822,
                "source": 19,
                "target": 23
            },
            {
                "prerequisite": null,
                "similarity": 0.17677669529663687,
                "source": 19,
                "target": 24
            },
            {
                "prerequisite": null,
                "similarity": 0.08838834764831843,
                "source": 19,
                "target": 25
            },
            {
                "prerequisite": null,
                "similarity": 0.17677669529663687,
                "source": 19,
                "target": 26
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 19,
                "target": 27
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 19,
                "target": 28
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 19,
                "target": 29
            },
            {
                "prerequisite": null,
                "similarity": 0.22941573387056174,
                "source": 20,
                "target": 0
            },
            {
                "prerequisite": null,
                "similarity": 0.0944911182523068,
                "source": 20,
                "target": 1
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 20,
                "target": 2
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 20,
                "target": 3
            },
            {
                "prerequisite": null,
                "similarity": 0.17677669529663687,
                "source": 20,
                "target": 4
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 20,
                "target": 5
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 20,
                "target": 6
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 20,
                "target": 7
            },
            {
                "prerequisite": null,
                "similarity": 0.47360679774997894,
                "source": 20,
                "target": 8
            },
            {
                "prerequisite": null,
                "similarity": 0.1720618004029213,
                "source": 20,
                "target": 9
            },
            {
                "prerequisite": null,
                "similarity": 0.125,
                "source": 20,
                "target": 10
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 20,
                "target": 11
            },
            {
                "prerequisite": null,
                "similarity": 0.5,
                "source": 20,
                "target": 12
            },
            {
                "prerequisite": null,
                "similarity": 0.35355339059327373,
                "source": 20,
                "target": 13
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 20,
                "target": 14
            },
            {
                "prerequisite": null,
                "similarity": 0.408113883008419,
                "source": 20,
                "target": 15
            },
            {
                "prerequisite": null,
                "similarity": 0.17677669529663687,
                "source": 20,
                "target": 16
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 20,
                "target": 17
            },
            {
                "prerequisite": null,
                "similarity": 0.14433756729740646,
                "source": 20,
                "target": 18
            },
            {
                "prerequisite": null,
                "similarity": 0.17677669529663687,
                "source": 20,
                "target": 19
            },
            {
                "prerequisite": null,
                "similarity": 1.0,
                "source": 20,
                "target": 20
            },
            {
                "prerequisite": 1.3041596312892527,
                "similarity": 0.7834733547569204,
                "source": 21,
                "target": 20
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 20,
                "target": 22
            },
            {
                "prerequisite": 3.6128335790186883,
                "similarity": 0.7080125735844609,
                "source": 23,
                "target": 20
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 20,
                "target": 24
            },
            {
                "prerequisite": null,
                "similarity": 0.125,
                "source": 20,
                "target": 25
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 20,
                "target": 26
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 20,
                "target": 27
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 20,
                "target": 28
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 20,
                "target": 29
            },
            {
                "prerequisite": null,
                "similarity": 0.510132990857236,
                "source": 21,
                "target": 0
            },
            {
                "prerequisite": null,
                "similarity": 0.14285714285714282,
                "source": 21,
                "target": 1
            },
            {
                "prerequisite": null,
                "similarity": 0.3444911182523068,
                "source": 21,
                "target": 2
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 21,
                "target": 3
            },
            {
                "prerequisite": null,
                "similarity": 0.38363062095621214,
                "source": 21,
                "target": 4
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 21,
                "target": 5
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 21,
                "target": 6
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 21,
                "target": 7
            },
            {
                "prerequisite": null,
                "similarity": 0.503546276418555,
                "source": 21,
                "target": 8
            },
            {
                "prerequisite": null,
                "similarity": 0.26013299085723596,
                "source": 21,
                "target": 9
            },
            {
                "prerequisite": null,
                "similarity": 0.6889822365046137,
                "source": 21,
                "target": 10
            },
            {
                "prerequisite": null,
                "similarity": 0.46821789023599236,
                "source": 21,
                "target": 11
            },
            {
                "prerequisite": null,
                "similarity": 0.6889822365046137,
                "source": 21,
                "target": 12
            },
            {
                "prerequisite": null,
                "similarity": 0.6336306209562121,
                "source": 21,
                "target": 13
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 21,
                "target": 14
            },
            {
                "prerequisite": null,
                "similarity": 0.5488071523335984,
                "source": 21,
                "target": 15
            },
            {
                "prerequisite": null,
                "similarity": 0.38363062095621214,
                "source": 21,
                "target": 16
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 21,
                "target": 17
            },
            {
                "prerequisite": null,
                "similarity": 0.3591089451179962,
                "source": 21,
                "target": 18
            },
            {
                "prerequisite": null,
                "similarity": 0.3340765523905304,
                "source": 21,
                "target": 19
            },
            {
                "prerequisite": null,
                "similarity": 0.7834733547569204,
                "source": 21,
                "target": 20
            },
            {
                "prerequisite": null,
                "similarity": 0.9999999999999998,
                "source": 21,
                "target": 21
            },
            {
                "prerequisite": null,
                "similarity": 0.13363062095621217,
                "source": 21,
                "target": 22
            },
            {
                "prerequisite": 1.3709504805886064,
                "similarity": 0.5120712091804795,
                "source": 23,
                "target": 21
            },
            {
                "prerequisite": null,
                "similarity": 0.1889822365046136,
                "source": 21,
                "target": 24
            },
            {
                "prerequisite": 2.22581520998627,
                "similarity": 0.5944911182523068,
                "source": 21,
                "target": 25
            },
            {
                "prerequisite": null,
                "similarity": 0.0944911182523068,
                "source": 21,
                "target": 26
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 21,
                "target": 27
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 21,
                "target": 28
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 21,
                "target": 29
            },
            {
                "prerequisite": null,
                "similarity": 0.16222142113076252,
                "source": 22,
                "target": 0
            },
            {
                "prerequisite": null,
                "similarity": 0.26726124191242434,
                "source": 22,
                "target": 1
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 22,
                "target": 2
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 22,
                "target": 3
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 22,
                "target": 4
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 22,
                "target": 5
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 22,
                "target": 6
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 22,
                "target": 7
            },
            {
                "prerequisite": null,
                "similarity": 0.15811388300841894,
                "source": 22,
                "target": 8
            },
            {
                "prerequisite": null,
                "similarity": 0.41222142113076254,
                "source": 22,
                "target": 9
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 22,
                "target": 10
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 22,
                "target": 11
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 22,
                "target": 12
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 22,
                "target": 13
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 22,
                "target": 14
            },
            {
                "prerequisite": null,
                "similarity": 0.47360679774997894,
                "source": 22,
                "target": 15
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 22,
                "target": 16
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 22,
                "target": 17
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 22,
                "target": 18
            },
            {
                "prerequisite": null,
                "similarity": 0.49999999999999994,
                "source": 22,
                "target": 19
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 22,
                "target": 20
            },
            {
                "prerequisite": null,
                "similarity": 0.13363062095621217,
                "source": 22,
                "target": 21
            },
            {
                "prerequisite": null,
                "similarity": 0.9999999999999999,
                "source": 22,
                "target": 22
            },
            {
                "prerequisite": null,
                "similarity": 0.348058067569092,
                "source": 22,
                "target": 23
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 22,
                "target": 24
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 22,
                "target": 25
            },
            {
                "prerequisite": null,
                "similarity": 0.35355339059327373,
                "source": 22,
                "target": 26
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 22,
                "target": 27
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 22,
                "target": 28
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 22,
                "target": 29
            },
            {
                "prerequisite": null,
                "similarity": 0.6317708577854666,
                "source": 23,
                "target": 0
            },
            {
                "prerequisite": null,
                "similarity": 0.4072427255082878,
                "source": 23,
                "target": 1
            },
            {
                "prerequisite": null,
                "similarity": 0.5273500981126146,
                "source": 23,
                "target": 2
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 23,
                "target": 3
            },
            {
                "prerequisite": null,
                "similarity": 0.49514516892273003,
                "source": 23,
                "target": 4
            },
            {
                "prerequisite": null,
                "similarity": 0.1386750490563073,
                "source": 23,
                "target": 5
            },
            {
                "prerequisite": null,
                "similarity": 0.19611613513818402,
                "source": 23,
                "target": 6
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 23,
                "target": 7
            },
            {
                "prerequisite": null,
                "similarity": 0.5600868364730212,
                "source": 23,
                "target": 8
            },
            {
                "prerequisite": null,
                "similarity": 0.5681423814878888,
                "source": 23,
                "target": 9
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 23,
                "target": 10
            },
            {
                "prerequisite": null,
                "similarity": 0.41012815380508716,
                "source": 23,
                "target": 11
            },
            {
                "prerequisite": null,
                "similarity": 0.5273500981126146,
                "source": 23,
                "target": 12
            },
            {
                "prerequisite": null,
                "similarity": 0.598058067569092,
                "source": 23,
                "target": 13
            },
            {
                "prerequisite": null,
                "similarity": 0.19611613513818402,
                "source": 23,
                "target": 14
            },
            {
                "prerequisite": null,
                "similarity": 0.7192645048267573,
                "source": 23,
                "target": 15
            },
            {
                "prerequisite": null,
                "similarity": 0.19611613513818402,
                "source": 23,
                "target": 16
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 23,
                "target": 17
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 23,
                "target": 18
            },
            {
                "prerequisite": null,
                "similarity": 0.343203236491822,
                "source": 23,
                "target": 19
            },
            {
                "prerequisite": null,
                "similarity": 0.7080125735844609,
                "source": 23,
                "target": 20
            },
            {
                "prerequisite": null,
                "similarity": 0.5120712091804795,
                "source": 23,
                "target": 21
            },
            {
                "prerequisite": null,
                "similarity": 0.348058067569092,
                "source": 23,
                "target": 22
            },
            {
                "prerequisite": null,
                "similarity": 0.9999999999999999,
                "source": 23,
                "target": 23
            },
            {
                "prerequisite": null,
                "similarity": 0.1386750490563073,
                "source": 23,
                "target": 24
            },
            {
                "prerequisite": null,
                "similarity": 0.31933752452815367,
                "source": 23,
                "target": 25
            },
            {
                "prerequisite": null,
                "similarity": 0.1386750490563073,
                "source": 23,
                "target": 26
            },
            {
                "prerequisite": null,
                "similarity": 0.09805806756909201,
                "source": 23,
                "target": 27
            },
            {
                "prerequisite": null,
                "similarity": 0.1386750490563073,
                "source": 23,
                "target": 28
            },
            {
                "prerequisite": null,
                "similarity": 0.1386750490563073,
                "source": 23,
                "target": 29
            },
            {
                "prerequisite": null,
                "similarity": 0.11470786693528087,
                "source": 24,
                "target": 0
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 24,
                "target": 1
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 24,
                "target": 2
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 24,
                "target": 3
            },
            {
                "prerequisite": null,
                "similarity": 0.17677669529663687,
                "source": 24,
                "target": 4
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 24,
                "target": 5
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 24,
                "target": 6
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 24,
                "target": 7
            },
            {
                "prerequisite": null,
                "similarity": 0.22360679774997896,
                "source": 24,
                "target": 8
            },
            {
                "prerequisite": null,
                "similarity": 0.11470786693528087,
                "source": 24,
                "target": 9
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 24,
                "target": 10
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 24,
                "target": 11
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 24,
                "target": 12
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 24,
                "target": 13
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 24,
                "target": 14
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 24,
                "target": 15
            },
            {
                "prerequisite": null,
                "similarity": 0.35355339059327373,
                "source": 24,
                "target": 16
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 24,
                "target": 17
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 24,
                "target": 18
            },
            {
                "prerequisite": null,
                "similarity": 0.17677669529663687,
                "source": 24,
                "target": 19
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 24,
                "target": 20
            },
            {
                "prerequisite": null,
                "similarity": 0.1889822365046136,
                "source": 24,
                "target": 21
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 24,
                "target": 22
            },
            {
                "prerequisite": null,
                "similarity": 0.1386750490563073,
                "source": 24,
                "target": 23
            },
            {
                "prerequisite": null,
                "similarity": 1.0,
                "source": 24,
                "target": 24
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 24,
                "target": 25
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 24,
                "target": 26
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 24,
                "target": 27
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 24,
                "target": 28
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 24,
                "target": 29
            },
            {
                "prerequisite": null,
                "similarity": 0.7294157338705618,
                "source": 25,
                "target": 0
            },
            {
                "prerequisite": null,
                "similarity": 0.1889822365046136,
                "source": 25,
                "target": 1
            },
            {
                "prerequisite": null,
                "similarity": 0.375,
                "source": 25,
                "target": 2
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 25,
                "target": 3
            },
            {
                "prerequisite": null,
                "similarity": 0.5151650429449552,
                "source": 25,
                "target": 4
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 25,
                "target": 5
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 25,
                "target": 6
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 25,
                "target": 7
            },
            {
                "prerequisite": null,
                "similarity": 0.11180339887498948,
                "source": 25,
                "target": 8
            },
            {
                "prerequisite": null,
                "similarity": 0.4220618004029213,
                "source": 25,
                "target": 9
            },
            {
                "prerequisite": null,
                "similarity": 0.875,
                "source": 25,
                "target": 10
            },
            {
                "prerequisite": null,
                "similarity": 0.14433756729740646,
                "source": 25,
                "target": 11
            },
            {
                "prerequisite": null,
                "similarity": 0.375,
                "source": 25,
                "target": 12
            },
            {
                "prerequisite": null,
                "similarity": 0.42677669529663687,
                "source": 25,
                "target": 13
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 25,
                "target": 14
            },
            {
                "prerequisite": null,
                "similarity": 0.3290569415042095,
                "source": 25,
                "target": 15
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 25,
                "target": 16
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 25,
                "target": 17
            },
            {
                "prerequisite": null,
                "similarity": 0.7886751345948129,
                "source": 25,
                "target": 18
            },
            {
                "prerequisite": null,
                "similarity": 0.08838834764831843,
                "source": 25,
                "target": 19
            },
            {
                "prerequisite": null,
                "similarity": 0.125,
                "source": 25,
                "target": 20
            },
            {
                "prerequisite": null,
                "similarity": 0.5944911182523068,
                "source": 25,
                "target": 21
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 25,
                "target": 22
            },
            {
                "prerequisite": null,
                "similarity": 0.31933752452815367,
                "source": 25,
                "target": 23
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 25,
                "target": 24
            },
            {
                "prerequisite": null,
                "similarity": 1.0,
                "source": 25,
                "target": 25
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 25,
                "target": 26
            },
            {
                "prerequisite": null,
                "similarity": 0.17677669529663687,
                "source": 25,
                "target": 27
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 25,
                "target": 28
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 25,
                "target": 29
            },
            {
                "prerequisite": null,
                "similarity": 0.47941573387056174,
                "source": 26,
                "target": 0
            },
            {
                "prerequisite": null,
                "similarity": 0.6279644730092272,
                "source": 26,
                "target": 1
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 26,
                "target": 2
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 26,
                "target": 3
            },
            {
                "prerequisite": null,
                "similarity": 0.08838834764831843,
                "source": 26,
                "target": 4
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 26,
                "target": 5
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 26,
                "target": 6
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 26,
                "target": 7
            },
            {
                "prerequisite": null,
                "similarity": 0.11180339887498948,
                "source": 26,
                "target": 8
            },
            {
                "prerequisite": null,
                "similarity": 0.22941573387056174,
                "source": 26,
                "target": 9
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 26,
                "target": 10
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 26,
                "target": 11
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 26,
                "target": 12
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 26,
                "target": 13
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 26,
                "target": 14
            },
            {
                "prerequisite": null,
                "similarity": 0.23717082451262844,
                "source": 26,
                "target": 15
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 26,
                "target": 16
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 26,
                "target": 17
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 26,
                "target": 18
            },
            {
                "prerequisite": null,
                "similarity": 0.17677669529663687,
                "source": 26,
                "target": 19
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 26,
                "target": 20
            },
            {
                "prerequisite": null,
                "similarity": 0.0944911182523068,
                "source": 26,
                "target": 21
            },
            {
                "prerequisite": null,
                "similarity": 0.35355339059327373,
                "source": 26,
                "target": 22
            },
            {
                "prerequisite": null,
                "similarity": 0.1386750490563073,
                "source": 26,
                "target": 23
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 26,
                "target": 24
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 26,
                "target": 25
            },
            {
                "prerequisite": null,
                "similarity": 1.0,
                "source": 26,
                "target": 26
            },
            {
                "prerequisite": null,
                "similarity": 0.17677669529663687,
                "source": 26,
                "target": 27
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 26,
                "target": 28
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 26,
                "target": 29
            },
            {
                "prerequisite": null,
                "similarity": 0.16222142113076252,
                "source": 27,
                "target": 0
            },
            {
                "prerequisite": null,
                "similarity": 0.13363062095621217,
                "source": 27,
                "target": 1
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 27,
                "target": 2
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 27,
                "target": 3
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 27,
                "target": 4
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 27,
                "target": 5
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 27,
                "target": 6
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 27,
                "target": 7
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 27,
                "target": 8
            },
            {
                "prerequisite": null,
                "similarity": 0.16222142113076252,
                "source": 27,
                "target": 9
            },
            {
                "prerequisite": null,
                "similarity": 0.17677669529663687,
                "source": 27,
                "target": 10
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 27,
                "target": 11
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 27,
                "target": 12
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 27,
                "target": 13
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 27,
                "target": 14
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 27,
                "target": 15
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 27,
                "target": 16
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 27,
                "target": 17
            },
            {
                "prerequisite": null,
                "similarity": 0.4541241452319315,
                "source": 27,
                "target": 18
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 27,
                "target": 19
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 27,
                "target": 20
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 27,
                "target": 21
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 27,
                "target": 22
            },
            {
                "prerequisite": null,
                "similarity": 0.09805806756909201,
                "source": 27,
                "target": 23
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 27,
                "target": 24
            },
            {
                "prerequisite": null,
                "similarity": 0.17677669529663687,
                "source": 27,
                "target": 25
            },
            {
                "prerequisite": null,
                "similarity": 0.17677669529663687,
                "source": 27,
                "target": 26
            },
            {
                "prerequisite": null,
                "similarity": 0.9999999999999999,
                "source": 27,
                "target": 27
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 27,
                "target": 28
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 27,
                "target": 29
            },
            {
                "prerequisite": null,
                "similarity": 0.11470786693528087,
                "source": 28,
                "target": 0
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 28,
                "target": 1
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 28,
                "target": 2
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 28,
                "target": 3
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 28,
                "target": 4
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 28,
                "target": 5
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 28,
                "target": 6
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 28,
                "target": 7
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 28,
                "target": 8
            },
            {
                "prerequisite": null,
                "similarity": 0.11470786693528087,
                "source": 28,
                "target": 9
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 28,
                "target": 10
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 28,
                "target": 11
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 28,
                "target": 12
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 28,
                "target": 13
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 28,
                "target": 14
            },
            {
                "prerequisite": null,
                "similarity": 0.15811388300841897,
                "source": 28,
                "target": 15
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 28,
                "target": 16
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 28,
                "target": 17
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 28,
                "target": 18
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 28,
                "target": 19
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 28,
                "target": 20
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 28,
                "target": 21
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 28,
                "target": 22
            },
            {
                "prerequisite": null,
                "similarity": 0.1386750490563073,
                "source": 28,
                "target": 23
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 28,
                "target": 24
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 28,
                "target": 25
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 28,
                "target": 26
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 28,
                "target": 27
            },
            {
                "prerequisite": null,
                "similarity": 1.0,
                "source": 28,
                "target": 28
            },
            {
                "prerequisite": 0.10735928988584309,
                "similarity": 0.5,
                "source": 28,
                "target": 29
            },
            {
                "prerequisite": null,
                "similarity": 0.11470786693528087,
                "source": 29,
                "target": 0
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 29,
                "target": 1
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 29,
                "target": 2
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 29,
                "target": 3
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 29,
                "target": 4
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 29,
                "target": 5
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 29,
                "target": 6
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 29,
                "target": 7
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 29,
                "target": 8
            },
            {
                "prerequisite": null,
                "similarity": 0.11470786693528087,
                "source": 29,
                "target": 9
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 29,
                "target": 10
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 29,
                "target": 11
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 29,
                "target": 12
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 29,
                "target": 13
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 29,
                "target": 14
            },
            {
                "prerequisite": null,
                "similarity": 0.15811388300841897,
                "source": 29,
                "target": 15
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 29,
                "target": 16
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 29,
                "target": 17
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 29,
                "target": 18
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 29,
                "target": 19
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 29,
                "target": 20
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 29,
                "target": 21
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 29,
                "target": 22
            },
            {
                "prerequisite": null,
                "similarity": 0.1386750490563073,
                "source": 29,
                "target": 23
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 29,
                "target": 24
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 29,
                "target": 25
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 29,
                "target": 26
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 29,
                "target": 27
            },
            {
                "prerequisite": null,
                "similarity": 0.5,
                "source": 29,
                "target": 28
            },
            {
                "prerequisite": null,
                "similarity": 1.0,
                "source": 29,
                "target": 29
            }
        ],
        "nodes": [
            {
                "conceptCountForEachVid": {
                    "-zteIdpQ5UE": 1,
                    "3ESNTxor_VM": 2,
                    "3Q8wacwA4gs": 2,
                    "5ctbvkAMQO4": 13,
                    "8FGHDBwoPVo": 7,
                    "AJVP96tAWxw": 1,
                    "BYdtJs6wU5o": 1,
                    "FLZvOKSCkxY": 7,
                    "FggN0VtWYTk": 1,
                    "J5IlKj7H8T8": 2,
                    "MNvT5JekDpg": 2,
                    "PpGbujtnm1k": 2,
                    "QIdB6M5WdkI": 6,
                    "ReakZVh2Xwk": 1,
                    "Sx3Fpw0XCXk": 1,
                    "d4gGtcobq8M": 2,
                    "fOvTtapxa9c": 1,
                    "iGmHnICXDss": 4,
                    "imPpT2Qo2sk": 1
                },
                "count": 57,
                "group": 1,
                "index": 0,
                "name": "natural language processing",
                "videos_id": [
                    [
                        "5ctbvkAMQO4",
                        13
                    ],
                    [
                        "FLZvOKSCkxY",
                        7
                    ],
                    [
                        "8FGHDBwoPVo",
                        7
                    ],
                    [
                        "QIdB6M5WdkI",
                        6
                    ],
                    [
                        "iGmHnICXDss",
                        4
                    ],
                    [
                        "3ESNTxor_VM",
                        2
                    ]
                ]
            },
            {
                "conceptCountForEachVid": {
                    "3ESNTxor_VM": 3,
                    "5ctbvkAMQO4": 1,
                    "BYdtJs6wU5o": 20,
                    "FLZvOKSCkxY": 2,
                    "fOvTtapxa9c": 2,
                    "imPpT2Qo2sk": 4,
                    "ohM7D21C_8Q": 1
                },
                "count": 33,
                "group": 1,
                "index": 1,
                "name": "part of speech",
                "videos_id": [
                    [
                        "BYdtJs6wU5o",
                        20
                    ],
                    [
                        "imPpT2Qo2sk",
                        4
                    ],
                    [
                        "3ESNTxor_VM",
                        3
                    ],
                    [
                        "FLZvOKSCkxY",
                        2
                    ],
                    [
                        "fOvTtapxa9c",
                        2
                    ],
                    [
                        "5ctbvkAMQO4",
                        1
                    ]
                ]
            },
            {
                "conceptCountForEachVid": {
                    "5ctbvkAMQO4": 2,
                    "8FGHDBwoPVo": 5,
                    "J5IlKj7H8T8": 7,
                    "PpGbujtnm1k": 8
                },
                "count": 22,
                "group": 1,
                "index": 2,
                "name": "natural language understanding",
                "videos_id": [
                    [
                        "PpGbujtnm1k",
                        8
                    ],
                    [
                        "J5IlKj7H8T8",
                        7
                    ],
                    [
                        "8FGHDBwoPVo",
                        5
                    ],
                    [
                        "5ctbvkAMQO4",
                        2
                    ]
                ]
            },
            {
                "conceptCountForEachVid": {
                    "UeiUiCRchiU": 10
                },
                "count": 10,
                "group": 1,
                "index": 3,
                "name": "box cutter",
                "videos_id": [
                    [
                        "UeiUiCRchiU",
                        10
                    ]
                ]
            },
            {
                "conceptCountForEachVid": {
                    "3Q8wacwA4gs": 1,
                    "5ctbvkAMQO4": 1,
                    "AJVP96tAWxw": 1,
                    "BYdtJs6wU5o": 1,
                    "FggN0VtWYTk": 1,
                    "J5IlKj7H8T8": 1,
                    "PpGbujtnm1k": 1,
                    "fOvTtapxa9c": 2
                },
                "count": 9,
                "group": 1,
                "index": 4,
                "name": "human language",
                "videos_id": [
                    [
                        "fOvTtapxa9c",
                        2
                    ],
                    [
                        "FggN0VtWYTk",
                        1
                    ],
                    [
                        "3Q8wacwA4gs",
                        1
                    ],
                    [
                        "AJVP96tAWxw",
                        1
                    ],
                    [
                        "5ctbvkAMQO4",
                        1
                    ],
                    [
                        "BYdtJs6wU5o",
                        1
                    ]
                ]
            },
            {
                "conceptCountForEachVid": {
                    "ReakZVh2Xwk": 18
                },
                "count": 18,
                "group": 1,
                "index": 5,
                "name": "pickle",
                "videos_id": [
                    [
                        "ReakZVh2Xwk",
                        18
                    ]
                ]
            },
            {
                "conceptCountForEachVid": {
                    "J5IlKj7H8T8": 4,
                    "PpGbujtnm1k": 5
                },
                "count": 9,
                "group": 1,
                "index": 6,
                "name": "natural language understanding service",
                "videos_id": [
                    [
                        "PpGbujtnm1k",
                        5
                    ],
                    [
                        "J5IlKj7H8T8",
                        4
                    ]
                ]
            },
            {
                "conceptCountForEachVid": {
                    "Sx3Fpw0XCXk": 3
                },
                "count": 3,
                "group": 1,
                "index": 7,
                "name": "google wave",
                "videos_id": [
                    [
                        "Sx3Fpw0XCXk",
                        3
                    ]
                ]
            },
            {
                "conceptCountForEachVid": {
                    "-zteIdpQ5UE": 2,
                    "3Q8wacwA4gs": 1,
                    "5ctbvkAMQO4": 1,
                    "AJVP96tAWxw": 5,
                    "FLZvOKSCkxY": 4
                },
                "count": 13,
                "group": 1,
                "index": 8,
                "name": "sentiment analysis",
                "videos_id": [
                    [
                        "AJVP96tAWxw",
                        5
                    ],
                    [
                        "FLZvOKSCkxY",
                        4
                    ],
                    [
                        "-zteIdpQ5UE",
                        2
                    ],
                    [
                        "3Q8wacwA4gs",
                        1
                    ],
                    [
                        "5ctbvkAMQO4",
                        1
                    ]
                ]
            },
            {
                "conceptCountForEachVid": {
                    "3ESNTxor_VM": 5,
                    "3Q8wacwA4gs": 6,
                    "4kyJVwew0lg": 2,
                    "5ctbvkAMQO4": 15,
                    "8FGHDBwoPVo": 10,
                    "AJVP96tAWxw": 15,
                    "BYdtJs6wU5o": 36,
                    "FLZvOKSCkxY": 20,
                    "MNvT5JekDpg": 4,
                    "QIdB6M5WdkI": 22,
                    "ReakZVh2Xwk": 2,
                    "Sx3Fpw0XCXk": 7,
                    "UeiUiCRchiU": 3,
                    "d4gGtcobq8M": 6,
                    "fOvTtapxa9c": 15,
                    "iGmHnICXDss": 34,
                    "imPpT2Qo2sk": 5,
                    "ohM7D21C_8Q": 3,
                    "yGKTphqxR9Q": 27
                },
                "count": 237,
                "group": 1,
                "index": 9,
                "name": "word",
                "videos_id": [
                    [
                        "BYdtJs6wU5o",
                        36
                    ],
                    [
                        "iGmHnICXDss",
                        34
                    ],
                    [
                        "yGKTphqxR9Q",
                        27
                    ],
                    [
                        "QIdB6M5WdkI",
                        22
                    ],
                    [
                        "FLZvOKSCkxY",
                        20
                    ],
                    [
                        "AJVP96tAWxw",
                        15
                    ],
                    [
                        "4kyJVwew0lg",
                        2
                    ]
                ]
            },
            {
                "conceptCountForEachVid": {
                    "FggN0VtWYTk": 1,
                    "MNvT5JekDpg": 8,
                    "fOvTtapxa9c": 1,
                    "iGmHnICXDss": 1
                },
                "count": 11,
                "group": 1,
                "index": 10,
                "name": "neural network",
                "videos_id": [
                    [
                        "MNvT5JekDpg",
                        8
                    ],
                    [
                        "iGmHnICXDss",
                        1
                    ],
                    [
                        "FggN0VtWYTk",
                        1
                    ],
                    [
                        "fOvTtapxa9c",
                        1
                    ]
                ]
            },
            {
                "conceptCountForEachVid": {
                    "5ctbvkAMQO4": 2,
                    "8FGHDBwoPVo": 3,
                    "MNvT5JekDpg": 8
                },
                "count": 13,
                "group": 1,
                "index": 11,
                "name": "natural language generation",
                "videos_id": [
                    [
                        "MNvT5JekDpg",
                        8
                    ],
                    [
                        "8FGHDBwoPVo",
                        3
                    ],
                    [
                        "5ctbvkAMQO4",
                        2
                    ]
                ]
            },
            {
                "conceptCountForEachVid": {
                    "-zteIdpQ5UE": 1,
                    "5ctbvkAMQO4": 1,
                    "8FGHDBwoPVo": 3,
                    "QIdB6M5WdkI": 2
                },
                "count": 7,
                "group": 1,
                "index": 12,
                "name": "artificial intelligence",
                "videos_id": [
                    [
                        "8FGHDBwoPVo",
                        3
                    ],
                    [
                        "QIdB6M5WdkI",
                        2
                    ],
                    [
                        "-zteIdpQ5UE",
                        1
                    ],
                    [
                        "5ctbvkAMQO4",
                        1
                    ]
                ]
            },
            {
                "conceptCountForEachVid": {
                    "QIdB6M5WdkI": 3,
                    "fOvTtapxa9c": 3
                },
                "count": 6,
                "group": 1,
                "index": 13,
                "name": "computer science",
                "videos_id": [
                    [
                        "QIdB6M5WdkI",
                        3
                    ],
                    [
                        "fOvTtapxa9c",
                        3
                    ]
                ]
            },
            {
                "conceptCountForEachVid": {
                    "J5IlKj7H8T8": 2,
                    "PpGbujtnm1k": 2
                },
                "count": 4,
                "group": 1,
                "index": 14,
                "name": "ibm watson",
                "videos_id": [
                    [
                        "PpGbujtnm1k",
                        2
                    ],
                    [
                        "J5IlKj7H8T8",
                        2
                    ]
                ]
            },
            {
                "conceptCountForEachVid": {
                    "-zteIdpQ5UE": 1,
                    "BYdtJs6wU5o": 1,
                    "FLZvOKSCkxY": 2,
                    "MNvT5JekDpg": 7,
                    "ReakZVh2Xwk": 1,
                    "d4gGtcobq8M": 1,
                    "fOvTtapxa9c": 3,
                    "imPpT2Qo2sk": 6,
                    "ohM7D21C_8Q": 30,
                    "yGKTphqxR9Q": 1
                },
                "count": 53,
                "group": 1,
                "index": 15,
                "name": "question",
                "videos_id": [
                    [
                        "ohM7D21C_8Q",
                        30
                    ],
                    [
                        "MNvT5JekDpg",
                        7
                    ],
                    [
                        "imPpT2Qo2sk",
                        6
                    ],
                    [
                        "fOvTtapxa9c",
                        3
                    ],
                    [
                        "FLZvOKSCkxY",
                        2
                    ],
                    [
                        "-zteIdpQ5UE",
                        1
                    ]
                ]
            },
            {
                "conceptCountForEachVid": {
                    "AJVP96tAWxw": 6,
                    "ReakZVh2Xwk": 16
                },
                "count": 22,
                "group": 1,
                "index": 16,
                "name": "classifier",
                "videos_id": [
                    [
                        "ReakZVh2Xwk",
                        16
                    ],
                    [
                        "AJVP96tAWxw",
                        6
                    ]
                ]
            },
            {
                "conceptCountForEachVid": {
                    "UeiUiCRchiU": 5
                },
                "count": 5,
                "group": 1,
                "index": 17,
                "name": "good thing",
                "videos_id": [
                    [
                        "UeiUiCRchiU",
                        5
                    ]
                ]
            },
            {
                "conceptCountForEachVid": {
                    "Sx3Fpw0XCXk": 3,
                    "fOvTtapxa9c": 2,
                    "iGmHnICXDss": 11
                },
                "count": 16,
                "group": 1,
                "index": 18,
                "name": "language model",
                "videos_id": [
                    [
                        "iGmHnICXDss",
                        11
                    ],
                    [
                        "Sx3Fpw0XCXk",
                        3
                    ],
                    [
                        "fOvTtapxa9c",
                        2
                    ]
                ]
            },
            {
                "conceptCountForEachVid": {
                    "-zteIdpQ5UE": 2,
                    "5ctbvkAMQO4": 1,
                    "8FGHDBwoPVo": 1,
                    "AJVP96tAWxw": 2,
                    "FLZvOKSCkxY": 20,
                    "ReakZVh2Xwk": 5,
                    "imPpT2Qo2sk": 3,
                    "yGKTphqxR9Q": 24
                },
                "count": 58,
                "group": 1,
                "index": 19,
                "name": "python",
                "videos_id": [
                    [
                        "yGKTphqxR9Q",
                        24
                    ],
                    [
                        "FLZvOKSCkxY",
                        20
                    ],
                    [
                        "ReakZVh2Xwk",
                        5
                    ],
                    [
                        "imPpT2Qo2sk",
                        3
                    ],
                    [
                        "-zteIdpQ5UE",
                        2
                    ],
                    [
                        "AJVP96tAWxw",
                        2
                    ]
                ]
            },
            {
                "conceptCountForEachVid": {
                    "-zteIdpQ5UE": 11,
                    "AJVP96tAWxw": 5,
                    "QIdB6M5WdkI": 2,
                    "fOvTtapxa9c": 1
                },
                "count": 19,
                "group": 1,
                "index": 20,
                "name": "data set",
                "videos_id": [
                    [
                        "-zteIdpQ5UE",
                        11
                    ],
                    [
                        "AJVP96tAWxw",
                        5
                    ],
                    [
                        "QIdB6M5WdkI",
                        2
                    ],
                    [
                        "fOvTtapxa9c",
                        1
                    ]
                ]
            },
            {
                "conceptCountForEachVid": {
                    "-zteIdpQ5UE": 1,
                    "8FGHDBwoPVo": 1,
                    "AJVP96tAWxw": 4,
                    "FLZvOKSCkxY": 1,
                    "MNvT5JekDpg": 5,
                    "fOvTtapxa9c": 2,
                    "yGKTphqxR9Q": 1
                },
                "count": 15,
                "group": 1,
                "index": 21,
                "name": "machine learning",
                "videos_id": [
                    [
                        "MNvT5JekDpg",
                        5
                    ],
                    [
                        "AJVP96tAWxw",
                        4
                    ],
                    [
                        "fOvTtapxa9c",
                        2
                    ],
                    [
                        "-zteIdpQ5UE",
                        1
                    ],
                    [
                        "yGKTphqxR9Q",
                        1
                    ],
                    [
                        "FLZvOKSCkxY",
                        1
                    ]
                ]
            },
            {
                "conceptCountForEachVid": {
                    "FLZvOKSCkxY": 3,
                    "imPpT2Qo2sk": 8
                },
                "count": 11,
                "group": 1,
                "index": 22,
                "name": "regular expression",
                "videos_id": [
                    [
                        "imPpT2Qo2sk",
                        8
                    ],
                    [
                        "FLZvOKSCkxY",
                        3
                    ]
                ]
            },
            {
                "conceptCountForEachVid": {
                    "-zteIdpQ5UE": 2,
                    "3ESNTxor_VM": 1,
                    "3Q8wacwA4gs": 7,
                    "5ctbvkAMQO4": 6,
                    "8FGHDBwoPVo": 8,
                    "AJVP96tAWxw": 5,
                    "FLZvOKSCkxY": 6,
                    "J5IlKj7H8T8": 1,
                    "PpGbujtnm1k": 1,
                    "QIdB6M5WdkI": 1,
                    "ReakZVh2Xwk": 1,
                    "d4gGtcobq8M": 2,
                    "yGKTphqxR9Q": 4
                },
                "count": 45,
                "group": 1,
                "index": 23,
                "name": "analysis",
                "videos_id": [
                    [
                        "8FGHDBwoPVo",
                        8
                    ],
                    [
                        "3Q8wacwA4gs",
                        7
                    ],
                    [
                        "5ctbvkAMQO4",
                        6
                    ],
                    [
                        "FLZvOKSCkxY",
                        6
                    ],
                    [
                        "AJVP96tAWxw",
                        5
                    ],
                    [
                        "yGKTphqxR9Q",
                        4
                    ]
                ]
            },
            {
                "conceptCountForEachVid": {
                    "AJVP96tAWxw": 6
                },
                "count": 6,
                "group": 1,
                "index": 24,
                "name": "testing data",
                "videos_id": [
                    [
                        "AJVP96tAWxw",
                        6
                    ]
                ]
            },
            {
                "conceptCountForEachVid": {
                    "5ctbvkAMQO4": 1,
                    "FggN0VtWYTk": 1,
                    "fOvTtapxa9c": 6,
                    "iGmHnICXDss": 1
                },
                "count": 9,
                "group": 1,
                "index": 25,
                "name": "speech recognition",
                "videos_id": [
                    [
                        "fOvTtapxa9c",
                        6
                    ],
                    [
                        "iGmHnICXDss",
                        1
                    ],
                    [
                        "FggN0VtWYTk",
                        1
                    ],
                    [
                        "5ctbvkAMQO4",
                        1
                    ]
                ]
            },
            {
                "conceptCountForEachVid": {
                    "3ESNTxor_VM": 2,
                    "BYdtJs6wU5o": 3,
                    "FLZvOKSCkxY": 1,
                    "imPpT2Qo2sk": 3
                },
                "count": 9,
                "group": 1,
                "index": 26,
                "name": "speech tagging",
                "videos_id": [
                    [
                        "imPpT2Qo2sk",
                        3
                    ],
                    [
                        "BYdtJs6wU5o",
                        3
                    ],
                    [
                        "3ESNTxor_VM",
                        2
                    ],
                    [
                        "FLZvOKSCkxY",
                        1
                    ]
                ]
            },
            {
                "conceptCountForEachVid": {
                    "3ESNTxor_VM": 1,
                    "iGmHnICXDss": 6
                },
                "count": 7,
                "group": 1,
                "index": 27,
                "name": "previous word",
                "videos_id": [
                    [
                        "iGmHnICXDss",
                        6
                    ],
                    [
                        "3ESNTxor_VM",
                        1
                    ]
                ]
            },
            {
                "conceptCountForEachVid": {
                    "d4gGtcobq8M": 3
                },
                "count": 3,
                "group": 1,
                "index": 28,
                "name": "spark cognition",
                "videos_id": [
                    [
                        "d4gGtcobq8M",
                        3
                    ]
                ]
            },
            {
                "conceptCountForEachVid": {
                    "d4gGtcobq8M": 3
                },
                "count": 3,
                "group": 1,
                "index": 29,
                "name": "injury report",
                "videos_id": [
                    [
                        "d4gGtcobq8M",
                        3
                    ]
                ]
            }
        ]
    },
    "concept_sequences": {
        "0": [
            9,
            0
        ],
        "1": [
            9,
            0,
            1
        ],
        "2": [
            9,
            0,
            15,
            23,
            4,
            2
        ],
        "3": [],
        "4": [
            9,
            0,
            15,
            23,
            4
        ],
        "5": [
            19,
            5
        ],
        "6": [],
        "7": [],
        "8": [
            9,
            0,
            15,
            23,
            21,
            8
        ],
        "9": [],
        "10": [
            9,
            0,
            15,
            23,
            21,
            10
        ],
        "11": [
            9,
            0,
            15,
            23,
            21,
            4,
            10,
            8,
            2,
            12,
            11
        ],
        "12": [
            9,
            0,
            15,
            23,
            21,
            4,
            10,
            8,
            2,
            12
        ],
        "13": [
            9,
            0,
            15,
            23,
            21,
            4,
            10,
            8,
            2,
            12,
            13
        ],
        "14": [
            9,
            0,
            15,
            23,
            4,
            6,
            2,
            14
        ],
        "15": [
            9,
            0,
            15
        ],
        "16": [],
        "17": [
            3,
            17
        ],
        "18": [
            9,
            0,
            15,
            23,
            21,
            27,
            10,
            18
        ],
        "19": [],
        "20": [
            9,
            0,
            15,
            23,
            21,
            4,
            10,
            8,
            2,
            12,
            20
        ],
        "21": [
            9,
            0,
            15,
            23,
            21
        ],
        "22": [
            9,
            0,
            19,
            15,
            22
        ],
        "23": [
            9,
            0,
            15,
            23
        ],
        "24": [],
        "25": [
            9,
            0,
            15,
            23,
            21,
            4,
            10,
            8,
            2,
            27,
            12,
            18,
            13,
            25
        ],
        "26": [
            9,
            0,
            1,
            26
        ],
        "27": [],
        "28": [],
        "29": [
            28,
            29
        ]
    },
    "highlight_nodes": {
        "0": [
            0,
            9
        ],
        "1": [
            0,
            1,
            9
        ],
        "2": [
            0,
            2,
            4,
            9,
            15,
            23
        ],
        "3": [
            3
        ],
        "4": [
            0,
            9,
            4,
            15,
            23
        ],
        "5": [
            19,
            5
        ],
        "6": [
            6
        ],
        "7": [
            7
        ],
        "8": [
            0,
            8,
            9,
            15,
            21,
            23
        ],
        "9": [
            9
        ],
        "10": [
            0,
            9,
            10,
            15,
            21,
            23
        ],
        "11": [
            0,
            2,
            4,
            8,
            9,
            10,
            11,
            12,
            15,
            21,
            23
        ],
        "12": [
            0,
            2,
            4,
            8,
            9,
            10,
            12,
            15,
            21,
            23
        ],
        "13": [
            0,
            2,
            4,
            8,
            9,
            10,
            12,
            13,
            15,
            21,
            23
        ],
        "14": [
            0,
            2,
            4,
            6,
            9,
            14,
            15,
            23
        ],
        "15": [
            0,
            9,
            15
        ],
        "16": [
            16
        ],
        "17": [
            17,
            3
        ],
        "18": [
            0,
            9,
            10,
            15,
            18,
            21,
            23,
            27
        ],
        "19": [
            19
        ],
        "20": [
            0,
            2,
            4,
            8,
            9,
            10,
            12,
            15,
            20,
            21,
            23
        ],
        "21": [
            0,
            9,
            23,
            21,
            15
        ],
        "22": [
            0,
            9,
            19,
            22,
            15
        ],
        "23": [
            0,
            9,
            15,
            23
        ],
        "24": [
            24
        ],
        "25": [
            0,
            2,
            4,
            8,
            9,
            10,
            12,
            13,
            15,
            18,
            21,
            23,
            25,
            27
        ],
        "26": [
            0,
            1,
            26,
            9
        ],
        "27": [
            27
        ],
        "28": [
            28
        ],
        "29": [
            28,
            29
        ]
    },
    "search_info": {
        "NumOfVideos": 23,
        "key": "natural_language_processing_50",
        "similarity_threshold": 0.41222142113076254,
        "time_delta": 15.283333333333333,
        "voclist_SelectMethod": 0
    },
    "video_sequences": {
        "0": [
            [
                "BYdtJs6wU5o",
                0.5
            ],
            [
                "5ctbvkAMQO4",
                0.5
            ]
        ],
        "1": [
            [
                "BYdtJs6wU5o",
                1.0
            ],
            [
                "5ctbvkAMQO4",
                1.0
            ],
            [
                "imPpT2Qo2sk",
                1.0
            ]
        ],
        "2": [
            [
                "ohM7D21C_8Q",
                1.0
            ],
            [
                "BYdtJs6wU5o",
                1.75
            ],
            [
                "fOvTtapxa9c",
                1.75
            ],
            [
                "8FGHDBwoPVo",
                2.25
            ],
            [
                "5ctbvkAMQO4",
                2.6
            ],
            [
                "J5IlKj7H8T8",
                3.25
            ]
        ],
        "4": [
            [
                "ohM7D21C_8Q",
                1.0
            ],
            [
                "8FGHDBwoPVo",
                1.3333333333333333
            ],
            [
                "BYdtJs6wU5o",
                1.75
            ],
            [
                "fOvTtapxa9c",
                1.75
            ],
            [
                "5ctbvkAMQO4",
                2.0
            ]
        ],
        "5": [
            [
                "yGKTphqxR9Q",
                0.0
            ],
            [
                "ReakZVh2Xwk",
                0.5
            ]
        ],
        "8": [
            [
                "BYdtJs6wU5o",
                1.0
            ],
            [
                "ohM7D21C_8Q",
                1.0
            ],
            [
                "MNvT5JekDpg",
                1.75
            ],
            [
                "8FGHDBwoPVo",
                2.0
            ],
            [
                "5ctbvkAMQO4",
                2.25
            ],
            [
                "AJVP96tAWxw",
                2.6
            ]
        ],
        "10": [
            [
                "BYdtJs6wU5o",
                1.0
            ],
            [
                "ohM7D21C_8Q",
                1.0
            ],
            [
                "5ctbvkAMQO4",
                1.3333333333333333
            ],
            [
                "8FGHDBwoPVo",
                2.0
            ],
            [
                "MNvT5JekDpg",
                2.4
            ]
        ],
        "11": [
            [
                "ohM7D21C_8Q",
                1.0
            ],
            [
                "BYdtJs6wU5o",
                2.0
            ],
            [
                "fOvTtapxa9c",
                3.0
            ],
            [
                "AJVP96tAWxw",
                3.3333333333333335
            ],
            [
                "MNvT5JekDpg",
                3.8333333333333335
            ],
            [
                "J5IlKj7H8T8",
                4.25
            ],
            [
                "8FGHDBwoPVo",
                5.0
            ],
            [
                "5ctbvkAMQO4",
                5.375
            ]
        ],
        "12": [
            [
                "ohM7D21C_8Q",
                1.0
            ],
            [
                "BYdtJs6wU5o",
                2.0
            ],
            [
                "MNvT5JekDpg",
                2.6
            ],
            [
                "fOvTtapxa9c",
                3.0
            ],
            [
                "AJVP96tAWxw",
                3.3333333333333335
            ],
            [
                "8FGHDBwoPVo",
                4.166666666666667
            ],
            [
                "J5IlKj7H8T8",
                4.25
            ],
            [
                "5ctbvkAMQO4",
                4.714285714285714
            ]
        ],
        "13": [
            [
                "ohM7D21C_8Q",
                1.0
            ],
            [
                "BYdtJs6wU5o",
                2.0
            ],
            [
                "MNvT5JekDpg",
                2.6
            ],
            [
                "AJVP96tAWxw",
                3.3333333333333335
            ],
            [
                "fOvTtapxa9c",
                4.0
            ],
            [
                "8FGHDBwoPVo",
                4.166666666666667
            ],
            [
                "J5IlKj7H8T8",
                4.25
            ],
            [
                "5ctbvkAMQO4",
                4.714285714285714
            ]
        ],
        "14": [
            [
                "ohM7D21C_8Q",
                1.0
            ],
            [
                "BYdtJs6wU5o",
                1.75
            ],
            [
                "fOvTtapxa9c",
                1.75
            ],
            [
                "8FGHDBwoPVo",
                2.5
            ],
            [
                "5ctbvkAMQO4",
                2.8
            ],
            [
                "J5IlKj7H8T8",
                4.333333333333333
            ]
        ],
        "15": [
            [
                "5ctbvkAMQO4",
                0.5
            ],
            [
                "BYdtJs6wU5o",
                1.0
            ],
            [
                "ohM7D21C_8Q",
                1.0
            ]
        ],
        "17": [
            [
                "UeiUiCRchiU",
                0.5
            ]
        ],
        "18": [
            [
                "BYdtJs6wU5o",
                1.0
            ],
            [
                "ohM7D21C_8Q",
                1.0
            ],
            [
                "5ctbvkAMQO4",
                1.3333333333333333
            ],
            [
                "8FGHDBwoPVo",
                2.0
            ],
            [
                "MNvT5JekDpg",
                2.6
            ],
            [
                "Sx3Fpw0XCXk",
                2.6666666666666665
            ],
            [
                "iGmHnICXDss",
                3.8
            ]
        ],
        "20": [
            [
                "ohM7D21C_8Q",
                1.0
            ],
            [
                "BYdtJs6wU5o",
                2.0
            ],
            [
                "MNvT5JekDpg",
                2.6
            ],
            [
                "fOvTtapxa9c",
                4.0
            ],
            [
                "8FGHDBwoPVo",
                4.166666666666667
            ],
            [
                "J5IlKj7H8T8",
                4.25
            ],
            [
                "AJVP96tAWxw",
                4.285714285714286
            ],
            [
                "5ctbvkAMQO4",
                4.714285714285714
            ]
        ],
        "21": [
            [
                "BYdtJs6wU5o",
                1.0
            ],
            [
                "ohM7D21C_8Q",
                1.0
            ],
            [
                "5ctbvkAMQO4",
                1.3333333333333333
            ],
            [
                "MNvT5JekDpg",
                1.75
            ],
            [
                "8FGHDBwoPVo",
                2.0
            ]
        ],
        "22": [
            [
                "5ctbvkAMQO4",
                1.0
            ],
            [
                "BYdtJs6wU5o",
                1.3333333333333333
            ],
            [
                "ohM7D21C_8Q",
                1.5
            ],
            [
                "yGKTphqxR9Q",
                1.6666666666666667
            ],
            [
                "imPpT2Qo2sk",
                2.0
            ]
        ],
        "23": [
            [
                "BYdtJs6wU5o",
                1.0
            ],
            [
                "ohM7D21C_8Q",
                1.0
            ],
            [
                "5ctbvkAMQO4",
                1.3333333333333333
            ],
            [
                "8FGHDBwoPVo",
                1.3333333333333333
            ]
        ],
        "25": [
            [
                "ohM7D21C_8Q",
                1.0
            ],
            [
                "BYdtJs6wU5o",
                2.0
            ],
            [
                "MNvT5JekDpg",
                2.6
            ],
            [
                "AJVP96tAWxw",
                3.3333333333333335
            ],
            [
                "Sx3Fpw0XCXk",
                4.0
            ],
            [
                "J5IlKj7H8T8",
                4.25
            ],
            [
                "8FGHDBwoPVo",
                4.333333333333333
            ],
            [
                "5ctbvkAMQO4",
                5.875
            ],
            [
                "fOvTtapxa9c",
                6.0
            ],
            [
                "iGmHnICXDss",
                6.666666666666667
            ]
        ],
        "26": [
            [
                "5ctbvkAMQO4",
                1.0
            ],
            [
                "BYdtJs6wU5o",
                1.5
            ],
            [
                "imPpT2Qo2sk",
                1.5
            ]
        ],
        "29": [
            [
                "d4gGtcobq8M",
                0.5
            ]
        ]
    },
    "videos_info": {
        "-zteIdpQ5UE": {
            "NumOfComments": 0,
            "caption_exist": "T",
            "channel_id": "UCNqT-aH62gCVWrCOlfNoNig",
            "channel_title": "Gabriel Villazan Impastato",
            "comment_sentiment": 0,
            "concepts": [
                [
                    "text",
                    25
                ],
                [
                    "model",
                    18
                ],
                [
                    "data",
                    11
                ],
                [
                    "data set",
                    11
                ],
                [
                    "precision",
                    8
                ],
                [
                    "false negative",
                    7
                ],
                [
                    "text item",
                    7
                ],
                [
                    "first",
                    7
                ],
                [
                    "csv",
                    6
                ],
                [
                    "ml",
                    6
                ],
                [
                    "good",
                    6
                ],
                [
                    "language",
                    5
                ],
                [
                    "trained",
                    5
                ],
                [
                    "nature",
                    5
                ],
                [
                    "natural",
                    5
                ],
                [
                    "training",
                    5
                ],
                [
                    "create",
                    5
                ],
                [
                    "moment",
                    4
                ],
                [
                    "step",
                    4
                ],
                [
                    "find",
                    4
                ],
                [
                    "category",
                    4
                ],
                [
                    "pretty",
                    4
                ],
                [
                    "false positive",
                    4
                ],
                [
                    "yeah",
                    3
                ],
                [
                    "focus",
                    3
                ],
                [
                    "column",
                    3
                ],
                [
                    "cloud",
                    3
                ],
                [
                    "thing",
                    3
                ],
                [
                    "analysis",
                    2
                ],
                [
                    "linear",
                    2
                ],
                [
                    "laser",
                    2
                ],
                [
                    "average precision",
                    2
                ],
                [
                    "course",
                    2
                ],
                [
                    "tool",
                    2
                ],
                [
                    "service",
                    2
                ],
                [
                    "python",
                    2
                ],
                [
                    "name",
                    2
                ],
                [
                    "start training",
                    2
                ],
                [
                    "sentiment analysis",
                    2
                ],
                [
                    "basically",
                    2
                ],
                [
                    "function",
                    2
                ],
                [
                    "link",
                    2
                ],
                [
                    "classification",
                    2
                ],
                [
                    "model id",
                    2
                ],
                [
                    "social fiction",
                    2
                ],
                [
                    "type",
                    2
                ],
                [
                    "machine",
                    1
                ],
                [
                    "solution",
                    1
                ],
                [
                    "boiling",
                    1
                ],
                [
                    "bucket",
                    1
                ],
                [
                    "error",
                    1
                ],
                [
                    "monopoly",
                    1
                ],
                [
                    "artificial intelligence",
                    1
                ],
                [
                    "boolean",
                    1
                ],
                [
                    "science",
                    1
                ],
                [
                    "protein",
                    1
                ],
                [
                    "cell",
                    1
                ],
                [
                    "processing",
                    1
                ],
                [
                    "main page",
                    1
                ],
                [
                    "metal",
                    1
                ],
                [
                    "resource",
                    1
                ],
                [
                    "natural language processing",
                    1
                ],
                [
                    "material",
                    1
                ],
                [
                    "statistic",
                    1
                ],
                [
                    "machine learning",
                    1
                ],
                [
                    "currency",
                    1
                ],
                [
                    "special",
                    1
                ],
                [
                    "state",
                    1
                ],
                [
                    "competition",
                    1
                ],
                [
                    "family",
                    1
                ],
                [
                    "level",
                    1
                ],
                [
                    "ai",
                    1
                ],
                [
                    "till",
                    1
                ],
                [
                    "import",
                    1
                ],
                [
                    "big data",
                    1
                ],
                [
                    "walk",
                    1
                ],
                [
                    "curve",
                    1
                ],
                [
                    "window",
                    1
                ],
                [
                    "score",
                    1
                ],
                [
                    "accuracy",
                    1
                ],
                [
                    "scroll",
                    1
                ],
                [
                    "nlp",
                    1
                ],
                [
                    "element",
                    1
                ],
                [
                    "navigation",
                    1
                ],
                [
                    "question",
                    1
                ],
                [
                    "stuff",
                    1
                ],
                [
                    "algorithm",
                    1
                ],
                [
                    "translation",
                    1
                ]
            ],
            "description": "Using AutoML NLP (Natural Language Processing) to classify and predict multilabel texts with a custom model. The demo consists of 3 parts:\n- Uploading dataset\n- Training\n- Evaluating results and prediction\n\nDownload .csv here: https://cloud.google.com/natural-language/automl/docs/sample/happiness.csv\n\nThank you!",
            "dislikeCount": "0",
            "duration": "PT19M14S",
            "likeCount": "3",
            "published_time": "2018-09-21T10:38:01.000Z",
            "tags": [],
            "thumbnail": "https://i.ytimg.com/vi/-zteIdpQ5UE/hqdefault.jpg",
            "title": "Using Google AutoML NLP (Natural Language Processing)",
            "transcript": "  hello and welcome to everyone to this demo in this in this video we're gonna focus on using Auto ml tool recently launched beta version from GCP Google cloud platform and what we're gonna do is train from scratch a model to predict and classify several texts into eight different categories okay so we're gonna do this in a very simple way and I hope you enjoy let's get started okay so first I would usually do we access our console so is console dot cloud.google.com this is gonna take us to the main page from of DCP this is what we start okay so as you can see I have created a project already a if you can if you haven't created a project yet please check my other videos that shows how to create a protein DCP and you have to associate to a billing account always playing this another video okay so now we have the a project created we go to a navigation video and we have to cross scroll all the way to the mathilde learning artificial intelligence tools okay so in this case we're gonna use the natural language one we've got ml n giant time solution translation mission but we're gonna focus today on NLP natural language processing okay so first here I totally it shows us two options we can use the pre trained model is trained from a Google and I need it it can be used for sentiment analysis and we can use this in a way we will see this in another video but now for the moment we're going to focus here on qad9 question model okay so basically the following instructions is building the other cell without two ml natural language we add our own text items and twinam all to recognize categories we define okay and here's the interesting stuff we're gonna see in the following step Naoko is required okay that's why I mentioned it's it's it's really simple to do this and I will show you later okay so we can start it we can do we can learn one here but we're gonna start get started directly so a if you haven't used how to ml before these data set page is gonna appear blank okay so what we're gonna do is add a new data set okay we just gonna name the data set it's gonna be named demo three for me here it comes with the another internship point okay we have a three different possibilities two of them two of them is uploading a file from local and the other one is uploading directly from cloud storage I find it super super useful okay this is for me it's really useful you have a bucket or cloud story this is called GS is Google storage okay you just can put the the CSV URL and it will of the relation in this case we're gonna upload it from local ok so we're gonna load at this V okay and in my case I'm going to Train this one here it's called happiness dot CSV I'm gonna it's part for me from a Carol competition I'm gonna I'm gonna link it in the description below so everyone can come practice ok so let's take a look first first of this data set ok so just open it in a text editor yeah and let's take a look okay so for example we can see we have a sentences like I meditated last night it's classified in negatively laser my grandmother starts to walk from the bed after a long time this is affection I want shopping miss laser again and when I received flowers from my best friend is bonding ok so what we have here is a CSV formatted file with halves on the left side on the on the first column we have the sentence and then the other one we have a the classification ok they're different labels so we're going to do is upload this CSV without any preparation okay we're just gonna enable in this case multi-label classification because as you can see we have more than two okay so we have multiple labels of it as is our case okay so we just pressed create data set okay so when you press create data set is gonna create it directly but it's gonna it's gonna be a it's gonna take a couple of minutes to create okay so therefore I have done this a before as you can see I did it yesterday or the day before yeah the day before and a if we can take a look okay we can say this is a multi label text it has a twelve thousand three hundred sixty three items and all of them are labeled okay remember this because we're gonna take a look at it later so let's take a look first okay so for example I visit again I love to play Monopoly with my wife and sons in my house it's social fiction I took my kids out for a special snack his social fiction okay so we're going to work what we're gonna see here is a how many texts we have in each label okay so we have one two three four five six seven labels we can see there's a big difference between how many labels how many texts excuse me how many types we have in a label achievement which is almost 4,000 but for nature we're gonna have 250 okay as I mentioned before we have zero and label text okay so why is this important this is important when we're gonna study the results later and we're gonna see a difference in the resource we have from achievement and when the results we'll have from nature okay we have to take into account okay so now we have our our data set here uploaded as I said before no pre preparation a no preparation needed I didn't have to modify this state in boolean specific format it just has to be a CSV and it automatically in fears we have one column with text and will in column with label okay we haven't mentioned this in the CSV he has automatically directly done it ok so we have it now we can take a look at the label stats yeah this is a very useful way to see it we can see achievement affection of boiling we have enough into the moment we have them exercise Nader and later maybe we can have a bit more what we can see is enough is enough because as you can see out of a metal suggests we have to have at least 100 text item a science week this is because what they are Guri them it has a behind it it mentions I think is really important as I said before fevered text items also result in inaccurate position and recall your scores okay we're gonna see this later okay so now we have our dataset let's train so we just go to the Train one and a in your case you haven't trained a model before this is gonna be a pure blank again and you have to just press train new model okay so it's gonna give it model ID okay this model ID has to be unique and I find it really really useful first there are summary almost thirty not at two thousand and a half two thousand five hundred label text item seven labels okay and I find this super useful because I'll continue with my work and a Auto ml will send me a message an email when the training is complete ok so I don't even have to give these these this page open okay I can just close this when I start training and it will automatically a train okay so I can tell you a when I press the start training is going to last a between one hour and one hour and a half depending ok so it's gonna cancel now and a one the model has finished this is what we love what we will appear okay as I said before you will receive an email with a URL will directly to this page okay so let's see let's go on let's wanna take a look okay so we have an average precision of zero point nine to five and in this case we have a precision off so eighty five point nine two percent okay so we can do we can say for this a size we would have the 2600 text items this position is really good is almost eighty six percent okay we're speaking about a in this case we have a really really little rate of false positive okay when I was peeking out at retail Rico we can speak about a false negatives and we also have a really really low rate of false negatives okay so just to sum up a false positives is is a a text you have classified in a label and it is not its label okay and false negatives is the other way around okay so this is a little too kind of errors we typically a statistic I look at this we have on average precision of zero point nine to five okay so let's see precision and recall again and we can also take a look at their a at the rock course okay so what we're looking into the rock corpse it's a false negatives and false positives this one will have precision and recall so what we're trying to see here is this this kerf is gonna be a we're trying to pursue is this course to be as squared as possible okay so ideally this will be a linear horizontal function and this will be a linear vertical function okay so we can see we're pretty close there okay this is the main precision okay so next next thing so here what else do I find useful here okay so for example if we have a lot of labels we can filter here I can just tip tip here till then and we go to achieve and I think it's useful too but okay let's go the important part as he said before we have less a texts from nature than we have from for example achievement or affection okay so therefore if we go and press achievement what we're showing here is the precision and we go for a just the category of achievement okay so this is super useful because we can take a look at the individual record we can take a level and main text items we're gonna take a look at the false negative we are looking on so this is super useful okay so as I said before false negatives are sentences that have been classified under the label of achievement but they aren't okay and the other way around there are way around these sentences that are actually part of achievement but have been classified into other other categories okay so we're doing pretty good pretty good so far a affection there we go affection we can see doesn't mention before it's almost a perfect ROC curve okay we can see the precision improve we can see the recall improve and we can see that almost no false positive elements no false negatives okay so we're putting pretty good here enjoy the moment we can take a look it's almost the same but we can see this is better this is this is worse sorry why because we have less text items okay so here we can take a look and and because it requires almost no work at all to analyze this these and have these kind of conclusions maybe we can go a go back a bit of steps a if we have a dataset for example by our company we can modified somehow this data set maybe try to include more of this type and therefore improve improve our our rates okay so overall the models is really more than acceptable is really good and a we continue to our next part and final part final step is predict so we can predict our our model okay so what what this is doing is actually with the model we have created with this data set with the algorithm ploy by Google we have something to predict text ok from here so what he did from behind I'm gonna I'm gonna talk about what about this model out to ml a as it used to before I'm gonna go back a bit demo to e as you saw before we had to 12,000 a 600 texts okay so what he does automatically is divided this set in three parts okay what it does is it divides in in three parts okay so these three parts I'm gonna talk a bit now what machine learning is this is a bit basic is dividing the whole data set in eighty percent for training ten percent for tests and ten percent for validation okay so what it does is divides this way then it tests a the currency is not enough it comes back to train then test again and then when it is happy when the training and validating train and test it goes to to validation okay so these three steps are done automatically if you prefer to have another percentages these can be this can be changed but a as as I see it the purpose of a automail is to to to do as less coding a as possible right so so dividing the idea set up to matically for me is a really really nice and cool cool feature right I think you will agree okay so now we go to predict we can test our model okay so we have a limit of 10,000 characters so we can put a whole text and let's try let's try what happens for example if I put what a beautiful day it is any press predict okay he's not currently he says this has to do with enjoy the moment okay so there are really really really really infinite applications on this okay because just amend you're part of a company realizing some text and you you just divide all all your your customer text into seven different labels you have to train it with a big data set and basically are two motivating fears okay let's try again for example if I put I love my family you have time to try this on your it's just affection okay and it's really really sure this is affection okay so for example destroy up last one I'd like to be with my friends out in the nature let's see what he does with this it's a bit tricky okay there we go even if I press nature he Affairs the main of the main meaning of this test is showing bonding okay with my friends so he's doing actually really really okay so there we go we have two ain't trained evaluated and predicted a model from scratch we are only using 12,000 a text here as you saw before we only need 100 texts per label but you're gonna agree with me as more texts we have the predictions are going to be really really rude of you so if you're having the 2600 on having accuracy of almost 90 percent a testimony would have 1 million text okay so as I said before the applications here are really really nice it's really cool the easy interface and easy UI this house and you want to use a custom model this this is the model we have trained a we have a recipe we have a Python so buried our text and for example if you're gonna use a Python we just mixed these two libraries from work loud import this one and this one service and we're gonna have is a this extra material has my my model here because of the prediction service client and we are going to press price and predict pi and we just gonna have my test here a my name of a project and this is this a picky okay because of the building and things so a thank you thank you for listening I think we did everything here we just gonna take a look again our evaluation window because it's reproducible as I said before 12600 text multi-level in seven labels so this is really really really good okay so thank you thank you again for joining me a in our next demos we're gonna show how to work first with the with the with this one here with the natural language pre-trained model okay i'm gonna put a link in the description of the natural language a p it automatically in furious if the if the a is sentimental sentiment analysis okay positive negative neutral all these things but i hope you enjoyed a microsite",
            "userFeedbackScore": 0.3,
            "videoid": "-zteIdpQ5UE",
            "viewCount": "72"
        },
        "3ESNTxor_VM": {
            "NumOfComments": 1,
            "caption_exist": "T",
            "channel_id": "UC8IxfqqDmGGWZj1EuNCVlEQ",
            "channel_title": "Lucy Walsh",
            "comment_sentiment": 0.0,
            "concepts": [
                [
                    "computer",
                    7
                ],
                [
                    "language",
                    6
                ],
                [
                    "rule",
                    5
                ],
                [
                    "speech",
                    5
                ],
                [
                    "word",
                    5
                ],
                [
                    "information",
                    4
                ],
                [
                    "natural",
                    4
                ],
                [
                    "noun",
                    4
                ],
                [
                    "nlp",
                    3
                ],
                [
                    "understand",
                    3
                ],
                [
                    "part of speech",
                    3
                ],
                [
                    "text",
                    2
                ],
                [
                    "processing",
                    2
                ],
                [
                    "applied",
                    2
                ],
                [
                    "common",
                    2
                ],
                [
                    "natural language processing",
                    2
                ],
                [
                    "speech tagging",
                    2
                ],
                [
                    "process",
                    1
                ],
                [
                    "previous word",
                    1
                ],
                [
                    "data",
                    1
                ],
                [
                    "form",
                    1
                ],
                [
                    "understand text",
                    1
                ],
                [
                    "statistic",
                    1
                ],
                [
                    "analysis",
                    1
                ],
                [
                    "programming",
                    1
                ],
                [
                    "response",
                    1
                ],
                [
                    "technology",
                    1
                ],
                [
                    "programming language",
                    1
                ],
                [
                    "help",
                    1
                ],
                [
                    "computer programming",
                    1
                ],
                [
                    "human",
                    1
                ],
                [
                    "translation",
                    1
                ],
                [
                    "level",
                    1
                ]
            ],
            "description": "A short explanation of how Natural Language Processing works and why the technology is important",
            "dislikeCount": "0",
            "duration": "PT2M3S",
            "likeCount": "5",
            "published_time": "2016-10-22T10:22:15.000Z",
            "tags": [],
            "thumbnail": "https://i.ytimg.com/vi/3ESNTxor_VM/hqdefault.jpg",
            "title": "Natural Language Processing",
            "transcript": "  natural language processing abbreviate as NL pain refers to computers being able to process information in a natural language like English as opposed to a computer programming language this allows computers to read and understand the information we input into them every day on the internet we generate 250 million DVDs worth of information an NLP is important because it can help us manage this huge information overload by turning into useful data most applications of NLP understand text using something called part of speech tagging this means going through a sentence and labeling each word as a noun verb adjective or so on this may seem relatively simple until you consider that natural languages are ambiguous depending on the context the part of speech for words can change take the simple sentence for example these two parts speech analysis are both technically valid as humans we automatically recognize that in the sentence because a noun and bought is a verb this is because of the grammar and syntax Wars we use that thinking but without NLP computers did not have the same lower and standing there for the most common part speech tiger the braille tiger works by initially assigning the most common tag to each word then applying predefined rules to correct any mistakes so in our example these are the tax that will be applied initially the preset rules take into account other words in the sentence which is the computers way of understanding context the rule that will be used in this case is an adjective change to a noun if the previous word is a proper noun so our sentence tags are corrected and from there more rules can be applied to determine the tense of the verb this kind of rule based part of speech tagging is currently around 96 and accurate and allows computer to understand the sentence enough to be our students like translating from the language extract keywords to form a summary of the text or even formulate a response natural language processing has advanced significantly since its conception in the 1950s but it still has a long way to go applications of the technology will around us in our spam filters translation ups and personal statistics serene as the tech gets more advanced perhaps we will see a day where every computer be able to interact to the same level as humans",
            "userFeedbackScore": 0.3,
            "videoid": "3ESNTxor_VM",
            "viewCount": "926"
        },
        "3Q8wacwA4gs": {
            "NumOfComments": 4,
            "caption_exist": "T",
            "channel_id": "UCl6cUxI-KREDbwC3WRa-1Ew",
            "channel_title": "CSRocks",
            "comment_sentiment": 0.30625,
            "concepts": [
                [
                    "analysis",
                    7
                ],
                [
                    "word",
                    6
                ],
                [
                    "language",
                    4
                ],
                [
                    "natural",
                    3
                ],
                [
                    "pragmatic",
                    3
                ],
                [
                    "text",
                    2
                ],
                [
                    "processing",
                    2
                ],
                [
                    "knowledge",
                    2
                ],
                [
                    "syntactic analysis",
                    2
                ],
                [
                    "computer",
                    2
                ],
                [
                    "form",
                    2
                ],
                [
                    "interpretation",
                    2
                ],
                [
                    "human",
                    2
                ],
                [
                    "understand",
                    2
                ],
                [
                    "natural language processing",
                    2
                ],
                [
                    "machine",
                    1
                ],
                [
                    "rule",
                    1
                ],
                [
                    "tree",
                    1
                ],
                [
                    "field",
                    1
                ],
                [
                    "find",
                    1
                ],
                [
                    "noun",
                    1
                ],
                [
                    "deep learning",
                    1
                ],
                [
                    "nlp",
                    1
                ],
                [
                    "semantics",
                    1
                ],
                [
                    "ambiguity",
                    1
                ],
                [
                    "pragmatic analysis",
                    1
                ],
                [
                    "lexical analysis",
                    1
                ],
                [
                    "parsing",
                    1
                ],
                [
                    "sentiment analysis",
                    1
                ],
                [
                    "structural",
                    1
                ],
                [
                    "level",
                    1
                ],
                [
                    "help",
                    1
                ],
                [
                    "semantic analysis",
                    1
                ],
                [
                    "translation",
                    1
                ],
                [
                    "create",
                    1
                ],
                [
                    "human language",
                    1
                ]
            ],
            "description": "Natural language processing allows computers to understand human language. It has plenty of applications. For example: \nText summarization, translation, keyword generation, sentiment analysis or chat bots.\n\nSo how it works? Let\u2019s take a closer look at it.\n\nPlease Like and Subscribe for more weekly videos!\n\nFollow me on Twitter: https://twitter.com/thecompscirocks\nFollow me on Instagram: https://www.instagram.com/thecompscirocks/\nFollow me on Facebook: https://www.facebook.com/thecompscirocks/\n\nSome sources & further reading:\nhttp://www.mind.ilstu.edu/curriculum/protothinker/natural_language_processing.php\nhttps://nlp.stanford.edu/\nhttps://research.google.com/pubs/NaturalLanguageProcessing.html\nhttps://en.wikipedia.org/wiki/Natural_language_processing\nhttps://en.wikipedia.org/wiki/Natural_language_understanding\nhttps://en.wikipedia.org/wiki/Natural_language_generation\nhttps://en.wikipedia.org/wiki/Tokenization_(lexical_analysis)\nhttps://en.wikipedia.org/wiki/Syntax\nhttps://en.wikipedia.org/wiki/Parsing\nhttps://en.wikipedia.org/wiki/Context-free_grammar\nhttps://en.wikipedia.org/wiki/Semantics\nhttps://en.wikipedia.org/wiki/Pragmatics\nhttps://en.wikipedia.org/wiki/Sentiment_analysis",
            "dislikeCount": "6",
            "duration": "PT2M47S",
            "likeCount": "87",
            "published_time": "2017-09-21T18:06:41.000Z",
            "tags": [
                "computer science",
                "programming",
                "education",
                "computer algorithms",
                "programming algorithms",
                "nlp",
                "natural language processing",
                "natural language understanding",
                "machine learning",
                "computational linguistics",
                "artificial analysis",
                "artificial intelligence",
                "sentiment analysis",
                "how nlp works",
                "nlp explained",
                "how natural language processing works",
                "natural language processing explained",
                "natural language processing in artificial intelligence",
                "syntactic analysis",
                "semantics",
                "pragmatics",
                "lexical analysis"
            ],
            "thumbnail": "https://i.ytimg.com/vi/3Q8wacwA4gs/hqdefault.jpg",
            "title": "How Can Computers Understand Human Language? | Natural Language Processing Explained",
            "transcript": "  natural language processing allows computers to understand human language let's take a closer look at it natural language processing or NLP has plenty of applications for example text summarization translation keyword generation sentiment analysis or chatbots so how it works we start with lexical analysis also known as tokenization which divides text into paragraphs sentences and words then we proceed with syntactic analysis or parsing parsing goes through sentence word by word to create structural description of a sentence usually in a form of tree it applies rules of context-free grammar to identify whether the word is noun or verb and so on this helps to understand the relationships between words once we know the structure of a sentence we need to find out its meaning this is the most complex phase because natural language can be quite ambiguous a simple sentence can be interpreted in many different ways semantic analysis gives us context independent interpretation or in other words a meaning without knowledge of the other sentences the result is called logical form at this point there is still may be some level of ambiguity so we call to help pragmatic analysis pragmatic analysis can derive better interpretation of the sentence by looking at the previous and succeeding sentences it also applies real-world knowledge for example that banana is a fruit birds can fly and so on finally it's worth noting that sometimes syntactic analysis semantics and pragmatics aren't completed in a sequential manner but rather simultaneous even though an LP is yunk field we've made quite progress over the last few years with advances in machine and deep learning it will be interesting to watch how the way humans and computers communicate with each other of evolves and as always thanks for watching if you enjoyed this video please hit that like button and don't forget to subscribe to see more videos like this in future",
            "userFeedbackScore": 0.7086816269284713,
            "videoid": "3Q8wacwA4gs",
            "viewCount": "6119"
        },
        "4kyJVwew0lg": {
            "NumOfComments": 0,
            "caption_exist": "T",
            "channel_id": "UC6ZQ-SuhvQAeQIR5tHJGGmQ",
            "channel_title": "The Audiopedia",
            "comment_sentiment": 0,
            "concepts": [
                [
                    "water",
                    7
                ],
                [
                    "mouse",
                    5
                ],
                [
                    "space",
                    4
                ],
                [
                    "money",
                    3
                ],
                [
                    "gold",
                    3
                ],
                [
                    "second",
                    3
                ],
                [
                    "drink",
                    3
                ],
                [
                    "service",
                    2
                ],
                [
                    "step",
                    2
                ],
                [
                    "cai",
                    2
                ],
                [
                    "rolling",
                    2
                ],
                [
                    "order",
                    2
                ],
                [
                    "eye",
                    2
                ],
                [
                    "wave",
                    2
                ],
                [
                    "law",
                    2
                ],
                [
                    "air",
                    2
                ],
                [
                    "word",
                    2
                ],
                [
                    "state",
                    2
                ],
                [
                    "stress",
                    2
                ],
                [
                    "good",
                    2
                ],
                [
                    "base",
                    2
                ],
                [
                    "first",
                    2
                ],
                [
                    "method",
                    2
                ],
                [
                    "solution",
                    1
                ],
                [
                    "tournament",
                    1
                ],
                [
                    "group",
                    1
                ],
                [
                    "minus",
                    1
                ],
                [
                    "tour",
                    1
                ],
                [
                    "forest",
                    1
                ],
                [
                    "sign",
                    1
                ],
                [
                    "mole",
                    1
                ],
                [
                    "funny",
                    1
                ],
                [
                    "front",
                    1
                ],
                [
                    "tribe",
                    1
                ],
                [
                    "dome",
                    1
                ],
                [
                    "crystal",
                    1
                ],
                [
                    "cell",
                    1
                ],
                [
                    "internal",
                    1
                ],
                [
                    "sensor",
                    1
                ],
                [
                    "switch",
                    1
                ],
                [
                    "chip",
                    1
                ],
                [
                    "funding",
                    1
                ],
                [
                    "arm",
                    1
                ],
                [
                    "bridge",
                    1
                ],
                [
                    "german",
                    1
                ],
                [
                    "consumption",
                    1
                ],
                [
                    "computer",
                    1
                ],
                [
                    "node",
                    1
                ],
                [
                    "blind",
                    1
                ],
                [
                    "web",
                    1
                ],
                [
                    "cam",
                    1
                ],
                [
                    "woman",
                    1
                ],
                [
                    "history",
                    1
                ],
                [
                    "special",
                    1
                ],
                [
                    "human",
                    1
                ],
                [
                    "osmosis",
                    1
                ],
                [
                    "cancer",
                    1
                ],
                [
                    "bos",
                    1
                ],
                [
                    "level",
                    1
                ],
                [
                    "rotation",
                    1
                ],
                [
                    "shape",
                    1
                ],
                [
                    "journal",
                    1
                ],
                [
                    "future",
                    1
                ],
                [
                    "c",
                    1
                ],
                [
                    "name",
                    1
                ],
                [
                    "born",
                    1
                ],
                [
                    "wear",
                    1
                ],
                [
                    "mathematics",
                    1
                ],
                [
                    "score",
                    1
                ],
                [
                    "bacteria",
                    1
                ],
                [
                    "text",
                    1
                ],
                [
                    "transmission",
                    1
                ],
                [
                    "scan",
                    1
                ],
                [
                    "toe",
                    1
                ],
                [
                    "computation",
                    1
                ],
                [
                    "fire",
                    1
                ],
                [
                    "wood",
                    1
                ],
                [
                    "training",
                    1
                ],
                [
                    "bus",
                    1
                ],
                [
                    "steel",
                    1
                ],
                [
                    "sludge",
                    1
                ],
                [
                    "model",
                    1
                ],
                [
                    "mantle",
                    1
                ],
                [
                    "joint",
                    1
                ],
                [
                    "liquid",
                    1
                ],
                [
                    "edge",
                    1
                ],
                [
                    "circle",
                    1
                ],
                [
                    "core",
                    1
                ],
                [
                    "tool",
                    1
                ]
            ],
            "description": "What is NATURAL LANGUAGE PROCESSING? What does NATURAL LANGUAGE PROCESSING mean? NATURAL LANGUAGE PROCESSING meaning - NATURAL LANGUAGE PROCESSING definition - NATURAL LANGUAGE PROCESSING explanation.\n\nSource: Wikipedia.org article, adapted under https://creativecommons.org/licenses/by-sa/3.0/ license.\n\nSUBSCRIBE to our Google Earth flights channel - https://www.youtube.com/channel/UC6UuCPh7GrXznZi0Hz2YQnQ\n\nNatural language processing (NLP) is a field of computer science, artificial intelligence and computational linguistics concerned with the interactions between computers and human (natural) languages, and, in particular, concerned with programming computers to fruitfully process large natural language corpora. Challenges in natural language processing frequently involve natural language understanding, natural language generation (frequently from formal, machine-readable logical forms), connecting language and machine perception, dialog systems, or some combination thereof.\n\nThe history of NLP generally started in the 1950s, although work can be found from earlier periods. In 1950, Alan Turing published an article titled \"Computing Machinery and Intelligence\" which proposed what is now called the Turing test as a criterion of intelligence.\n\nThe Georgetown experiment in 1954 involved fully automatic translation of more than sixty Russian sentences into English. The authors claimed that within three or five years, machine translation would be a solved problem. However, real progress was much slower, and after the ALPAC report in 1966, which found that ten-year-long research had failed to fulfill the expectations, funding for machine translation was dramatically reduced. Little further research in machine translation was conducted until the late 1980s, when the first statistical machine translation systems were developed.\n\nSome notably successful NLP systems developed in the 1960s were SHRDLU, a natural language system working in restricted \"blocks worlds\" with restricted vocabularies, and ELIZA, a simulation of a Rogerian psychotherapist, written by Joseph Weizenbaum between 1964 and 1966. Using almost no information about human thought or emotion, ELIZA sometimes provided a startlingly human-like interaction. When the \"patient\" exceeded the very small knowledge base, ELIZA might provide a generic response, for example, responding to \"My head hurts\" with \"Why do you say your head hurts?\".\n\nDuring the 1970s, many programmers began to write \"conceptual ontologies\", which structured real-world information into computer-understandable data. Examples are MARGIE (Schank, 1975), SAM (Cullingford, 1978), PAM (Wilensky, 1978), TaleSpin (Meehan, 1976), QUALM (Lehnert, 1977), Politics (Carbonell, 1979), and Plot Units (Lehnert 1981). During this time, many chatterbots were written including PARRY, Racter, and Jabberwacky.\n\nUp to the 1980s, most NLP systems were based on complex sets of hand-written rules. Starting in the late 1980s, however, there was a revolution in NLP with the introduction of machine learning algorithms for language processing. This was due to both the steady increase in computational power (see Moore's law) and the gradual lessening of the dominance of Chomskyan theories of linguistics (e.g. transformational grammar), whose theoretical underpinnings discouraged the sort of corpus linguistics that underlies the machine-learning approach to language processing. Some of the earliest-used machine learning algorithms, such as decision trees, produced systems of hard if-then rules similar to existing hand-written rules. However, part-of-speech tagging introduced the use of hidden Markov models to NLP, and increasingly, research has focused on statistical models, which make soft, probabilistic decisions based on attaching real-valued weights to the features making up the input data. The cache language models upon which many speech recognition systems now rely are examples of such statistical models. Such models are generally more robust when given unfamiliar input, especially input that contains errors (as is very common for real-world data), and produce more reliable results when integrated into a larger system comprising multiple subtasks....",
            "dislikeCount": "0",
            "duration": "PT9M41S",
            "likeCount": "0",
            "published_time": "2017-09-17T04:00:00.000Z",
            "tags": [
                "dictionary",
                "english dictionary",
                "online dictionary",
                "vocabulary",
                "english vocabulary",
                "online vocabulary",
                "how to pronounce words",
                "what do words mean",
                "natural language processing",
                "what is natural language processing",
                "natural language processing meaning",
                "natural language processing definition",
                "natural language processing explanation",
                "what is the meaning of natural language processing",
                "what is the definition of natural language processing",
                "what does natural language processing mean"
            ],
            "thumbnail": "https://i.ytimg.com/vi/4kyJVwew0lg/hqdefault.jpg",
            "title": "What is NATURAL LANGUAGE PROCESSING? What does NATURAL LANGUAGE PROCESSING mean?",
            "transcript": " [music]  Matte Representative Spell the answer, feel the part, see the part through the parts replies  In the inner node, we have a larger ratio of po  one's old cell counts are not crying Sun Computation Issue Nexus Sex King at low tide  2 large percentage of middle school students to work past their joining Ulsan is easy lowering and Eve's, not seen  If the Cai Jing tablespoons plus 2 Cam filled pug consumption sensors identify the state or frozen  Middle- bottom code and  Do not erase syawoeun junse Day 2 Cold water tribes fight this weekend or after doing  A Bone In My second star of the Nazi naeswik know what the theme prequel Paul  Rush up and give full bet My Mall genre  My eyes get rid of my eyes and the Ulsan combination.  Death drama operate or personal notes de came  Total More Fractional Channels Gone With Gold Room 50 Why Not Come Tea  Bounce punishment was the death of a naturally afraid it's an okay tour victory  This summer the US team disappointed in Switzerland showed that if x Sethi always carrying jjolji  Holden has sex, even though the net a little myr Deer stew Del Scarborough Lima write images  Essence verb wool cern Spy in Wardi Bar Born per web look medical  So you should know the shape of food Yaba be polite x dis  All drinks come Thanksgiving maechan yulma well written law codex 8 Presentation Standing  Insurance poet left spy shot Buy monsters and horses News  In July than chicken above sseuljul im not special Moss Dina were in the car owner  Something Monopolistic Chest Com Centers Oceanic Som Solder And Wrap  Nauru vs Abyss in a packaged but some of the same generations of women in the sex of anonymous sex  Weekend Gold Sense is not even a concept No Steel Hook Hoo woo post-it virtue  In February, I bought a cheap film, and then I went to the studio to buy a buggy cucumber .  Daseuti wide x font text 2 x 2 appearance after winning the veil  Formation Ava Human Per Roll Mouse New  Maybe the prenatal forest vero Mi-hee manga kenzo ono  The wi-fi in the air gave nothing is Greece and bounce 4x3 wer  Applicable  Joint holdings in de neve cancer of the woods this fight 1000000000000 Under Bright Memories  The Authorization flash if the water line is called Po dress that tickled a bit rate countless  What is Liquid Bodhisattva  Shah cheap ink Jim minutes you deserve tide shifted row you drink gin sub-da  You Ullmann following three sub-Jin da  Hmm. You Zeus Moo - hyun Jin sub-lease Sword  You're woomaru Jean d sub is more beneficial ansseo Scarborough burner luggage stacked circle  The bacterium is operated by a squarer d ribbon line 4 Star ii Man Ichu ring clinker  Mart said Kwak not catch the mouse anywhere 1a bis score goals put in front of the base  Dome of the press cases ssakssak nights one week  In the Tin-ai Iida area with the Sterling year , there are 54 billion  Eve, 02 pages, including chicken anesthesiologist Dr. troubling look at a lot of schools Liege Bush Press  Another juice can be two times the bridge to Saint yen square computer ic Nova  As good if osmosis is given when writing the first training Yui West Bengal  Written in seut Expo with cold Smash Basketball You probably are not going to load the pan  Desk ssoing know of my Seoul and easy method that with a grain of rice eseutek  Do not leading lady slut foul jipeoseo life to write a summary of Somerville stand the taste, huh itneunge  Joe's place in the Church once wrote Flats thin 2e Bruce writes that in ssem vacation  x-ridden one week no 2 agonists with all the filing Lee Yu also saw no more news  Seo Hye Rin Eat What did Chris Ross and neo pay under the above soljeoseu Pocket Manual  Stress also look meoro water chi-mc written press, albeit autonomous objects in space, money  Intangible Now yubeyiseu The Tudors 2 May Queen can give where first called me,  Asura Refurbished add more water, wet pocket by giving it what she recently updated  10-year-old Xuan Tidori gently remove the mask with a collection Roy cheap to do a test search  1 Sign Then counters coin More sweet Zen Life Stacking Egger  Day of the mantle above the law dael Bruce Willis &amp; His job is seojuwon  Color mouse compiled sseudeon bus lines make good the Styx Nanny fine sseuk to 6 people  Delos packed standing in autumn followed by the center sion news all levels shellfish  What are my wxd component Gail heartfelt and step test as a gum  Face Somewhere Extra Wave Jake rex Stinger Empty attachment Super one month  Do not fire the puck ds5 interview late Care Andres yulpi aenieun I switch Liaison times  Given the routing session, and bones can be solved Screenshot Ford transmission order bulmyeonseo  Junior State Girls Funding Storm  os Ross can be seen in 4.3 pens came in and almost outboard  Jaseuk writing as soon as look at the main device a little ugly water Steiner wrote muscle chain usb  Mr. eopne order difference multicolor fight evil in chapter one star cow hand  Haejumyeon thorough the media history when doing any service or none of the riches rolling sled  Jay  The number of the crystal wafers may be higher than the number of you can have the half-sen  The three sling Durham Chow other similar purposes in the imagination of the Church of Aosta only party  I took a cup of my work, and I put on a pair of two-year-old ,  I got the vialdre word, and the cucumber that my child sleeps in the back  I wrote the number rises will not use the main house peuba taking it cold quilt after 4 months  There will be a stop in mathematics and the town will hold on.  When you put the mouse in the tournament and you do this at more than 2 m Gabe toe Ankle  Perhaps 10 cuts of tea for the sence Well cut or something What cuts the cube 3 I  Petite chain here  I break the kid's funny how I hate what no x5 took place in unfamiliar areas chips Yahoo Ying  Sky rod 10 junseo shot but are memorial vibrator Judas captured rice kept y  Framed Moule Wine Bee Sen An Real Written by Kangwon Stress Confession Cushion nov  You the gift of 22nd 4 one t you do not have a match naejueo zip Residents  A frustration and a pounding  Dwien the method discussed in the bones Zucker metallocene step difference of 20 can not be increased even Malta  We Wars for doing additional sludge lens to the side Bath aunt gave yogeol internal rotation next week  50 pairs plus the rolling'll no sense dfu sense also test the fence Ivoire  Mare kulron cheap sell you can freeze in minutes Deanna clothing companies panoramic street of the future  Baeksan wrx Mainly in gerber, buds bathtub sts wave easier recording  The ratio is now in the annotated snail brush cut from the inulin school  High deulmyeo de tone hardware make the second Star mutual ghost version written by Blake after seukseuk  Write as sliced illuminate the full width of the second waeyiri funds required use cases 5 times  2e deoni Suns scene from Bruce Saunders points in time give me water hose minus 4  Blue stones on both sides  The Eve of Saint nv share our drinks so as to sd tests that meoro Crit  mixer  Load space is also given de Today fraction not poison the rats as an intangible Mauro European commitment  I forgot the boss Izu I was going to dance 3 I had to use it JUDAHIKEN x I WAS A GROUP  Annie's pain now come with arms spread between servers so pure winger non-members vain life  Tax Solutions hot lens make more juice for the San tools to seojuwon 4 Chunma  Ceze Ceses Ceses Ceses Ceses Ceses Ceses Ceses Ceses Ceses  Part 4 is divided in Ste Cecile Cecile Cecile Cecile Cecile Cecile Cecile Cecile Cecile Cecile Cecile Cecile air  For Dupuy Sub 1, 101 can be more lined c lined standing single ring ice  There is also a son ps on the mouse cup case  warez in Nozawa would give such actual axel  Full Tilt 2 jukgetne took to the ice edge services also German chess fight  Ghostbusters Skiing Separated by laughter Mast Original name Wen poison Bergen car  Inning All the way to and out of one line from one line to one user to  xbox dao May Seoul Jiangsu crackdown party  1 Event in Chin naked to Evil 4 inside Demo All Mole  In vais point 4 on both sides wrx Otsu Seansu also wear sex  The embargo comes in lv blind Susukino little space facing the journal and dwaetguyo  Unto you see what it took behind Weiss wrote yagael puck to come to the Nou scan dma  Sex was caused by a break in Scotland Blue Embroidered it means money spread to the base of one n Chris  Well, what you may be a bit gratuities academic attendance personalized steamed kicking around a stone Cai Jing Yan  US afterglow even after the Bible cabbage ahead of him a little money, no space Ax Lee Camp  Do not share this view sseujyo objects up mwohan  The letter came just 50 Rawlings Buzz jungen water as va-ring model where the core  Trading spun ding of its Hear I got my chest  In other words, standing there looking at me with a handshake bacteria that gold fan for rear  Edition Insurance SARS  U  Oo ",
            "userFeedbackScore": 0,
            "videoid": "4kyJVwew0lg",
            "viewCount": "4142"
        },
        "5ctbvkAMQO4": {
            "NumOfComments": 2,
            "caption_exist": "T",
            "channel_id": "UCkw4JCwteGrDHIsyIIKo4tQ",
            "channel_title": "edureka!",
            "comment_sentiment": 0.2,
            "concepts": [
                [
                    "language",
                    28
                ],
                [
                    "natural",
                    23
                ],
                [
                    "word",
                    15
                ],
                [
                    "text",
                    14
                ],
                [
                    "processing",
                    14
                ],
                [
                    "natural language processing",
                    13
                ],
                [
                    "process",
                    9
                ],
                [
                    "stemming",
                    9
                ],
                [
                    "nlp",
                    7
                ],
                [
                    "form",
                    7
                ],
                [
                    "analysis",
                    6
                ],
                [
                    "data",
                    6
                ],
                [
                    "information",
                    6
                ],
                [
                    "chunk",
                    6
                ],
                [
                    "noun",
                    5
                ],
                [
                    "text mining",
                    5
                ],
                [
                    "root",
                    5
                ],
                [
                    "limit ization",
                    5
                ],
                [
                    "mining",
                    5
                ],
                [
                    "named entity",
                    4
                ],
                [
                    "human",
                    4
                ],
                [
                    "understand",
                    4
                ],
                [
                    "po tag",
                    3
                ],
                [
                    "speech",
                    3
                ],
                [
                    "named entity recognition",
                    3
                ],
                [
                    "common",
                    3
                ],
                [
                    "group",
                    2
                ],
                [
                    "basically",
                    2
                ],
                [
                    "noun phrase",
                    2
                ],
                [
                    "step",
                    2
                ],
                [
                    "text analysis",
                    2
                ],
                [
                    "natural language generation",
                    2
                ],
                [
                    "function",
                    2
                ],
                [
                    "natural language understanding",
                    2
                ],
                [
                    "link",
                    2
                ],
                [
                    "root form",
                    2
                ],
                [
                    "bigger piece",
                    2
                ],
                [
                    "tokenization stemming limit ization",
                    2
                ],
                [
                    "algorithm",
                    2
                ],
                [
                    "dictionary",
                    1
                ],
                [
                    "machine",
                    1
                ],
                [
                    "service",
                    1
                ],
                [
                    "python",
                    1
                ],
                [
                    "article",
                    1
                ],
                [
                    "specie",
                    1
                ],
                [
                    "nature",
                    1
                ],
                [
                    "pattern",
                    1
                ],
                [
                    "list",
                    1
                ],
                [
                    "location",
                    1
                ],
                [
                    "sentiment analysis",
                    1
                ],
                [
                    "prefix",
                    1
                ],
                [
                    "significant",
                    1
                ],
                [
                    "map",
                    1
                ],
                [
                    "string",
                    1
                ],
                [
                    "system",
                    1
                ],
                [
                    "earth",
                    1
                ],
                [
                    "second",
                    1
                ],
                [
                    "keyword search",
                    1
                ],
                [
                    "classification",
                    1
                ],
                [
                    "machine translation",
                    1
                ],
                [
                    "nlp tutorial",
                    1
                ],
                [
                    "human language",
                    1
                ],
                [
                    "quantity",
                    1
                ],
                [
                    "base",
                    1
                ],
                [
                    "name",
                    1
                ],
                [
                    "siri",
                    1
                ],
                [
                    "cortana",
                    1
                ],
                [
                    "structured data",
                    1
                ],
                [
                    "order",
                    1
                ],
                [
                    "till",
                    1
                ],
                [
                    "suffix",
                    1
                ],
                [
                    "translation",
                    1
                ],
                [
                    "first",
                    1
                ],
                [
                    "speech recognition",
                    1
                ],
                [
                    "history",
                    1
                ],
                [
                    "help",
                    1
                ],
                [
                    "type",
                    1
                ],
                [
                    "method",
                    1
                ],
                [
                    "thing",
                    1
                ],
                [
                    "artificial intelligence",
                    1
                ],
                [
                    "internal",
                    1
                ],
                [
                    "part of speech",
                    1
                ]
            ],
            "description": "** Natural Language Processing Using Python: https://www.edureka.co/python-natural-language-processing-course **\nThis Edureka video will provide you with a short and crisp description of NLP (Natural Language Processing) and Text Mining. You will also learn about the various applications of NLP in the industry.\n\nNLP Tutorial : https://www.youtube.com/watch?v=05ONoGfmKvA\n\nSubscribe to our channel to get video updates. Hit the subscribe button above. \n\n-------------------------------------------------------------------------------------------------------\n#NLPin10minutes #NLPtutorial #NLPtraining #Edureka\n\nFacebook: https://www.facebook.com/edurekaIN/\nTwitter: https://twitter.com/edurekain\nLinkedIn: https://www.linkedin.com/company/edureka\nInstagram: https://www.instagram.com/edureka_learning/\n-------------------------------------------------------------------------------------------------------\n\n- - - - - - - - - - - - - -\n\nHow it Works?\n\n1. This is 21 hrs of Online Live Instructor-led course. Weekend class: 7 sessions of 3 hours each.\n2. We have a 24x7 One-on-One LIVE Technical Support to help you with any problems you might face or any clarifications you may require during the course.\n3. At the end of the training, you will have to undergo a 2-hour LIVE Practical Exam based on which we will provide you a Grade and a Verifiable Certificate!\n\n\n- - - - - - - - - - - - - -\n\nAbout the Course\n\nEdureka's Natural Language Processing using Python Training focuses on step by step guide to NLP and Text Analytics with extensive hands-on using Python Programming Language. It has been packed up with a lot of real-life examples, where you can apply the learned content to use. Features such as Semantic Analysis, Text Processing, Sentiment Analytics and Machine Learning have been discussed.\n\nThis course is for anyone who works with data and text\u2013 with good analytical background and little exposure to Python Programming Language. It is designed to help you understand the important concepts and techniques used in Natural Language Processing using Python Programming Language.  You will be able to build your own machine learning model for text classification. Towards the end of the course, we will be discussing various practical use cases of NLP in python programming language to enhance your learning experience.\n\n--------------------------\n\nWho Should go for this course ?\n\nEdureka\u2019s NLP Training is a good fit for the below professionals:\nFrom a college student having exposure to programming to a technical architect/lead in an organisation\nDevelopers aspiring to be a \u2018Data Scientist'\nAnalytics Managers who are leading a team of analysts\nBusiness Analysts who want to understand Text Mining Techniques\n'Python' professionals who want to design automatic predictive models on text data \n\"This is apt for everyone\u201d\n\n---------------------------------\n\n\nWhy Learn Natural Language Processing or NLP?\n\nNatural Language Processing (or Text Analytics/Text Mining) applies analytic tools to learn from collections of text data, like social media, books, newspapers, emails, etc. The goal can be considered to be similar to humans learning by reading such material. However, using automated algorithms we can learn from massive amounts of text, very much more than a human can. It is bringing a new revolution by giving rise to chatbots and virtual assistants to help one system address queries of millions of users.\n\nNLP is a branch of artificial intelligence that has many important implications on the ways that computers and humans interact. Human language, developed over thousands and thousands of years, has become a nuanced form of communication that carries a wealth of information that often transcends the words alone. NLP will become an important technology in bridging the gap between human communication and digital data.\n\n\n---------------------------------\n\nFor Natural Language Processing Training call us at US: +18336900808 (Toll Free) or India: +918861301699 , Or, write back to us at sales@edureka.co",
            "dislikeCount": "3",
            "duration": "PT8M26S",
            "likeCount": "118",
            "published_time": "2018-10-16T05:26:04.000Z",
            "tags": [
                "yt:cc=on",
                "natural language processing tutorial",
                "natural language processing in 10 minutes",
                "nlp tutorial for beginners",
                "nlp tutorial",
                "nlp in 10 minutes",
                "nltk tutorial",
                "nltk tutorial for beginners",
                "nltk training",
                "natural language processing training",
                "nlp with python tutorial",
                "nlp programming tutorial",
                "nlp neural network tutorial",
                "nlp machine learning tutorial",
                "nltk tutorial in python",
                "edureka",
                "nlp edureka"
            ],
            "thumbnail": "https://i.ytimg.com/vi/5ctbvkAMQO4/hqdefault.jpg",
            "title": "Natural Language Processing In 10 Minutes | NLP Tutorial For Beginners | NLP Training | Edureka",
            "transcript": "  [Music] well human beings are the most advanced species on earth and there's no doubt in that and our success as human beings is because of our ability to communicate and share information now that's where the concept of developing a language comes in and when we talk about the human language it is one of the most diverse and complex part of us considering a total of six thousand and five hundred languages that exists so coming to the 31st century according to the industry estimates only twenty one percent of the available data is present in the structured form data is being generated as I speak tweet and send messages on whatsapp or the various of the crops of Facebook and majority of the stata exist in the textual form which is highly unstructured in nature now in order to produce significant and actionable insights from this data it is important to get acquainted with the techniques of text analysis and natural language processing so let's understand what is text mining and natural language processing so text mining or text analytics is the process of deriving meaningful information from natural language text it usually involves the process of structuring the input text deriving patterns within the structured data and finally evaluating and interpreting the output now on the other hand natural language processing refers to the artificial intelligence method of communicating with an intelligent system using the natural language as text mining refers to the process of deriving high-quality information from the text the overall goal is here to essentially turn the text into data analysis via the application of natural language processing satisfied text mining and NLP go hand-in-hand so let's understand some of the applications of text mining or natural language processing so one of the first and the most important applications of natural language processing is sentiment analysis be it Twitter sentimental analysis or the Facebook sentiment as it's being used heavily now next we have the implementation of chat pod now you might have used the customer chat services pride by various companies and the process behind all of that is because of the NLP now we have speech recognition and here we are also talking about divorce assistants like Siri Google assistant and Cortana and the process behind all of this is because of the natural language processing no machine translation is also another use case of natural language processing and the most common example for it is the Google Translate which uses NLP to translate data from one language to another and that too in the real time now other applications of NLP include spell checking keyword search and also extracting information from any doc or any website and finally one of the coolest application of natural language processing is advertisement matching basically recommendation of ads based on your history now NLP is divided into two major components that is the natural language understanding and the natural language generation the understanding generally refers to mapping the given input into natural language into useful representation and analyzing those aspects of the language whereas generation is the process of producing meaningful phrases and sentences in the form of natural language from some internal representation now the natural language understanding is usually harder than the natural language generation because it takes a lot of time and a lot of things to usually understand a particularly specially if you are not a human being now there are various steps involved in the natural language processing which are tokenization stemming limit ization the POS tags named entity recognition and chunking now starting with tokenization tokenization is the process operating strings into tokens which in turn are small structures or units that can be used for tokenization so if we have a look at the example here taking this sentence into consideration it can be divided into seven tokens now this is very useful in the natural language processing part now coming to the second process in natural language processing is stemming now stemming usually refers to normalizing the words into its base or the root form so if we have a look at the words here we have affectations effects affections affected affection and effective now all of these word originate from a single root word and as you might have guessed it is effect now stemming algorithm works by cutting off the end or the beginning of the war taking into account a list of common prefixes suffixes that can be found in an infected word this indiscriminate cutting can be successful in some occasions but not always so let's understand the concept of limitation now limitation on the other hand takes into consideration the morphological analysis of the void to do so it is necessary to have a detail dictionary which the algorithm can look through to link the form back to its original word or the root word which is also known as lemma now what limit ization does is groups together different infected forms of the word called lemma and is somehow similar to stemming as it mapped several words into one common root but the major difference between stemming and limitation is that the output of the limit ization is a proper word for example a limit Iser should map the word gone going and went into KO that will not be the output for stemming now once we have the tokens and once we have divided the tokens into its root form next comes the POS tags now generally speaking the grammatical type of the word is referred to as POS tags or the paths of speech be it the verb noun adjective adverb article and many more it indicates how a word functions in meaning as well as grammatically within the sentence a word can have more than one part of speech based on the context in which it is used for example let's take the sentence who was something on the internet here Google is used as a verb although it's a proper noun now these are some of the limitations or I should say the problems that occur while processing the natural language now to overcome all of these challenges we have the named entity recognition also known as NER so it is the process of detecting the named entities such as the person name the company names we have the quantities or the location now it has three steps which are the noun phrase identification the phrase classification and entity disambiguation so if you look at this particular example here Google CEO sundar pichai introduced the new pixel 3 at new york central mall so as you can see here google is identified as a organization so in the picture as a person we have new york as lucky and central mall is also defined as an organization now once we have divided the sentences into tokens done the stemming the limit ization added the tags as the named entity recognition it's time for us to group it back together and make sense out of it so for that we have chunking so chunking basically means picking up individual pieces of information and grouping them together into the bigger pieces now these bigger pieces are also known as chunks in the context of NLP chunking means grouping of words or tokens into chunks so as you can see here we have pink as an adjective Panther as a noun and D as a determiner and all of these are together chunked into a noun phrase now this helps in getting insights and meaningful information from the given text now you might be wondering where does one execute or run all of these programs and all of this function on a given text file so for that Python came up with an LD K now what is an LD again n LT K is the natural language toolkit library which is heavily used for all the natural language processing and the text analysis so guys if you want to know the details about how to execute each and every parts like tokenization stemming limit ization through an LT k you can refer to our NLP tutorial the link to it is given in the description box below till then thank you and happy then I hope you have enjoyed listening to this video please be kind enough to like it and you can comment any of your doubts and queries and we will reply them at the earliest do look out for more videos in our playlist and subscribe to any rekha channel to learn more happy learning",
            "userFeedbackScore": 0.577297879985627,
            "videoid": "5ctbvkAMQO4",
            "viewCount": "3291"
        },
        "8FGHDBwoPVo": {
            "NumOfComments": 1,
            "caption_exist": "T",
            "channel_id": "UCl1Tqc3U-TAOjuh4izHLsUw",
            "channel_title": "Ranji Raj",
            "comment_sentiment": 0.2,
            "concepts": [
                [
                    "language",
                    44
                ],
                [
                    "thing",
                    27
                ],
                [
                    "natural",
                    24
                ],
                [
                    "understand",
                    14
                ],
                [
                    "machine",
                    13
                ],
                [
                    "basically",
                    11
                ],
                [
                    "word",
                    10
                ],
                [
                    "processing",
                    9
                ],
                [
                    "nlp",
                    9
                ],
                [
                    "analysis",
                    8
                ],
                [
                    "system",
                    8
                ],
                [
                    "first",
                    8
                ],
                [
                    "natural language processing",
                    7
                ],
                [
                    "german",
                    6
                ],
                [
                    "vocabulary",
                    6
                ],
                [
                    "c",
                    6
                ],
                [
                    "real world",
                    5
                ],
                [
                    "natural language understanding",
                    5
                ],
                [
                    "step",
                    5
                ],
                [
                    "knowledge",
                    5
                ],
                [
                    "text",
                    5
                ],
                [
                    "base",
                    4
                ],
                [
                    "alexa",
                    4
                ],
                [
                    "lexical analysis",
                    4
                ],
                [
                    "mean",
                    4
                ],
                [
                    "process",
                    3
                ],
                [
                    "natural language generation",
                    3
                ],
                [
                    "artificial intelligence",
                    3
                ],
                [
                    "form",
                    3
                ],
                [
                    "communication",
                    2
                ],
                [
                    "static",
                    2
                ],
                [
                    "nonliving object",
                    2
                ],
                [
                    "frame",
                    2
                ],
                [
                    "home automation system",
                    2
                ],
                [
                    "static object",
                    2
                ],
                [
                    "siri",
                    2
                ],
                [
                    "human",
                    2
                ],
                [
                    "drone",
                    2
                ],
                [
                    "nature",
                    2
                ],
                [
                    "amazon alexa",
                    2
                ],
                [
                    "alphabet",
                    2
                ],
                [
                    "english language",
                    2
                ],
                [
                    "weather",
                    2
                ],
                [
                    "nlu",
                    2
                ],
                [
                    "base language",
                    2
                ],
                [
                    "parsing",
                    2
                ],
                [
                    "knowledge base",
                    2
                ],
                [
                    "german language spanish russian",
                    2
                ],
                [
                    "automation",
                    2
                ],
                [
                    "dictionary",
                    1
                ],
                [
                    "book",
                    1
                ],
                [
                    "semantics",
                    1
                ],
                [
                    "energy",
                    1
                ],
                [
                    "bracket",
                    1
                ],
                [
                    "light",
                    1
                ],
                [
                    "overview",
                    1
                ],
                [
                    "map",
                    1
                ],
                [
                    "desert",
                    1
                ],
                [
                    "information",
                    1
                ],
                [
                    "natural language processing in artificial intelligence",
                    1
                ],
                [
                    "interaction",
                    1
                ],
                [
                    "pragmatic analysis",
                    1
                ],
                [
                    "machine learning",
                    1
                ],
                [
                    "order",
                    1
                ],
                [
                    "eye",
                    1
                ],
                [
                    "compiler",
                    1
                ],
                [
                    "semantic analysis",
                    1
                ],
                [
                    "level",
                    1
                ],
                [
                    "internal",
                    1
                ],
                [
                    "python",
                    1
                ],
                [
                    "ai",
                    1
                ],
                [
                    "text realization",
                    1
                ],
                [
                    "name",
                    1
                ],
                [
                    "differentiation",
                    1
                ],
                [
                    "mathematics",
                    1
                ],
                [
                    "plant",
                    1
                ],
                [
                    "sound",
                    1
                ],
                [
                    "scan",
                    1
                ],
                [
                    "content",
                    1
                ],
                [
                    "cd",
                    1
                ],
                [
                    "chunk",
                    1
                ],
                [
                    "android",
                    1
                ],
                [
                    "lattice",
                    1
                ],
                [
                    "second",
                    1
                ],
                [
                    "cortana",
                    1
                ],
                [
                    "pragmatic",
                    1
                ],
                [
                    "text planning",
                    1
                ],
                [
                    "method",
                    1
                ],
                [
                    "regional",
                    1
                ]
            ],
            "description": "Order my books at \ud83d\udc49 http://www.tek97.com/\n\nLearn technically about how the communication in various machines takes place in smart devices by using natural language in Artificial Intelligence. Watch Now !\n\n\u062a\u0639\u0644\u0645 \u062a\u0642\u0646\u064a\u064b\u0627 \u0639\u0646 \u0643\u064a\u0641\u064a\u0629 \u0625\u062c\u0631\u0627\u0621 \u0627\u0644\u0627\u062a\u0635\u0627\u0644\u0627\u062a \u0641\u064a \u0627\u0644\u0623\u062c\u0647\u0632\u0629 \u0627\u0644\u0645\u062e\u062a\u0644\u0641\u0629 \u0641\u064a \u0627\u0644\u0623\u062c\u0647\u0632\u0629 \u0627\u0644\u0630\u0643\u064a\u0629 \u0628\u0627\u0633\u062a\u062e\u062f\u0627\u0645 \u0644\u063a\u0629 \u0637\u0628\u064a\u0639\u064a\u0629 \u0641\u064a \u0627\u0644\u0630\u0643\u0627\u0621 \u0627\u0644\u0627\u0635\u0637\u0646\u0627\u0639\u064a. \u0634\u0627\u0647\u062f \u0627\u0644\u0622\u0646 !\n\n\u0418\u0437\u0443\u0447\u0438\u0442\u0435 \u0442\u0435\u0445\u043d\u0438\u0447\u0435\u0441\u043a\u0438 \u043e \u0442\u043e\u043c, \u043a\u0430\u043a \u043e\u0431\u0449\u0435\u043d\u0438\u0435 \u043d\u0430 \u0440\u0430\u0437\u043d\u044b\u0445 \u043c\u0430\u0448\u0438\u043d\u0430\u0445 \u043f\u0440\u043e\u0438\u0441\u0445\u043e\u0434\u0438\u0442 \u0432 \u0438\u043d\u0442\u0435\u043b\u043b\u0435\u043a\u0442\u0443\u0430\u043b\u044c\u043d\u044b\u0445 \u0443\u0441\u0442\u0440\u043e\u0439\u0441\u0442\u0432\u0430\u0445, \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044f \u0435\u0441\u0442\u0435\u0441\u0442\u0432\u0435\u043d\u043d\u044b\u0439 \u044f\u0437\u044b\u043a \u0432 \u0438\u0441\u043a\u0443\u0441\u0441\u0442\u0432\u0435\u043d\u043d\u043e\u043c \u0438\u043d\u0442\u0435\u043b\u043b\u0435\u043a\u0442\u0435. \u0421\u043c\u043e\u0442\u0440\u0438 !\n\nAprenda t\u00e9cnicamente sobre c\u00f3mo la comunicaci\u00f3n en varias m\u00e1quinas se lleva a cabo en dispositivos inteligentes mediante el uso de lenguaje natural en Inteligencia Artificial. Ver ahora !\n\nLernen Sie technisch, wie die Kommunikation in verschiedenen Maschinen in intelligenten Ger\u00e4ten erfolgt, indem Sie nat\u00fcrliche Sprache in der k\u00fcnstlichen Intelligenz verwenden. Schau jetzt !\n\nApprenez techniquement comment la communication dans diverses machines se d\u00e9roule dans les appareils intelligents en utilisant le langage naturel dans l'intelligence artificielle. Regarde maintenant !\n\n\u2b50\u2b50\u2b50\u2b50\u2b50\u2b50\u2b50\u2b50\u2b50\u2b50\u2b50\u2b50\u2b50\u2b50\u2b50\u2b50\u2b50\u2b50\n\nAdd me on Facebook \ud83d\udc49https://www.facebook.com/renji.nair.09\n\nFollow me on Twitter\ud83d\udc49https://twitter.com/iamRanjiRaj\n\nRead my Story\ud83d\udc49https://www.linkedin.com/pulse/engineering-my-quadrennial-trek-ranji-raj-nair\n\nVisit my Profile\ud83d\udc49https://www.linkedin.com/in/reng99/\n\nLike TheStudyBeast on Facebook\ud83d\udc49https://www.facebook.com/thestudybeast/\n\n\u2b50\u2b50\u2b50\u2b50\u2b50\u2b50\u2b50\u2b50\u2b50\u2b50\u2b50\u2b50\u2b50\u2b50\u2b50\u2b50\u2b50\u2b50\n\nFor more such videos LIKE SHARE SUBSCRIBE\n\n\nIphone 6s  : http://amzn.to/2eyU8zi\n\nGorilla Pod : http://amzn.to/2gAdVPq\n\nWhite Board : http://amzn.to/2euGJ7F\n\nDuster : http://amzn.to/2ev0qvX\n\nFeltip Markers : http://amzn.to/2eutbZC",
            "dislikeCount": "0",
            "duration": "PT12M25S",
            "likeCount": "8",
            "published_time": "2018-05-20T03:10:21.000Z",
            "tags": [
                "Artificial Intelligence",
                "Natural Language Processing",
                "NLP",
                "Natural Language Undertanding",
                "NLU",
                "How NLP works in AI",
                "Natural Language Generation",
                "NLG",
                "Text Planning",
                "Text Realization",
                "Scentence Structure",
                "Google Echo",
                "Google Home",
                "Amazon Alexa 2018",
                "Pragmatic Analysis",
                "Lexical Analysis",
                "Syntatic Analysis",
                "Parsing",
                "Semantic Analysis",
                "Discourse Inegration",
                "Drone"
            ],
            "thumbnail": "https://i.ytimg.com/vi/8FGHDBwoPVo/hqdefault.jpg",
            "title": "Artificial Intelligence | Tutorial #34 | Natural Language Processing (NLP)",
            "transcript": "  hey guys welcome to yet another interesting video based on artificial intelligence so in today's video I will be discussing regarding what is natural language processing that is NLP so from the name we start with the bifurcation that is what is natural and language and what this processing so what is natural language something which is there from the nature so what is there from the nature the language English language and all German language Spanish Russian and all your regional languages which are there are basically natural language and we have two processes now when human beings we can process it very easily but when it comes to machine it it's a big big big challenge like how machines will communicate with each other and so be basically consider English as the base language for natural language processing and so in today's world we have so many so many different other items like home automation systems and other devices like the AB CD we have Cortana be a Google assistant where then Amazon Alexa then you have Google require all these devices so what do you think these all devices how they communicate with each other so that's basically based on NLP that is natural language processing so in today's video we'll be seen how this natural language processing is handled in this machines or how different machines that is machine to machine communication or how human to machine interaction actually takes place in a deep dive overview so let's get started so NLP refers to artificial intelligence method of communication with an intelligent system it could be anything like smart devices smart phones we have smart speakers in nowadays for home automation system like we have your Google eco Amazon Alex and so all these devices what they're does is they basically gets your command there should be proper commands so that they will turn music system on they will tune your lights on fan on and not what not everything they will on by themselves so by using all natural language such as English so English is the base language for all these things in order to command like you have if you're having a German language Spanish Russian all these things and you have to manually code and sit and make those system designed to understand what these things actually mean and so English is our natural or global language it becomes very easy for us to communicate and also for machines to communicate in a global fashion and so there are two components two major components contributing to NLP that is we have the very first one that is natural language understanding and then you have the natural language generation so let's go one by one so what is natural language understanding you have to understand the natural language like for example say a baby is there a small kid is there now he wants to understand an English language so what he does is he studies a for Apple B for ball C for cat and all these things so he has to get those objects from the real world and he has to just visualize these things are called in that way that does see for car is there C for cat is there a for Apple is there any for aeroplane B for ball and so many things so it has to get that object in a sense so the very first goes in natural language understanding is mapping of the given input in natural language into useful representations like where the object mapping you have different objects in the real world so from that you just map those into different different forms in your understand to build things so the very challenging thing in NLP that I would say is we have to understand the kind of language so it's very difficult like tomorrow if I'm learning German language then it becomes a challenge for me to understand what and everything is called like a pen is called what in German a book is called what in German or in a house is called what in German so all these things I have to get those things means I should have the vocabulary of that many items so that I can reproduce them in two sentences or I can formulate in a very constructive manner so that understanding is very much important in each of the domain it's not about languages but then also when you learn all the technical subjects and all the real-world things you have to first understand if you don't understand you cannot generate it so it's simple as that so the second step in NLU is analyzing the different aspects of the language so it's like a direction to that particular point now like particular objective Apple is there our ball is also in port context all these objects are being directed to one particular point so that thing is very important means you have to direct or we have to relate and study those things so that is the important thing in NLU that is the natural language understanding so once if you understood what is natural language then you will generate by your own like natural language generation energy so it is the process of producing meaningful phrases and sentences in the form of natural language from some internal kind of representations means for example say tomorrow we are given assignment of doing a college presentation a kind of seminar is there so what do you do you go on google you search for that particular topic what do you do you extract some kind of paragraphs you analyze some sentences you make some your vocabulary you will have your kind of own vocabulary and then you try to jot it down on the paper and then you prepare yourself and then you give it your presentation so it works in this similar fashion so you have three steps in that so first is the text planning so it is basically the return of relevant content from the knowledge base you have your own vocabulary but then to check whether that vocabulary is right to your situation then you have to review it you have to retry or you have to double-check it got its real meaning is so you need to actually plant those texts so in each and every step planning is very much important so in life also if there is no planning you cannot do anything in life so its goes with the same in case of NLP as well then you have this sentence planning once you have the appropriate text for your particular kind of calm particular kind of system then you have to framework that particular sentence so choosing the required words forming meaningful phrases and setting the tone of the sentence so that's very important like you have to choose that particular sentences and form one particular grammar and it should totally make sense to your command like for example if you have your Amazon Alexa and you are saying hey Alexa then it will not listen instead Alexa then it will listen so that kind of - and differentiation should be there while you do the sentence flying so that's very much important then you have the text realization means you have to realize whatever you have written is true to your knowledge so that's very much important and then you will give your presentation to the end-user so you have to realize that mapping each of the sentence plan into sentence structure so that's very much important in natural language generation so we have seen the components of NLP that is natural language understanding first you have to understand that language and then you have to generate that language in your particular way so everyone has a different way of producing their own language so that's difficult from individual to individual next we talk about these steps so there are five different steps of natural language processing so how this processing takes place so the very first is lexical analysis what is lexical or what is lexicon lexicon is nothing but words or vocabulary for that particular language it may be kind of jargon for many kind of things so lexicon of language means the collection of different words and phrases in a language then Elly's divided that is lexical analysis is being dividing the whole of chunk into different paragraph sentences and words like we have many apps and all in android so what it does is basically scans on particular entered paragraphs and then it tries to understand what is it particular sentence is what that word actually mentions so it's like a kind of natural language processing so first you need to have that particular knowledge base that is lexical analysis each and every word each and every alphabets so from alphabets it joins the words and from joins you frame sentences sentences you frame paragraphs so that becomes this story so it's like a big picture and all together so you need to have the first that is lexical analysis in that so that's very much important once that is done then you have the syntactical analysis lattice parsing we have different different coding languages we have Java Python c-sharp are and all those things so this has got its own certain kind of compiler sets it has so what it basically checks is it basically checks at the runtime whether those particular commands which you gave or the syntax whether it is correct or so it basically checks for the grammars and all those things that it's written properly or whether the semicolon or brackets is given properly and all so it basically checks for that so similarly in NLP also it goes in that way so syntactical analysis that is parsing is very much required it checks for the grammars and the arrangement of the words or the fashion of words how they are ordered and the relationship among different words like for example if I say these school goes to the boy in the it doesn't mix any sense like school is what it's a non object or it's a nonliving thing and boys a living thing and the school is a static object then it goes to the boy then it's like rejected if it's like the boy goes to the school then that makes a sense like a boys are what you can say it's a living object so it goes to one particular kind of building so in the NLP or the machine has to understand that school is a nonliving object it is a static object it is made of bricks and all these things so people go into that building and also that thing or that information or that level kind of categorical here are a that you must very much incorporate into your AI system so that is very much important and so if I say a drone goes to the boy this makes sense like drone is also nonliving object but it has its own kind of specifications and also it's like you have to understand that concept or whether it's called so that's mainly important in this then you have this semantic analysis semantics is nothing but the meaning so we have to understand the meaning of that particular sentence what it's actually trying to mean so it does the exact meaning or the dictionary meaning from the text and checks for it Sensibility and meaningfulness so weather checks whether it's sensible or not sensible for me to understand or sensible for me to produce it to the end user so it well very well checks for that example if I say hot ice cream so what does ice cream basically it's a very cold object you it's a delicacy that you have like as a desert and also it cannot be hot whereas if I say cold coffee coffee is hot but then you can have cold coffee with iced and all those things so it's like in a different context chef to identify what it actually means or in the real world if these particular things are there or not so that you have to very much have a sound knowledge for that then you have ice duty tea is also hot but who puts eyes in then but then to ice duty a kind of thing is there so that's very much has to be understood by the machine also you know but machine doesn't know so you have to teach that machine so that comes in machine learning so how much in learns all these things then comes the discourse your integration so it's nothing but the meaning of any sentence depends upon the meaning of the previous sentence just before it or the precursor to it for example if you have a sentence like in mathematics of the relation a is equal to B or B is equal to C so what do does if you like have a is equal to C so it's like if a is equal to B B is equal to C then a is also equal to C so it's in that way then you have the pragmatic analysis fragmenting in the real world is nothing but practical you have to practically analyze those things whether which is said by the user it's practical or not so further during this what was suddenly reinterpreted and what is actually meant so it involves those aspects of a language which require real world knowledge so what a user tells to Siri Siri hey Siri how's the weather today or city finally some restaurants to nearby or city check my mails and also it should make some kind of things so that the system should also respond in that way so well that was all about regarding the natural language processing in artificial intelligence so hope you guys enjoyed this video on you got achieve it by watching this video please do like share comment and most importantly don't forget subscribe to my channel thank you very much",
            "userFeedbackScore": 0.5921739130434782,
            "videoid": "8FGHDBwoPVo",
            "viewCount": "558"
        },
        "AJVP96tAWxw": {
            "NumOfComments": 81,
            "caption_exist": "T",
            "channel_id": "UCWN3xxRkmTPmbKwht9FuE5A",
            "channel_title": "Siraj Raval",
            "comment_sentiment": 0.20357020125263073,
            "concepts": [
                [
                    "data",
                    17
                ],
                [
                    "word",
                    15
                ],
                [
                    "testing data",
                    6
                ],
                [
                    "step",
                    6
                ],
                [
                    "classifier",
                    6
                ],
                [
                    "text",
                    6
                ],
                [
                    "tree",
                    6
                ],
                [
                    "bag",
                    6
                ],
                [
                    "create",
                    6
                ],
                [
                    "analysis",
                    5
                ],
                [
                    "sentiment analysis",
                    5
                ],
                [
                    "data set",
                    5
                ],
                [
                    "machine",
                    4
                ],
                [
                    "import",
                    4
                ],
                [
                    "training",
                    4
                ],
                [
                    "machine learning",
                    4
                ],
                [
                    "first",
                    4
                ],
                [
                    "model",
                    4
                ],
                [
                    "forest",
                    3
                ],
                [
                    "language",
                    3
                ],
                [
                    "random forest",
                    3
                ],
                [
                    "python",
                    2
                ],
                [
                    "nlt",
                    2
                ],
                [
                    "feature vector",
                    2
                ],
                [
                    "frequency",
                    2
                ],
                [
                    "class",
                    2
                ],
                [
                    "array",
                    2
                ],
                [
                    "natural",
                    2
                ],
                [
                    "system",
                    2
                ],
                [
                    "csv",
                    2
                ],
                [
                    "help",
                    2
                ],
                [
                    "vector",
                    2
                ],
                [
                    "probability",
                    2
                ],
                [
                    "helper class",
                    2
                ],
                [
                    "training data",
                    2
                ],
                [
                    "data point",
                    2
                ],
                [
                    "human",
                    2
                ],
                [
                    "module",
                    2
                ],
                [
                    "thing",
                    2
                ],
                [
                    "dictionary",
                    1
                ],
                [
                    "outcome",
                    1
                ],
                [
                    "processing",
                    1
                ],
                [
                    "field",
                    1
                ],
                [
                    "series",
                    1
                ],
                [
                    "leaf",
                    1
                ],
                [
                    "process",
                    1
                ],
                [
                    "curve",
                    1
                ],
                [
                    "understand",
                    1
                ],
                [
                    "graph",
                    1
                ],
                [
                    "technology",
                    1
                ],
                [
                    "link",
                    1
                ],
                [
                    "natural language processing",
                    1
                ],
                [
                    "regression",
                    1
                ],
                [
                    "pretty",
                    1
                ],
                [
                    "function",
                    1
                ],
                [
                    "information",
                    1
                ],
                [
                    "good",
                    1
                ],
                [
                    "geology",
                    1
                ],
                [
                    "stop word",
                    1
                ],
                [
                    "second",
                    1
                ],
                [
                    "method",
                    1
                ],
                [
                    "operating system",
                    1
                ],
                [
                    "decision tree",
                    1
                ],
                [
                    "machine learning model",
                    1
                ],
                [
                    "space",
                    1
                ],
                [
                    "site",
                    1
                ],
                [
                    "download",
                    1
                ],
                [
                    "algorithm",
                    1
                ],
                [
                    "human language",
                    1
                ]
            ],
            "description": "Link to the full Kaggle tutorial w/ code: https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-1-for-beginners-bag-of-words\n\nSentiment Analysis in 5 lines of code:\nhttp://blog.dato.com/sentiment-analysis-in-five-lines-of-python\n\nI created a Slack channel for us, sign up here:\nhttps://wizards.herokuapp.com/\n\nThe Stanford Natural Language Processing course: https://class.coursera.org/nlp/lecture\n\nCool API for sentiment analysis: http://www.alchemyapi.com/products/alchemylanguage/sentiment-analysis\n\nI recently created a Patreon page. If you like my videos, feel free to help support my effort here!:\nhttps://www.patreon.com/user?ty=h&u=3191693\nFollow me:\nTwitter: https://twitter.com/sirajraval\nFacebook: https://www.facebook.com/sirajology Instagram: https://www.instagram.com/sirajraval/ Instagram: https://www.instagram.com/sirajraval/ \nSignup for my newsletter for exciting updates in the field of AI:\nhttps://goo.gl/FZzJ5w",
            "dislikeCount": "48",
            "duration": "PT4M51S",
            "likeCount": "1165",
            "published_time": "2016-04-18T05:36:55.000Z",
            "tags": [
                "sentiment analysis",
                "python",
                "machine learning",
                "natural language processing",
                "nlp",
                "classification",
                "stop words",
                "random forest",
                "decision trees",
                "nltk",
                "scikit learn",
                "classifier",
                "bigrams"
            ],
            "thumbnail": "https://i.ytimg.com/vi/AJVP96tAWxw/hqdefault.jpg",
            "title": "Sentiment Analysis in 4 Minutes",
            "transcript": "  hello world welcome to sera geology today we're going to be building sentiment analysis in four minutes let's get started sentiment analysis is the process of determining the opinion or feeling of a piece of text we humans are pretty good at this I can look at this tweet and immediately know that it's negative it feels like the writer sentiment is one of anger and disgust do the negative wording companies across the world have implemented machine learning to do this automatically it's super useful for gaining insight into customer opinions once you understand how the customer feels after analyzing their comments or reviews you can identify what they liked and disliked and build things like recommendation systems or more targeted marketing campaigns for them in this demo we're going to be building a sentiment analysis program in Python that will identify whether a movie review is positive or negative based on the text in the review we'll get our training and testing data a bunch of labeled reviews from a site called kaggle we'll start off by importing our dependencies will import the operating system module to help us perform command line functions then we'll want to import the scikit-learn module which is a machine learning library in Python with a fast learning curve then we'll import a helper class that will help us clean our data pandas helps us read our data CSV files and NLT K will be used to remove unnecessary words from our data set all right so step one is to just read the data from our hard disk will import the label training data and the testing data then we'll print out that we're first review to the command line to ensure we read the data set correctly once we've read in our data step two is to clean it that means ensure that we remove all the HTML non letters and stop words stop words are words that are insignificant we can download them from the NLT K or natural language toolkit library words like the or 2 or as since it's hard to analyze emotion from them we'll iterate over every review in our training data set and fill our new clean review array with it cleaned reviews our helper class will do the cleaning for us step 3 is to create a bag of words the bag of words model is a simple numeric representation of a piece of text that is easy to classify we just count the frequency of each word in a piece of text and create a dictionary of them this is called token as a in natural language processing will use the count vectorizer object in the scikit-learn package to create it we'll set the max features to 5000 to keep things simple so our bag of words will contain ad max 5000 words and their associated frequencies then we use a fit transform method to fit a model to the bag of words and create the feature vectors we can then store the feature vectors in an array step 4 is to create the classifier a classifier is a machine learning model that will be used to classify whether a piece of text is positive or negative in this example our classifier is a random forest consisting of 100 trees a random forest is a set of decision trees decision trees are grass that model the possibilities of certain outcomes so let's say a piece of text has a word hate up here more than 20 times the probability that it's negative could be something like 80% then based on other word frequencies we increase or decrease that probability accordingly until we get to the leaf of the tree which will be a positive or negative rating this is different from a standard regression classifier where if a data point is on a certain side of the line of best fit we can easily classify it a random forest tree is more like a series of lines one for every tree that segments our possibilities once we've mapped all the lines onto the graph and we plot a new data point or review based on its coordinates we can then classify based on whether it's in a positive or negative space it's time to test our classifier on our testing data so let's format the test data by cleaning the reviews and creating a bag of words once we have our feature vectors for test data we can move on to the last step the last step is for our program to correctly classify the reviews in the testing data set as positive or negative we'll use our random forests to make a prediction well then take the result and write it to a new CSV file that's it let's run our program and see what happens ok it printed out the first review that means it's correctly reading our data set then it's going to clean and parse a training set create the bag of words train the classifier then predict the test labels awesome let's test the first three predictions where one is positive and zero is negative let's see the first 3 are 1 0 and 1 so positive negative and positive let's skim these it's truly a masterpiece positive it's so awful that once you know ok negative awesome seconds performing sentiment analysis like a charm sentiment analysis is still an evolving field of machine learning there's so many grammatical nuances and misspellings and slayings involved in human language that we haven't really taken into account but we can with more powerful algorithms so check out the links in the description below for more information and please please subscribe for more technology videos there's so much I want to make thanks for watching",
            "userFeedbackScore": 0.5736467249153453,
            "videoid": "AJVP96tAWxw",
            "viewCount": "84901"
        },
        "BYdtJs6wU5o": {
            "NumOfComments": 0,
            "caption_exist": "T",
            "channel_id": "UC5zx8Owijmv-bbhAK6Z9apg",
            "channel_title": "Artificial Intelligence - All in One",
            "comment_sentiment": 0,
            "concepts": [
                [
                    "word",
                    36
                ],
                [
                    "speech",
                    21
                ],
                [
                    "part of speech",
                    20
                ],
                [
                    "noun",
                    17
                ],
                [
                    "form",
                    17
                ],
                [
                    "language",
                    12
                ],
                [
                    "category",
                    10
                ],
                [
                    "thing",
                    10
                ],
                [
                    "dog",
                    6
                ],
                [
                    "second",
                    6
                ],
                [
                    "first",
                    6
                ],
                [
                    "article",
                    4
                ],
                [
                    "particle",
                    4
                ],
                [
                    "phrasal verb",
                    4
                ],
                [
                    "understand",
                    3
                ],
                [
                    "computer",
                    3
                ],
                [
                    "accusative",
                    3
                ],
                [
                    "syntactic category",
                    3
                ],
                [
                    "probability",
                    3
                ],
                [
                    "speech tagging",
                    3
                ],
                [
                    "include thing",
                    3
                ],
                [
                    "money",
                    2
                ],
                [
                    "derivative",
                    2
                ],
                [
                    "natural",
                    2
                ],
                [
                    "type",
                    2
                ],
                [
                    "word bar",
                    2
                ],
                [
                    "system",
                    2
                ],
                [
                    "text",
                    2
                ],
                [
                    "knowledge",
                    2
                ],
                [
                    "german",
                    2
                ],
                [
                    "dog chased",
                    2
                ],
                [
                    "action",
                    2
                ],
                [
                    "prior",
                    2
                ],
                [
                    "direct object",
                    2
                ],
                [
                    "common",
                    2
                ],
                [
                    "passive voice",
                    2
                ],
                [
                    "prior knowledge",
                    2
                ],
                [
                    "samantha gave",
                    2
                ],
                [
                    "single word",
                    2
                ],
                [
                    "superlative form",
                    2
                ],
                [
                    "special",
                    1
                ],
                [
                    "future",
                    1
                ],
                [
                    "process",
                    1
                ],
                [
                    "c",
                    1
                ],
                [
                    "wave",
                    1
                ],
                [
                    "sound",
                    1
                ],
                [
                    "nature",
                    1
                ],
                [
                    "list",
                    1
                ],
                [
                    "law",
                    1
                ],
                [
                    "processing",
                    1
                ],
                [
                    "shower",
                    1
                ],
                [
                    "walk",
                    1
                ],
                [
                    "bell",
                    1
                ],
                [
                    "sand",
                    1
                ],
                [
                    "help",
                    1
                ],
                [
                    "question",
                    1
                ],
                [
                    "natural language processing",
                    1
                ],
                [
                    "location",
                    1
                ],
                [
                    "switch",
                    1
                ],
                [
                    "state",
                    1
                ],
                [
                    "tree",
                    1
                ],
                [
                    "cd",
                    1
                ],
                [
                    "order",
                    1
                ],
                [
                    "book",
                    1
                ],
                [
                    "convention",
                    1
                ],
                [
                    "general",
                    1
                ],
                [
                    "arch",
                    1
                ],
                [
                    "negation",
                    1
                ],
                [
                    "matter",
                    1
                ],
                [
                    "course",
                    1
                ],
                [
                    "human",
                    1
                ],
                [
                    "vocabulary",
                    1
                ],
                [
                    "human language",
                    1
                ],
                [
                    "paradigm",
                    1
                ]
            ],
            "description": ".\nCopyright Disclaimer Under Section 107 of the Copyright Act 1976, allowance is made for \"FAIR USE\" for purposes such as criticism, comment, news reporting, teaching, scholarship, and research. Fair use is a use permitted by copyright statute that might otherwise be infringing. Non-profit, educational or personal use tips the balance in favor of fair use.\n.",
            "dislikeCount": "0",
            "duration": "PT15M50S",
            "likeCount": "15",
            "published_time": "2016-03-25T03:53:43.000Z",
            "tags": [
                "Natural Language Processing",
                "Language Processing",
                "University of Michigan",
                "Michigan",
                "NLP",
                "Coursera",
                "Dragomir R. Radev",
                "Computational Linguistics",
                "Linguistics",
                "Information Retrieval",
                "Computer Science",
                "Video Lecture",
                "Video Tutorial",
                "Video Course",
                "Course",
                "Data Science",
                "Parts of Speech"
            ],
            "thumbnail": "https://i.ytimg.com/vi/BYdtJs6wU5o/hqdefault.jpg",
            "title": "Lecture 9 \u2014 Parts of Speech - Natural Language Processing | University of Michigan",
            "transcript": "  so the next segment is going to be about the different parts of speech that existed language so let's look at the first example here we have a sentence like Nathalie likes black cats we can replace the word black with Persian tabby or small and still have sentences it makes sense Nathalie likes tabby cats or Nathalie likes Persian cats well it's a single word and they can be used in the same position those are usually refer to the same syntactic category in this case the syntactic category the part of speech is because it's a single word is an adjective there many other syntactic categories in English and other languages so those include open and close categories open categories include the categories that can have new words added to them over time so for example a no-fly zone is a noun and twork is a verb and those words were added to the vocabulary of English in the relative recent past we also have close functional categories such as the in which are respectively determiners or articles and prepositions let's look at an example sentence the dog chased the yellow bird we have multiple parts of speech here article noun verb article adjective and noun so in English they're about eight general types of parts of speech we looked at some of them already nouns and verbs and adjectives so what do those have in common so the nouns include things like dog tree computer and idea they can be either concrete like the first three or abstract they can vary in number singular and plural in gender and in case not in English but in other languages for example in Latin the same word pwhere which means boy can be spelled in many different ways in the singular one boy can be prayer in which is the so called nominative case or subject case playroom which is the accusative or object and 20 which is the genitive of the boy in the poor we can have query which is nominative again subject form we can have playoffs which is the accusative and finally we're wrong which is the genitive of the boys now the gender doesn't need to match the actual sex of the object being described so a typical example that people give is the word in German machen the word machen means girl and because it's a diminutive the Shen a part indicates that it's small person the word is a neuter in gender in German even though it refers to a female person now let's look at one of the most famous examples in courses of this nature it's a short poem extracted from Lewis Carroll's Alice in Wonderland and more specifically the second part of the book you may have seen this before but if you haven't seen it you will realize that this is in gibberish most of the words are not valid English words and yet it is possible or for a speaker of English to understand at least the parts of speech of those words even though in some cases this can be tricky so let's try to do this together I'm going to give you a minute now to figure out the parts of speech of the words in boldface so let's look at the answer to the previous question what are the parts of speech of the words in boldface the first word is wave and borogoves so those are most likely nouns and how do we know this well they both follow the word in English typically after the word there you have a now next one is a little trickier brillig what kind of part of speech is this well for your first guess is that this is an adjective but it could also be a noun let's look at the examples it could be something like an adjective like it was early or it can be a noun as in it was noon so in either case we don't know exactly the part of speech but we know it's one of those - what about mimsy why limousine is most likely an adjective because of the way it appears in the sand us now what about the slithy Toves well this is either an adjective and noun combination the same as small people or it can also be a noun and verb combination similar to the Bell Tolls finally we have the expression mome Raths outgrabe so this could be a combination of adjectives noun and verb but it can also be a combination of noun verb an adverb as in birds fly outside so the point here is that in some cases you can guess the part of speech of an unknown word based on the context but in other cases this can be very tricky in the reason why this is an important example is that it mimics what computers do with human language they see texts that they really don't understand this is 6 for the sequence of words and if they don't have enough prior knowledge about that language or about the words themselves they wouldn't be able to understand anything so in that case what they do is they either use prior knowledge or their reason for more ballistically so they may be a row in the system that says after the the next word is the noun with the probability of 99% and after an adjective the next word is a noun with the probability of 80% and so on and then when you combine all the probabilities of the sentences that you observe you can come up with the best estimate that is consistent with who's that you know and the text that you have seen it is also important to use context so if for example you have a word that is ambiguous maybe in the context you can see other related words that help you disambiguate it so for example if you use the word bar in a sentence and the whole text talks about legal issues and lawyers and people finishing law school it's more likely that this use of the word bar is in the legal sense rather than the establishment restaurant type of sense and also computers can be wrong so they can make a mistake and this can propagate in the whole system for example they can nor a negation and then assume that the opposite of what was said is actually true the next category of parts of speech is poor nouns in English they include things like she our sales and mine pronounced very in person for a second and third gender masculine and feminine number and case English actually has cases from photos and that includes nominative accusative possessive and second possessive so more specifically that could be I me my and mine and pronouns can also have a reflexive and anaphoric forms so for example if I say Samantha gave her her haircut her must refer to a different person whereas if I say Samantha gave herself a haircut then she gave the haircut to Samantha so this is a reflexive for now the other categories of parts of speech in English include determiners and adjectives determiners are things like articles and demonstratives things like the this that and adjectives includes words that describe properties they can be used either attribute of lis of addictively so small house is attributive and the house is small and that second example small is used predicatively adjectives agree in gender and number in different languages and they can have a positive form such as short and also comparative and superlative forms such as shorter and shortest and those comparative and superlative forms can be either derivative or periphrastic derivative is when you have form like small smaller smallest or periphrastic is difficult more difficult most difficult the next category of parts of speech is verbs that are includes words that describe actions such as throw activities such as walk and states such as have therefore verb forms in English other languages can have many more than that the tenses are present past and future and different variants of those other inflection that exists in English includes number in person and worse can also includes a gerund such as ing forms and infinitives that's the form of the verb that follows the word to verbs can also be distinguished by the aspect they can be the progressive or perfective depending on whether the action is to continuing or not and finally they can be distinguished based on their voice so in the sentence I bought a house the verb bought is in the active voice whereas in a house was bought the word bought is in the passive tense a passive voice sorry the some other things about verbs is that they can include things like participles CD forms for example and auxiliaries like may and will and shower verse can have different arguments those are the words that come after the verb and indicate some modifications to the verb for example the dog sleeps is an instance of an intransitive verb there is no direct object that's what intransitive means the dog chased the cat in that example the word chased is a transitive verb because it has a direct object and finally in the sentence Mary gave the dog the dog a bone we have an example of a die transitive verb that's a verb that takes two objects gave somebody something and you can also have things like your regular verbs so slapped and caught now what I said so far applies mostly to English other languages have much richer inflections the examples that you may be familiar with include languages like French and Latin which have more than a hundred different forms of a certain verb and languages like Finnish which have many different forms sometimes more than 20 of the same now so here's an example of a much more sophisticated inflectional paradigm for verbs this is an example from French on this slide alone there are 36 different forms of the verb to go and I can promise you that there are at least two more slides that I can show you which have even more forms of the verb depending on where it's in present and past and continuous form or when it's in a different moods are just conditional and subjunctive and now just to conclude the list of parts of speech the other adverbs things like happily which describes a manner here which describes a location and never which describes a time prepositions particles so particles can sometimes be confused with prepositions but there's a very important test that can tell them apart particles are usually used in the form of the so called phrasal verbs so phrasal verb could be something like take off take off is not a some special way of taking it's a completely different verb or I wanted to take up this matter with the principle so again take up is a phrasal verb and in the two cases that I just showed you off with two F's and up our particles that are part of those phase of verbs and there's the test that I mentioned can be used to distinguish between prepositions and particles a common example that people uses she ran up a bill versus she ran up a hill in the first example she ran up a bill ran up is a phrasal verb whereas in the second example she ran up a hill up a hill is a prepositional phrase and up is not associated with the ran and the way to tell the two apart is very simple can we move up and the rest of the sentence to the beginning so we can say she ran up a hill up a hill she ran so that sounds like a valid paraphrase therefore we have an instance of a preposition but we can all do the same thing with she ran up a bill we cannot say up a bill she ran which means that in this example around and up are parts of the same word parts are the same verb router and cannot be split up in that case up is a the other Kannada parts of speech that we haven't talked about include coordinating conjunctions such as an or an but which I used to connect similar parts of the sentence for example apples and oranges subordinating conjunctions which are used to connect different portions of the sentence that are not equal for example I can have an entire sentence be inserted by preceding it with the word that so I can say I will not go home unless you give me money so in that example unless introduces an entire relative clause you give me money but I cannot switch the order of the two parts of the sentence and obtain the same meaning and finally we have interjections those are things like sounds like meow or arch and to conclude this section I would like to show you the labels that I used in part of speech tagging in natural language processing so part of speech tagging is the process of automatically assigning parts of speech to words and most of the existing part of speech staggers and natural language parsers use this convention so NN is a shorthand for a singular noun and P is a shorthand for proper noun and an S is for plural noun and so on the first letter in each code tells you the part of speech so whether it's n or J or an or C or R or v and the second and third letters if any tell you something a little bit more detailed about that word for example VB is an uninflected verb v BN is the en or a passive perfective form of the verb such as taken or looked used as a parse participle and v BD stands for a verb used in its past tense such as took or looked as I looked out vs. and the previous sentence where looked was used in a passive voice so this concludes the section on para speech tagging and the next section is going to be on morphology and a lexical",
            "userFeedbackScore": 0.3,
            "videoid": "BYdtJs6wU5o",
            "viewCount": "3654"
        },
        "FLZvOKSCkxY": {
            "NumOfComments": 100,
            "caption_exist": "T",
            "channel_id": "UCfzlCWGWYyIQ0aLC5w48gBQ",
            "channel_title": "sentdex",
            "comment_sentiment": 0.20749528018278013,
            "concepts": [
                [
                    "language",
                    27
                ],
                [
                    "python",
                    20
                ],
                [
                    "word",
                    20
                ],
                [
                    "nlt",
                    15
                ],
                [
                    "thing",
                    14
                ],
                [
                    "text",
                    12
                ],
                [
                    "natural",
                    11
                ],
                [
                    "first",
                    10
                ],
                [
                    "pretty",
                    10
                ],
                [
                    "tokenize",
                    9
                ],
                [
                    "nlt kay",
                    8
                ],
                [
                    "download",
                    8
                ],
                [
                    "quick",
                    7
                ],
                [
                    "processing",
                    7
                ],
                [
                    "natural language processing",
                    7
                ],
                [
                    "form",
                    7
                ],
                [
                    "analysis",
                    6
                ],
                [
                    "separate",
                    6
                ],
                [
                    "programming",
                    5
                ],
                [
                    "basically",
                    5
                ],
                [
                    "stuff",
                    5
                ],
                [
                    "english language",
                    5
                ],
                [
                    "pip",
                    5
                ],
                [
                    "good",
                    5
                ],
                [
                    "window",
                    5
                ],
                [
                    "speech",
                    4
                ],
                [
                    "real quick",
                    4
                ],
                [
                    "find",
                    4
                ],
                [
                    "tokenizing",
                    4
                ],
                [
                    "journal",
                    4
                ],
                [
                    "computer",
                    4
                ],
                [
                    "step",
                    4
                ],
                [
                    "import",
                    4
                ],
                [
                    "sentiment analysis",
                    4
                ],
                [
                    "module",
                    4
                ],
                [
                    "space",
                    4
                ],
                [
                    "understand",
                    4
                ],
                [
                    "process",
                    3
                ],
                [
                    "medical journal",
                    3
                ],
                [
                    "course",
                    3
                ],
                [
                    "built",
                    3
                ],
                [
                    "real basic",
                    3
                ],
                [
                    "regular expression",
                    3
                ],
                [
                    "list",
                    3
                ],
                [
                    "element",
                    3
                ],
                [
                    "word tokenize",
                    3
                ],
                [
                    "programming language",
                    3
                ],
                [
                    "dictionary",
                    2
                ],
                [
                    "wheel",
                    2
                ],
                [
                    "pretty cool",
                    2
                ],
                [
                    "command prompt",
                    2
                ],
                [
                    "python list",
                    2
                ],
                [
                    "political",
                    2
                ],
                [
                    "64-bit version",
                    2
                ],
                [
                    "location",
                    2
                ],
                [
                    "understand text",
                    2
                ],
                [
                    "nlt kay download",
                    2
                ],
                [
                    "power",
                    2
                ],
                [
                    "nl ck",
                    2
                ],
                [
                    "part of speech",
                    2
                ],
                [
                    "yeah",
                    2
                ],
                [
                    "written language",
                    2
                ],
                [
                    "install thing",
                    2
                ],
                [
                    "capital",
                    2
                ],
                [
                    "hit enter",
                    2
                ],
                [
                    "type",
                    2
                ],
                [
                    "main idea",
                    2
                ],
                [
                    "second",
                    2
                ],
                [
                    "question",
                    2
                ],
                [
                    "spoken language",
                    2
                ],
                [
                    "python programming language",
                    2
                ],
                [
                    "market",
                    2
                ],
                [
                    "machine",
                    1
                ],
                [
                    "training",
                    1
                ],
                [
                    "indicator",
                    1
                ],
                [
                    "article",
                    1
                ],
                [
                    "series",
                    1
                ],
                [
                    "group",
                    1
                ],
                [
                    "weather",
                    1
                ],
                [
                    "significant",
                    1
                ],
                [
                    "globe",
                    1
                ],
                [
                    "system",
                    1
                ],
                [
                    "control",
                    1
                ],
                [
                    "operating system",
                    1
                ],
                [
                    "general",
                    1
                ],
                [
                    "interest",
                    1
                ],
                [
                    "machine learning",
                    1
                ],
                [
                    "fall",
                    1
                ],
                [
                    "parameter",
                    1
                ],
                [
                    "information",
                    1
                ],
                [
                    "link",
                    1
                ],
                [
                    "feel free",
                    1
                ],
                [
                    "state",
                    1
                ],
                [
                    "english dictionary",
                    1
                ],
                [
                    "data",
                    1
                ],
                [
                    "nlp",
                    1
                ],
                [
                    "speech tagging",
                    1
                ],
                [
                    "mean",
                    1
                ],
                [
                    "economy",
                    1
                ],
                [
                    "matter",
                    1
                ],
                [
                    "animal",
                    1
                ],
                [
                    "block",
                    1
                ],
                [
                    "core",
                    1
                ]
            ],
            "description": "Natural Language Processing is the task we give computers to read and understand (process) written text (natural language). By far, the most popular toolkit or API to do natural language processing is the Natural Language Toolkit for the Python programming language. \n\nThe NLTK module comes packed full of everything from trained algorithms to identify parts of speech to unsupervised machine learning algorithms to help you train your own machine to understand a specific bit of text. \n\nNLTK also comes with a large corpora of data sets containing things like chat logs, movie reviews, journals, and much more!\n\nBottom line, if you're going to be doing natural language processing, you should definitely look into NLTK!\n\nPlaylist link: https://www.youtube.com/watch?v=FLZvOKSCkxY&list=PLQVvvaa0QuDf2JswnfiGkliBInZnIC4HL&index=1\n\nsample code: http://pythonprogramming.net\nhttp://hkinsley.com\nhttps://twitter.com/sentdex\nhttp://sentdex.com\nhttp://seaofbtc.com",
            "dislikeCount": "52",
            "duration": "PT19M54S",
            "likeCount": "3024",
            "published_time": "2015-05-01T16:25:39.000Z",
            "tags": [
                "Programming Language (Software Genre)",
                "Outline Of Natural Language Processing",
                "Natural Language Toolkit (Software)",
                "Python (Programming Language)",
                "Natural Language Processing (Software Genre)",
                "Computer Science (Field Of Study)",
                "tokenizing",
                "sent_tokenize",
                "word_tokenize",
                "nltk",
                "nlp"
            ],
            "thumbnail": "https://i.ytimg.com/vi/FLZvOKSCkxY/hqdefault.jpg",
            "title": "Natural Language Processing With Python and NLTK p.1 Tokenizing words and Sentences",
            "transcript": "  hello everybody and welcome to a Python programming tutorial for the NLT K or natural language toolkits module the natural language toolkit module is for natural language processing or NLP so what is that so natural language processing is the process of getting a computer to understand natural language now usually this is in the form of written language and sometimes it can be in the form of spoken language but usually spoken language gets converted to written language then to numbers but sometimes it doesn't it just gets straight converted to numbers as well so it is the process of converting some form of language to something that the computer can understand which is numbers so what can we do with that so n LT K is actually the first module that I ever worked with and is actually the reason why I chose the Python programming language because really no other programming language has any sort of API or module or whatever you want to call it for natural language processing so this here is my example this is my personal company or business that uses sentiment analysis which is a form of natural language processing so I'll just show real quick just 30 seconds some of the things that we do here just as an example what you can do but obviously sentiment analysis is such a tiny portion of what all you can do with a computer that can understand text so for example we do sentiment analysis for finance stuff so this would be for stocks so we could choose Apple for example and this is the sentiment analysis for Apple and we can actually see this sentiment has been going up pretty pretty strong and today's actually a pretty good day for Apple as well so there's that then we've liked politics so we measure sentiment on political issues we've got about 50 different political issues war is the most popular by far than NSA economy oil immigration you just keep going down we also have sentiment on politicians themselves this is not just the indicator obviously we have graphs too so there's sentiment for Barack Obama for example anyway so we do stuff like that and then also we have geographical this is probably my favorite one that I have so far but based on what people are saying and where they are from I plot it up on a globe we go basically by city so this gets as granular as per city and so we can do this we can get you know the last 30 days of sentiment globally but also we can find out what are the popular topics that people are talking about so for the United States or North America rather you've got love YouTube youtubes probably just because people are linking to YouTube videos Durer that's literally turn that's that okay so people are really talking about that right now for some reason I believe that's the most popular thing in the last one week right now yeah you've got Supreme Felix but anyways you can find out what people are talking about and their opinion on it in via the location so I think that's pretty cool can we do that for other locations as well so that's just a quick example of something that you can do and how powerful natural language processing can become and how you can translate just natural language to all sorts of things that are pretty cool and useful so moving on we've got n ltk org is basically where you're going to go to acquire n lck or any information on alt K although you don't actually have to get it from here and this is probably the more difficult path that you would take to even get it so the shorter path to getting it let me just close the existing one and let's open up the command prompt so to get that what you would do is if you've just installed Python obviously you're going to need Python to do this so if you don't have Python like this might actually be a lot of people's first foray into the Python programming language as it was mine because of NLT k so if it is your first you will need Python and we don't have Python up so let me pull up Python or click python.org and what you want to do if you don't have python go to downloads and then I'm pretty sure this just senses my operating system so you would just choose Python 3.4 or whatever the latest version of python is it should be cross-compatible the only thing that won't be is with two so please don't use two otherwise download that that will be the 32-bit version of Python and that should be fine so once you have that you should be able to open up bash your command prompt whatever and just do a simple pit install and ltk I would hit enter but it's not going to work in fact I'll show you it's just not going to work because this is four to seven and I have a 64-bit version of to seven so it's going to get angry if you are somebody who has a 64-bit version of Python you'll want to go to that website usually link to when you go ahead and pull it up real quick that is what's taking a while this website and then you can do ctrl f NL TK click on that and then here's your allen ck wheel and just click on that download it and what that's going to do is allow you to install it with pip and if you don't know how to install things with pip luckily for you there's a website for that and that's this website whoa it's mine so he can go to start learning here that's in the basics which is control f4 pip and bang here we have a pip installation tutorial which will also cover how to install things with NLT or I'm sorry with the wheel files like from this website for example so you can grab this if you don't know how to use pip otherwise once you have an LT k and you've got it installed or at least you think you do you're ready to move on to the next part so I'm going to go ahead just minimize this stuff because we don't need this right now so the next step that you're going to want to do is you're going to make sure you can go import NLT K in your idle or whatever IDE you use so this is just where you type your code so I like to use idle everyone has their own favorite and everyone thinks everyone should use theirs I like idle you can use whatever you want pycharm or whatever if you don't know what an IDE is this is ID le and if you don't know how to get there hit go up to your search or start bardu ID le and then you'll just click on one of these this is the one I'm going to be using it'll pop up a window like this go file new file and this is your new Python file then when you're ready you can save it save it wherever you want eventually deploy matter where you have it but about that later and you'll just have to save it to run it and that's about it so I'm going to close this because I've already got one open whoops I guess I hit yes and once you have yours open do import NLT kay and let's go ahead and make sure that works first so press the f5 key to run it it'll save it we'll run it it's taking a second and boom so the import of NLT Kay worked great now what we want to do is go ahead and do NLT Kay download and then empty parameters run that and you should get a pop-up window not just this one but another one I'm waiting for it sometimes it doesn't pop up as it should it's not here yet but it'll show hopefully soon yeah so popped under here it is so this is the window you get now if you are operating headless so say you're operating by a shell or something like that you can still do this you don't actually need a GUI or X or whatever you can still do NLT kay download and then you'll go to like downloads I think and then it'll be like identifier and just type in a ll for all hit enter and you'll download everything if you have a GUI yours looks like this only everything's probably read make sure you go to all and just download all this process can take anywhere from a few minutes to hours depending on your internet connection so choose all hit download and pause the video for now and resume the video once you have everything downloaded okay so once you have everything downloaded what you're going to want to do is maybe see like a real basic and quick example of what ltk can do for you so NLT Kay a natural language processing obviously is an interest for a lot of people that you know want to have computers to read or understand text or speech or whatever so what is the first step that you might do when you want to pull apart a body of text let's say well you're going to want to organize it somehow so let's say you're looking at an article on I don't know the Wall Street Journal and you don't know how to take your next step well your next steps probably going to be either separating paragraph or even separating by sentence and then maybe storing generally you're probably separate by sentence and then store a little identifier about which paragraph that sentence was a part of because if you if you think about it when people write if they write well they have paragraphs that contain you know main ideas and sentences that kind of back up that main idea of that paragraph or the subject of that paragraph whatever it happens to be so you do want to keep that in mind you don't want to just throw that out the window because that you know if the author is is a good author they're actually doing you quite a bit of a favor so it's not like you want to throw that out of the window but organizing by paragraph that's easy you know you split by a new line or something like that no problem now how about organizing by sentence or something like that that gets a little harder so what I want to talk about is tokenizing before we move on to the next video but before we get there probably just knock out a few kind of terms for natural language processing so you have yo seer terms like tokenizing what does tokenizing mean it's a form of grouping things so generally you're going to have two forms of tokenizer x' you're going to work tokenizer x' and then you're going to have sentence tokenizer x' and what they do is a word tokenizer just separates by words sentence tokenizer separates by sentence easy so just keep that in mind that's what tokenizing is you're also going to hear terms of lexicon and corporals what the heck so a corpora is just a body of text so think about a corpora might be a body of medical journal journals so example would be medical journals so this is kind of like it's a body of text where they're all kind of around the same thing so you might have medical journals you might have an example of maybe presidential speeches was another one we've got lexicon and also a core probe would be anything in the English language that's another example of a corpora then you've got lexicon and look Scott you can just think lexicon like a dictionary okay this is the words and their meanings now again this varies right so for the English language that would be like the English dictionary but consider for example the difference between investors speak and regular English speak okay so the difference we can see with someone who is a a bull versus someone who is a bear right so let's say investor speak bowl investor speak bowl equals someone who is positive about the market right that's a bowl some bullish on the market as opposed to you know English speak which is just the general English language that Bowl is you know scary animal you don't want running at you right that's a bull so keep that in mind the difference between corpora just a body of text lexicon is the words and their meaning basically and then as when you convert two numbers words and their and their values so those are some words let's talk about tokenizer z' for now and then we'll conclude this video so i just want to show you a real quick example of something real basic with NLT k yet extremely powerful and valuable as well so let's get rid of this NLT k download nonsense and first we're going to say uh we're going to do from NLT k tokenize we want to import the cent tokenize in word tokenize you might be able to guess what these do okay so let's just come down here I'm going to leave these just for the record I always post all of my code online so you'll be able to find the code here by going to Python program Internet dashboard data analysis and NLT kay it'll be right here this is the older version of NLT Kay I did have a series on NLT Kay long ago is filmed with potato and it needed to be updated very badly so here it is so you'll be able to find it here anyways whenever this is live I also post all the source code on github slash Python programming so plenty of options for you and obviously if you're falling behind or have questions you can always post on the video anyway moving on I'm going to leave this here just for the sample code purposes not everybody watches the videos so let's say we have an example text and this is going to be a sentence so our group of sentences will be something like hello there how are you so if I was to ask you first of all to separate this sentence by word how might you do it well most people are gonna be like okay that's simple uh we're going to just separate split by space right that's good enough that would split every single one of these words and then if we said um let's see how are you doing today the weather is great and Python is awesome okay and then let's say let's do the sky is pinkish blue you should not eat cardboard ok so now how would you separate so you've decided you're gonna separate words by a space ok that's probably like 85% accurate now what about splitting up sentences well you'd say probably okay that's easy enough we're going to use punctuation followed by a space followed by a capital letter that's pretty good too but what if we had something like this hello mr. Smith we've got punctuation space capital that is not a new line or a new sentence rather so things like this are going to trip you up really fast now of course you could build a pretty good regular expression to split by sentence and split by words but it would be a pretty significant one to get as much as an Tek is going to get so that's basically how n LT K does it so we're going to utilize n LT K to split this by sentence and by word and at least show you how powerful n LT K is and save you like hour years of riding around regular expressions okay so first let's do by sentence so let's print sense underscore tokenize and we want to sent tokenize example text so I'm going to copy and paste example text right in there so let's print that okay and it creates a list okay so this is just denote denoting that this is a Python list so the first element hello mr. Smith how you doing today so it did not fail or fall for this right and it captured everything awesome so then what if we wanted to do by word okay so now we can do print word underscore tokenize example texts we'll leave the other one there it's fine to leave it there now you can see it split it by word and again it left mr. period as its own word entirely because as you'll see it actually treats punctuation as its own board you can do away with that if you want by default it recognizes punctuation as its own kind of meaning so it's going to split that but as you can see it did not do it there as a successful catch otherwise it splits everything as you might expect now this is again a Python list but of course if you wanted to reference individual elements you could do something pretty simple so if you want to comment out a block of text it's really simple highlight everything alt 3 if you're an idol anyways and that will do it now what we could do is we could say 4 5 in and we'll do word tokenize we'll just say it will highlight this copy paste for I so for each element basically in word tokenize example text print I save it run that and I've got a nice output hello mr. Smith how are you doing today and so on ok so um those are just some real quick example of how we can begin to pull apart text and even sentences and then obviously like I said paragraphs it's not really so necessary to tokenize by paragraph because it's really simple to tell what a paragraph is but telling sentence is not so easy and telling words surprisingly enough not so easy now of course this is just the real basic stuff this is more of pre-processing of anything rather than any sort of analysis or anything like that but as we go on we'll see that NL CK can do really powerful things like part of speech tagging where it recognizes what part of speech things are and all that it's a lot more complex these are things that you probably wouldn't be able to do even in a few hours with regular expressions so that's we're going to start talking about the only other thing I will mention is that tokenizer x' we'll talk a little bit more in one of the next videos maybe the next one about some of the various forms of tokenizer x' so there's more than just this basic scent tokenizer there are some more advanced ones where you can actually use unsupervised machine learning built into an LT k you don't even know how it works you just use it to make your own tokenizer is entirely based on your fancy word corpora so that's that the only other thing I will say too is on CK by default works with the English language for the most part but it actually does work with other languages so if you're looking to do this with Spanish or whatever else look into it because it probably is included with NL CK it's not probably not going to be as built up as the English language is but the other major languages are actually pretty well built up so make sure to check that out and if not they can be because there are trainers and ltk trainers for just about anything you could make your own language training ltk to it and it would work so anyway a little bit long for the first video most videos won't be this long but I did want to give you guys at least a quick taste of the power of nl t cane really you can utilize this power in about three or four lines and see how how incredible it is already so I think it's really cool so there's a lot of cool stuff coming as always if you have any questions or comments please feel free to leave them below otherwise as always thanks for watching thanks for all the support and subscriptions until next time",
            "userFeedbackScore": 0.5929804968324048,
            "videoid": "FLZvOKSCkxY",
            "viewCount": "380545"
        },
        "FggN0VtWYTk": {
            "NumOfComments": 2,
            "caption_exist": "T",
            "channel_id": "UCBVCi5JbYmfG3q5MEuoWdOw",
            "channel_title": "Udacity",
            "comment_sentiment": 0.4311813186813187,
            "concepts": [
                [
                    "language",
                    6
                ],
                [
                    "human",
                    4
                ],
                [
                    "human experience",
                    2
                ],
                [
                    "speech",
                    2
                ],
                [
                    "help",
                    2
                ],
                [
                    "translation",
                    2
                ],
                [
                    "machine",
                    1
                ],
                [
                    "text",
                    1
                ],
                [
                    "processing",
                    1
                ],
                [
                    "process",
                    1
                ],
                [
                    "machine translation",
                    1
                ],
                [
                    "communication",
                    1
                ],
                [
                    "computer",
                    1
                ],
                [
                    "find",
                    1
                ],
                [
                    "skill",
                    1
                ],
                [
                    "natural",
                    1
                ],
                [
                    "power",
                    1
                ],
                [
                    "modern",
                    1
                ],
                [
                    "understand",
                    1
                ],
                [
                    "speech recognition",
                    1
                ],
                [
                    "neural network",
                    1
                ],
                [
                    "natural language processing",
                    1
                ],
                [
                    "human language",
                    1
                ],
                [
                    "level",
                    1
                ],
                [
                    "computing",
                    1
                ]
            ],
            "description": "Master the skills to get computers to understand, process, and manipulate human language. Build models on real data, and get hands-on experience with sentiment analysis, machine translation, and more: udacity.com/nlp",
            "dislikeCount": "5",
            "duration": "PT1M21S",
            "likeCount": "101",
            "published_time": "2018-03-27T16:48:41.000Z",
            "tags": [
                "learn nlp",
                "nlp training",
                "nlp techniques",
                "nlp tutorial",
                "nlp books",
                "nlp ai",
                "learn sentiment analysis",
                "learn speech tagging",
                "learn machine translation",
                "learn speech recognition",
                "natural language processing",
                "udacity nlp"
            ],
            "thumbnail": "https://i.ytimg.com/vi/FggN0VtWYTk/hqdefault.jpg",
            "title": "Natural Language Processing Nanodegree Program",
            "transcript": "  language it's how we express ourselves inspire connect language has helped our society evolve where there is human experience there is language what if we can find insights in all that speech and all that text that's around us modern computing is now reaching the power to process human language from capabilities like universal translation to summarizing legal documents and even understanding human emotion we're just beginning to explore the possibilities that's why we've developed the natural language processing nanodegree program master the skills that are changing the world you'll build a github portfolio of projects with machine translation neural networks and speech recognition because when you teach computers to not just understand language but to actually generate new communication you can help take the human experience to a whole new level Georgina said Udacity get started and then help [Music]",
            "userFeedbackScore": 0.9015977787593865,
            "videoid": "FggN0VtWYTk",
            "viewCount": "65670"
        },
        "J5IlKj7H8T8": {
            "NumOfComments": 3,
            "caption_exist": "T",
            "channel_id": "UCxPJljXUHvUd9idyfEHvXqg",
            "channel_title": "IBM Watson",
            "comment_sentiment": 0.08333333333333333,
            "concepts": [
                [
                    "language",
                    10
                ],
                [
                    "natural",
                    10
                ],
                [
                    "service",
                    9
                ],
                [
                    "natural language understanding",
                    7
                ],
                [
                    "article",
                    6
                ],
                [
                    "text",
                    5
                ],
                [
                    "natural language understanding service",
                    4
                ],
                [
                    "content",
                    4
                ],
                [
                    "model",
                    4
                ],
                [
                    "watson",
                    4
                ],
                [
                    "knowledge",
                    3
                ],
                [
                    "find",
                    3
                ],
                [
                    "data",
                    2
                ],
                [
                    "extract metadata",
                    2
                ],
                [
                    "processing",
                    2
                ],
                [
                    "ibm watson",
                    2
                ],
                [
                    "natural language processing",
                    2
                ],
                [
                    "ibm watson knowledge studio",
                    2
                ],
                [
                    "category",
                    2
                ],
                [
                    "first",
                    2
                ],
                [
                    "developer",
                    2
                ],
                [
                    "ai",
                    1
                ],
                [
                    "peripheral",
                    1
                ],
                [
                    "oncology",
                    1
                ],
                [
                    "analysis",
                    1
                ],
                [
                    "web",
                    1
                ],
                [
                    "quantity",
                    1
                ],
                [
                    "trained",
                    1
                ],
                [
                    "supercomputer",
                    1
                ],
                [
                    "unstructured content",
                    1
                ],
                [
                    "link",
                    1
                ],
                [
                    "parameter",
                    1
                ],
                [
                    "human",
                    1
                ],
                [
                    "solution",
                    1
                ],
                [
                    "understand",
                    1
                ],
                [
                    "help",
                    1
                ],
                [
                    "catalog",
                    1
                ],
                [
                    "documentation",
                    1
                ],
                [
                    "cancer",
                    1
                ],
                [
                    "human language",
                    1
                ],
                [
                    "create",
                    1
                ]
            ],
            "description": "IBM Watson Natural Language Understanding analyzes text to extract meta-data from content such a concepts, entities, keywords, categories, sentiment, emotion, relations, and semantic roles. With custom annotation models developed using Watson Knowledge studio you can tailor your solution towards industry / domain specific entities and relations in unstructured text. For more information, visit: https://ibm.co/2s995MY",
            "dislikeCount": "1",
            "duration": "PT3M6S",
            "likeCount": "84",
            "published_time": "2017-02-27T22:03:53.000Z",
            "tags": [
                "Watson Natural Language",
                "IBM Watson"
            ],
            "thumbnail": "https://i.ytimg.com/vi/J5IlKj7H8T8/hqdefault.jpg",
            "title": "Watson Natural Language Understanding Service Overview",
            "transcript": "  welcome to the IBM wants a natural language understanding service the volume of unstructured and available is rapidly growing as news articles tweets blog posts and more are published online constantly as a developer do you need a way for your app to understand the complexities of human language to derive fast meaningful insights that may otherwise be missed from these large quantities of data the is a set of sophisticated natural language processing capabilities that allow users to analyze text and extract metadata from content such as concepts entities keywords categories relations semantic roles sentiments and emotion in this video you'll see how easy it is to analyze and draw insights from your contacts you'll also find out how to use a custom annotation model that was developed using IBM Watson knowledge studio to identify industry or domain-specific entities and relations the example in this video analyzes a news article about how IBM supercomputer is bringing AI fuel cancer care to everyday Americans to start using the service first log into your bluemix account you'll find the natural language understanding service in the catalog you can input the article in plain text HTML or a link to a public web page if you upload HTML or provide a URL the service cleans and normalizes the data by extracting the important text and removing ads and other peripheral content next you analyze the article content using sophisticated natural language processing techniques first you specify the concepts entities and keywords as the parameters you want to you also can analyze the text to identify sentiment and emotion find out if the sentiment of the article and entities is positive or negative gain insights into the emotions the writer was feeling using natural language understanding you can determine where in the article the writer is expressing anger disgust sadness fear or joy what's a natural language understanding also analyzes categories relations semantic roles and other metadata use the custom models developed using IBM Watson knowledge studio within the natural service to tailor the solution by extracting domain-specific entities and relations in this example you could create a custom model tailor to oncology or healthcare to extract domain-specific entities you want to identify in our analysis you can refer to the Watson knowledge studio documentation for instructions on how to deploy a trained model to the natural language understanding service now that you know how the natural language understanding service can extract metadata from unstructured content you can incorporate this service into powerful applications that help you derive insights from your unstructured text get started using natural language understanding and other Watson developer services today [Music]",
            "userFeedbackScore": 0.41468030690537083,
            "videoid": "J5IlKj7H8T8",
            "viewCount": "24105"
        },
        "MNvT5JekDpg": {
            "NumOfComments": 34,
            "caption_exist": "T",
            "channel_id": "UCJS9pqu9BzkAMNTmzNMNhvg",
            "channel_title": "Google Cloud Platform",
            "comment_sentiment": 0.4314198179271708,
            "concepts": [
                [
                    "yeah",
                    33
                ],
                [
                    "data",
                    22
                ],
                [
                    "language",
                    21
                ],
                [
                    "natural",
                    20
                ],
                [
                    "model",
                    15
                ],
                [
                    "research",
                    9
                ],
                [
                    "structured data",
                    8
                ],
                [
                    "natural language generation",
                    8
                ],
                [
                    "neural network",
                    8
                ],
                [
                    "weather",
                    7
                ],
                [
                    "rule",
                    7
                ],
                [
                    "human",
                    7
                ],
                [
                    "computer",
                    7
                ],
                [
                    "question",
                    7
                ],
                [
                    "cloudy",
                    6
                ],
                [
                    "recurrent neural network",
                    6
                ],
                [
                    "machine",
                    5
                ],
                [
                    "machine learning",
                    5
                ],
                [
                    "degree",
                    5
                ],
                [
                    "first",
                    5
                ],
                [
                    "step",
                    4
                ],
                [
                    "conversational",
                    4
                ],
                [
                    "information",
                    4
                ],
                [
                    "link",
                    4
                ],
                [
                    "word",
                    4
                ],
                [
                    "good",
                    4
                ],
                [
                    "nlp",
                    4
                ],
                [
                    "response",
                    4
                ],
                [
                    "processing",
                    3
                ],
                [
                    "column",
                    3
                ],
                [
                    "field",
                    3
                ],
                [
                    "pretty",
                    3
                ],
                [
                    "recurrent neural net",
                    3
                ],
                [
                    "general",
                    3
                ],
                [
                    "diagonal line",
                    3
                ],
                [
                    "thing",
                    3
                ],
                [
                    "understand",
                    3
                ],
                [
                    "user question",
                    2
                ],
                [
                    "sound",
                    2
                ],
                [
                    "time step",
                    2
                ],
                [
                    "location",
                    2
                ],
                [
                    "ai adventure",
                    2
                ],
                [
                    "deep neural network",
                    2
                ],
                [
                    "temperature",
                    2
                ],
                [
                    "future",
                    2
                ],
                [
                    "ai",
                    2
                ],
                [
                    "sound good",
                    2
                ],
                [
                    "basically",
                    2
                ],
                [
                    "natural language processing",
                    2
                ],
                [
                    "specific location",
                    2
                ],
                [
                    "content",
                    2
                ],
                [
                    "conversational user interface",
                    2
                ],
                [
                    "mean",
                    2
                ],
                [
                    "paying attention",
                    2
                ],
                [
                    "architecture",
                    2
                ],
                [
                    "generation side",
                    2
                ],
                [
                    "focus",
                    1
                ],
                [
                    "target",
                    1
                ],
                [
                    "policy",
                    1
                ],
                [
                    "pretty cool",
                    1
                ],
                [
                    "shower",
                    1
                ],
                [
                    "solution",
                    1
                ],
                [
                    "computer system",
                    1
                ],
                [
                    "find",
                    1
                ],
                [
                    "visualization",
                    1
                ],
                [
                    "order",
                    1
                ],
                [
                    "audio",
                    1
                ],
                [
                    "range",
                    1
                ],
                [
                    "parsing",
                    1
                ],
                [
                    "level",
                    1
                ],
                [
                    "stable",
                    1
                ],
                [
                    "input/output",
                    1
                ],
                [
                    "neuron",
                    1
                ],
                [
                    "conglomerate",
                    1
                ],
                [
                    "graph",
                    1
                ],
                [
                    "feel free",
                    1
                ],
                [
                    "system",
                    1
                ],
                [
                    "overview",
                    1
                ],
                [
                    "text",
                    1
                ],
                [
                    "lattice",
                    1
                ],
                [
                    "form",
                    1
                ],
                [
                    "engineering",
                    1
                ],
                [
                    "second",
                    1
                ],
                [
                    "common",
                    1
                ],
                [
                    "matter",
                    1
                ],
                [
                    "algorithm",
                    1
                ]
            ],
            "description": "In this episode of AI Adventures, Yufeng interviews Google Research engineer Justin Zhao to talk about natural text generation, recurrent neural networks, and state of the art research!\n\nRNNs in TensorFlow: https://goo.gl/ss5dEY\nCharacter-level language models: https://goo.gl/ffcq52\n\nWatch more episodes of AI Adventures: https://goo.gl/UC5usG\n\nSubscribe to get all the episodes as they come out: https://goo.gl/S0AS51",
            "dislikeCount": "16",
            "duration": "PT14M40S",
            "likeCount": "824",
            "published_time": "2017-10-19T20:42:18.000Z",
            "tags": [
                "Machine Learning",
                "TensorFlow",
                "Big Data",
                "Cloud",
                "Artificial intelligence",
                "AI",
                "ML",
                "machine learning with gcp",
                "gcp machine learning",
                "cloud and machine learning",
                "ai adventures",
                "training",
                "machine learning models",
                "natural language generation",
                "natural language processing",
                "research",
                "natural text generation",
                "product: machine learning",
                "fullname: Yufeng Guo",
                "Location: MTV",
                "Team: Scalable Advocacy",
                "Type: DevByte",
                "GDS: Full Production"
            ],
            "thumbnail": "https://i.ytimg.com/vi/MNvT5JekDpg/hqdefault.jpg",
            "title": "Natural Language Generation at Google Research (AI Adventures)",
            "transcript": "  welcome today on AI adventures we're joined in the studio by Justin Zhou a Google research engineer hey Justin all right thanks for joining in the studio today yeah it's great to be here we're gonna be talking today about natural language interfaces and how computers and humans can talk to each other in ways that are natural and not awkward yep sounds good awesome so I want to start by talking a little bit about your team's area of research and kind of the general natural language processing field and then we'll delve into your area of research and see where our conversation takes us yeah that sounds great so broadly speaking the area of my research is natural language processing or NLP and what that is NLP is all about trying to understand how humans communicate with each other and how to get a computer to kind of replicate that behavior so that we can interact with computers in a more natural manner wow you guys really picked a small field to target yeah I know if he sounds super broad yeah it's like everything yeah it's pretty broad um so in fact like I have some slides that we can pull yeah that'd be great yeah yeah so first I think it's important to talk about the conversational user interface and for something like the Google assistant there's two big domains of NLP problems that come into play on one side you have the problem of understanding which is what did the users say what was the user's intent and on the other side you have the problem of generation which was what should we say to the user and how do we respond in a way that's intelligent and conversational right so I work on the generation side and the ultimate goal of natural language generation is to teach computers to turn some kind of structured data into natural language which we can use to respond to the user in a conversation wonderful and this is definitely something that I feel like conventionally NLP has really been broadly thought about as a field where it's all about processing the words and understanding what text means but you are working on the generation side which in a lot of ways often gets overlooked and so it's really great that you're able to kind of tell us more about this side of things yeah mmm that's what I'm here for so how do you then teach a computer to generate natural language rather than just understand it right so for now let's set aside the structured data you know part of natural language generation and we can focus on the natural part of the natural language generation so what makes a conversation like the one we're having no human speaking of the one we're having it's a little meta that we're having a conversation about what makes something so that's a common remark on our team yeah we have to not be too robotic in our conversation yeah so I think this breaks down into two kinds of requirements first of all the content of what we have to say it has to make sense in the context of the conversation so is what I'm saying an appropriate response to what you're saying does or is it out of the blue hey what are you having for dinner so that's kind of out of the loop that's kind of out of the blue yeah definitely so yeah exactly and then I have to think about if what I'm gonna say is actually gonna answer your question so if you were gonna ask me where we want to go for dinner it would be weird to just to suggest like a coffee shop or a clothing store right yeah yeah unless you really wanted to get some coffee stains on your clothes for dinner yeah I guess the second requirement is that you accept to use the language correctly so this is like you know how's my grandma do my verbs agree or you know if I'm using a pronoun is it ambiguous that makes sense so it's basically what do you say and then how do you say it exactly okay yeah and you also mentioned earlier there's this structured data that we kind of put aside where does that come into play that's a great question so structured data primarily helps us figure out the first requirement which is what we want to say for example if let's say a user asked us about the weather next week in Santa Clara in Google search results we see a box filled with all this information about the weather for the next week okay and some are within this data hopefully answers the users question and we just have to figure out all this data into a response to the user that's kind of the problem that we're focusing on in natural language generation and that's because we're talking about a situation where we're going to say our answers not just show them the box to look at that's correct it's like an audio interface in that case I guess I can imagine a sort of naive solution for this sort of problem we already have the data right yeah so but I don't know if it would be sufficient well you know that's depends by all means go for it right so let's say we make some kind of a template right and we can say you know on blank day it'll be blank temperature and then some blank weather condition like on Tuesday it'll be 72 degrees and partly cloudy and then you could build a full forecast by just iterating through all the days of the week like that so I will say that that is a very straightforward approach and some assistants do use that implementation however in practice it's a lot less conversational than you might think so how about you try asking me what's the weather like this week and then I'll use your algorithm to generate a response all right sounds good we'll call this the Adjustment Assistance set perfect okay Justin what's the weather like next week hi you Fang Sunday it'll be 66 degrees and partly cloudy Monday it'll be 63 degrees and cloudy Tuesday it will be 66 degrees and partly cloudy Wednesday it'll be 68 degrees in cloudy oh boy okay yeah that's too long and just too robotic yeah let's let's call it let's call it that yeah you've been saying it for me you felt a little strange yeah so clearly generating natural language from structured data is non-trivial how would you actually go about using a computer system to answer the users question then well first you know I would want to think about how it answered as a human so you know as a human I would hope that I'd be a little more contextually aware and I would realize that there's actually a lot of repetitive information in the data so I'd probably try to summarize it something like it'll be cloudy until Thursday with showers the rest of the week temperatures range from the high 40s to the mid 60s hey you might want to consider a career as a weather forecaster if you know the research thing doesn't work out yeah maybe all right so we've done a little bit of an overview of natural language generation about what makes conversation natural and we even gave kind of a admittedly silly example of leveraging this structured data to select content for a natural language response yeah and we've also included some links with more info in the video that's right now we have all right so then getting back to the topic at hand how how does machine learning then get involved well that's the ultimate question that our team is trying to answer without machine learning everything that we've talked about so far from parsing the data to figure out what to say to actually figure out how to say it it had you have to do this with writing lots of rules and rules are great they're very stable they're very predictable but they're usually very specific and they require a lot of engineering and because of that it's not really scalable to new inputs and outputs for example if we wanted to talk about finance instead of weather or if we wanted to support an entire new language altogether sure it would require writing a whole new set of rules yeah and it sounds like that would be way harder to maintain as well keeping all those rules lined up as things change and it would also be hard to replicate that creativity and spontaneity that comes with human conversation right so that's exactly one of the motivations of our research our hope is that by giving the model examples of data and the language it needs to generate we can let the model form its own rules about what to do and not only does this save us from having to write these rules ourselves by hand but it also gives the computer more free rein to be creative in its own way so showing many examples to answer questions you might say so that you can write fewer rules I mean that's kind of the crux of machine learning as a whole that's wonderful exactly yeah and and so what kind of machine learning architectures then are you guys exploring to try to protect this problem well so far we've seen really promising results with recurrent neural networks but that's just one kind of neural architecture that we're exploring okay recurrent neural networks so on our previous episode we looked at deep neural networks on the show and that had neurons connected in layers resulting in something kind of lattice structure right and for our viewers can you explain what it means to have a recurrent neural network yeah so you can kind of think of a recurrent neural network as a deep neural network but just wrapped in a for loop and the network is recurrent because the outputs of the network feed back into itself and instead of this kind of one-shot you know input/output the model can kind of make decisions over several time steps okay awesome that's a really great way that I kind of conceptualize it really love that and we've also included some links about recurrent neural networks down below and if you have more questions about this kind of network structure feel free to leave them below in the comments and we'll try to get to them for now we'll talk about why recurrent neural networks will be useful for doing natural language generation right so it's important to keep in mind that language just in general is extremely sequential sure yeah for example the cat sat on the mat is a very different sentence from cats at than that on yeah order matters definitely so are nuns are especially good at remembering what it kind of saw earlier because it enforces a sequential policy over the data the inputs are decided in a very ordered manner and instead of in these kind of large conglomerates okay so I guess it's both amazing and not entirely surprising that recurrent neural Nets would be useful for natural language problems it sounds like so as humans we rely a lot on what we previously said to figure out what we'll say next exactly well let's talk a bit more than on how you're using these recurrent neural nets to generate this language so one kind of fun variation when it comes to recurrent neural nets is that since the output is generated one step at a time you can kind of choose the granularity of your output so some models can choose coarser outputs like entire word phrases just words in general and then this goes all the way down to models that outputs like bytes a single bytes at a time by the time and for us the we've been using outputs at the character level okay so be like spelling out the words really right okay and this kind of model is called ik is a character based RNN and you can find out more information in the links below so when we first talked about having you on the show you showed me this interesting graph here right I would love to understand a little better what is it showing us exactly so this is a small visualization of our recent research each row here represents different pieces of our structured data the shading of the squares indicates how much the model actually cares about that piece of structured data and lastly each column represents a single step in our model so as we travel across the columns you can see how the model has learned to pay attention to the structure data at different time steps okay so we're kind of traveling left to right character by character for each column and so the lit portions the lighter parts are parts that the model is paying attention to right exactly okay and then on this model over here for example it means the model is paying attention to this bit to decide what character it out but it's not saying that that's the character we'll say it'll just that's just the data it's looking at right exactly so it's gonna look at that particular piece of data to try to think about what - what character town exactly and then one really cool result is this diagonal line in headed oh about that it's kind of formulaic and it almost looks like you guys added that in afterwards - yeah it'd make it make for something interesting it's like hard coded yeah so those particular pieces of data are basically the characters for a specific location and what that diagonal line is showing us is that when the model has reached the part of the sentence where it wants to spell out the specific location is learn to read that from the data character by character wow that is that is awesome and no one taught the model to do that they were just able to learn how to do that just by looking at examples exactly that's the magic ncredible that's that's super outstanding yeah so um the diagonal line is pretty cool but if you dive into our data there's actually a lot of other intriguing ways that the model kind of learns by itself how to reference the data to decide what character to output so you know that said there's still a ton to explore but I'm super excited to see what you know where we come up with in the future and how far we can push our research this looks super cool Justin and I am really excited to hear about what your team comes up with next maybe maybe you'll write a research paper using one of these networks interview yeah that sounds pretty fun yeah Justin I want to thank you so much for coming into the studio today and teaching our viewers about natural language generation looking forward to catching up again in a minute I'm gonna wrap up here yeah okay sounds good it was my pleasure all right well I hope you enjoyed this episode of AI adventures I certainly did in our conversation we talked about using machine learning for natural language generation and its role in conversational user interfaces I had a blast chatting with Justin and if you like this format please let us know in the comments below and for more information and details about everything that we talked about we've included tons of links in the description and be sure to subscribe to the channel to catch future episodes and maybe some more interviews as they come out",
            "userFeedbackScore": 0.9188195104128607,
            "videoid": "MNvT5JekDpg",
            "viewCount": "40894"
        },
        "PpGbujtnm1k": {
            "NumOfComments": 0,
            "caption_exist": "T",
            "channel_id": "UCR5bQwLkDls81oOkKbx45dw",
            "channel_title": "IBMJapanChannel",
            "comment_sentiment": 0,
            "concepts": [
                [
                    "language",
                    11
                ],
                [
                    "natural",
                    10
                ],
                [
                    "service",
                    9
                ],
                [
                    "natural language understanding",
                    8
                ],
                [
                    "article",
                    6
                ],
                [
                    "content",
                    6
                ],
                [
                    "text",
                    5
                ],
                [
                    "natural language understanding service",
                    5
                ],
                [
                    "model",
                    4
                ],
                [
                    "watson",
                    4
                ],
                [
                    "knowledge",
                    3
                ],
                [
                    "find",
                    3
                ],
                [
                    "data",
                    2
                ],
                [
                    "processing",
                    2
                ],
                [
                    "ibm watson",
                    2
                ],
                [
                    "natural language processing",
                    2
                ],
                [
                    "category",
                    2
                ],
                [
                    "ibm watson knowledge studio",
                    2
                ],
                [
                    "unstructured content",
                    2
                ],
                [
                    "first",
                    2
                ],
                [
                    "extract metadata",
                    2
                ],
                [
                    "developer",
                    2
                ],
                [
                    "quantity",
                    1
                ],
                [
                    "ai",
                    1
                ],
                [
                    "peripheral",
                    1
                ],
                [
                    "field",
                    1
                ],
                [
                    "oncology",
                    1
                ],
                [
                    "analysis",
                    1
                ],
                [
                    "web",
                    1
                ],
                [
                    "trained",
                    1
                ],
                [
                    "link",
                    1
                ],
                [
                    "parameter",
                    1
                ],
                [
                    "human",
                    1
                ],
                [
                    "solution",
                    1
                ],
                [
                    "understand",
                    1
                ],
                [
                    "help",
                    1
                ],
                [
                    "supercomputer",
                    1
                ],
                [
                    "documentation",
                    1
                ],
                [
                    "cancer",
                    1
                ],
                [
                    "human language",
                    1
                ],
                [
                    "create",
                    1
                ]
            ],
            "description": "Natural Language Understanding\u306f\u3001\u9ad8\u5ea6\u306a\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u6280\u6cd5\u3092\u4f7f\u7528\u3057\u3066\u8a18\u4e8b\u5185\u5bb9\u3092\u5206\u6790\u3057\u3001\u7b46\u8005\u304c\u6301\u3063\u3066\u3044\u305f\u611f\u60c5\u3092\u6d1e\u5bdf\u3057\u307e\u3059\u3002IBM Watson Knowledge Studio \u3092\u4f7f\u7528\u3057\u3066\u958b\u767a\u3055\u308c\u305f\u30ab\u30b9\u30bf\u30e0\u30fb\u30e2\u30c7\u30eb\u3092\u4f7f\u7528\u3057\u3066\u3001\u30c9\u30e1\u30a4\u30f3\u56fa\u6709\u306e\u30a8\u30f3\u30c6\u30a3\u30c6\u30a3\u30fc\u3068\u30ea\u30ec\u30fc\u30b7\u30e7\u30f3\u3092\u7279\u5b9a\u3059\u308b\u65b9\u6cd5\u3082\u3054\u7d39\u4ecb\u3057\u307e\u3059\u3002\n\n\u8a73\u7d30\u306f\u3053\u3061\u3089\uff1ahttps://www.ibm.com/watson/services/natural-language-understanding/",
            "dislikeCount": "0",
            "duration": "PT3M6S",
            "likeCount": "3",
            "published_time": "2017-10-05T08:30:12.000Z",
            "tags": [
                "NLU"
            ],
            "thumbnail": "https://i.ytimg.com/vi/PpGbujtnm1k/hqdefault.jpg",
            "title": "IBM Watson Natural Language Understanding\u6982\u8981",
            "transcript": "  welcome to the IBM wants a natural language understanding service the volume of unstructured content available is rapidly growing as news articles tweets blog posts and more are published online constantly as a developer do you need a way for your app to understand the complexities of human language to derive fast meaningful insights that may otherwise be missed from these large quantities of data the natural language understanding service is a set of sophisticated natural language processing capabilities that allow users to analyze text and extract metadata from content such as concepts entities keywords categories relations semantic roles sentiments and emotion in this video you'll see how easy it is to analyze and draw insights from your content you'll also find out how to use a custom annotation model that was developed using IBM Watson knowledge studio to identify industry or domain-specific entities and relations the example in this video analyzes a news article about how IBM supercomputer is bringing AI field cancer care to everyday Americans to start using the service first log into your bluemix account you'll find the natural language understanding service in the catalogue you can input the article in plain text HTML or a link to a public web page if you upload HTML or provide a URL the service cleans and normalizes the data by extracting the important text and removing ads and other peripheral content next you analyze the article content using sophisticated natural language processing techniques first you specify the concepts entities and keywords as the parameters you want to you also can analyze the text to identify sentiment and emotion find out if the sentiment of the article and entities is positive or negative gain insights into the emotions the writer was feeling using natural language understanding you can determine where in the article the writer is expressing anger disgust sadness fear or joy what's a natural language understanding also analyzes categories relations semantic roles and other metadata use the custom models developed using IBM Watson knowledge studio within the to tailor the solution by extracting in this example you could create a custom model tailor to oncology or healthcare to extract domain-specific entities you want to identify in our analysis you can refer to the Watson knowledge studio documentation for instructions on how to deploy a trained model to the natural language understanding service now that you know how the natural language understanding service can extract metadata from unstructured content you can incorporate this service into powerful applications that help you derive insights from your unstructured text get started using natural language understanding and other Watson developer services today",
            "userFeedbackScore": 0.3,
            "videoid": "PpGbujtnm1k",
            "viewCount": "833"
        },
        "QIdB6M5WdkI": {
            "NumOfComments": 0,
            "caption_exist": "T",
            "channel_id": "UCR2Lyk2J6bVjPADz33LXDgg",
            "channel_title": "Iridescent",
            "comment_sentiment": 0,
            "concepts": [
                [
                    "word",
                    22
                ],
                [
                    "chip",
                    12
                ],
                [
                    "language",
                    11
                ],
                [
                    "expensive",
                    9
                ],
                [
                    "thing",
                    8
                ],
                [
                    "processing",
                    6
                ],
                [
                    "natural",
                    6
                ],
                [
                    "natural language processing",
                    6
                ],
                [
                    "computer",
                    5
                ],
                [
                    "ai",
                    4
                ],
                [
                    "computer science",
                    3
                ],
                [
                    "bad thing",
                    3
                ],
                [
                    "expensive restaurant",
                    3
                ],
                [
                    "science",
                    3
                ],
                [
                    "human",
                    3
                ],
                [
                    "menu",
                    3
                ],
                [
                    "field",
                    2
                ],
                [
                    "data",
                    2
                ],
                [
                    "expensive potato chip",
                    2
                ],
                [
                    "data set",
                    2
                ],
                [
                    "distinction word",
                    2
                ],
                [
                    "artificial intelligence",
                    2
                ],
                [
                    "window",
                    2
                ],
                [
                    "understand",
                    2
                ],
                [
                    "analysis",
                    1
                ],
                [
                    "negation word",
                    1
                ],
                [
                    "linguistics",
                    1
                ],
                [
                    "web",
                    1
                ],
                [
                    "mean",
                    1
                ],
                [
                    "name",
                    1
                ],
                [
                    "form",
                    1
                ],
                [
                    "development",
                    1
                ],
                [
                    "dan jurafsky",
                    1
                ],
                [
                    "psychology",
                    1
                ],
                [
                    "common",
                    1
                ],
                [
                    "first",
                    1
                ],
                [
                    "negation",
                    1
                ],
                [
                    "competition",
                    1
                ],
                [
                    "research",
                    1
                ],
                [
                    "find",
                    1
                ],
                [
                    "big data",
                    1
                ],
                [
                    "state",
                    1
                ],
                [
                    "tool",
                    1
                ]
            ],
            "description": "Technovation 2016 Winner Jennifer John introduces Dan Jurafsky, Professor of Linguistics and Computer Science at Stanford University. Dan explains how natural language processing is transforming the way we interact with the world and understand ourselves.",
            "dislikeCount": "1",
            "duration": "PT5M25S",
            "likeCount": "33",
            "published_time": "2017-05-12T19:52:58.000Z",
            "tags": [
                "artificial intelligence",
                "AI",
                "computer science",
                "CS",
                "AAAI",
                "language processing",
                "dan jurafsky",
                "education",
                "artificial intelligence education",
                "learn ai",
                "artificial intelligence learn",
                "ai computer",
                "computer intelligence",
                "what is artificial intelligence",
                "machine learning",
                "neural network",
                "artificial intelligence definition",
                "artificial intelligence future",
                "+ai",
                "artificial intelligence revolution",
                "ai applications"
            ],
            "thumbnail": "https://i.ytimg.com/vi/QIdB6M5WdkI/hqdefault.jpg",
            "title": "Dan Jurafsky on Natural Language Processing",
            "transcript": "  hi my name is Jennifer and I won the new tech innovation competition in 2016 so a really interesting field in computer science has emerged over the past fifty fifty years Purcell is AI or artificial intelligence which is the idea that computers themselves can have some sort of form of intelligence that will allow them to solve important problems and so this development is very exciting because it allows computers to take on roles that previously we only thought that humans could possibly do so today we're going to be speaking with some leading researchers in the field of AI to hear about how they're applying a AI to their work in what sorts of achievements and research they're conducting to make AI even more intelligent hi i'm dan jurafsky I'm a professor at Stanford University of linguistics in computer science and I want to talk about natural language processing so natural language processing that anytime we use computer science techniques to analyze language you talk to your phone you translate languages every time you're doing a search on Google you're using natural language processing and what I want to do today is talk about how natural language processing can be used not just to build a widget but to actually understand something about ourselves and I want to start with a very everyday kind of language that we're going to use natural language processing to understand and that's the language of food we're going to look at some cheap potato chips some expensive potato chips and we've done this experiment in the lab you could do this at home and just looked at how the words differ on expensive chips versus cheap chips so one difference is what's been called technically distinction so the more expensive a chip the more a-- tries to distinguish itself from other chips for example words like more or less words that compare things words like best or finest or even just words for negation words like no or not or never and so sure enough if you if you just count the distinction words on the cheap chips and on expensive potato chips you find five times more of these distinction words on expensive chips so let's read the back of one of the most expensive Oh chip brands never fried never baked we don't fry it we don't bake it nothing fake or phony no fake colors no fake flavors no fluorescent orange fingertips no wiping your greasy chip hand on your jeans no lots of nose there you're paying four cents extra for every one of those nose so that's a simple analysis but we can do things that are much more complicated with artificial intelligence because we can look at big data sets on inexpensive menu the cheapest menus you get real whipped cream real mashed potatoes and real bacon bits on a slightly more expensive restaurant you might get real crab or real maple syrup but the most expensive restaurants just don't use the word real because everything is real or at least that's what you're supposed to think so if cheap restaurants are talking about adjectives and real things and the expensive restaurants aren't doing that what are they doing well they're they're using fancy words so they're using rare words words like toner le or bestia or percy odd fancy foreign words that you might have to ask the waiter what they mean or they just use longer versions of common words so you might see words like decaffeinated or accompaniment instead of decaf or sides and in fact you can measure these rare words because rare words tend to be longer you can just ask how much does the length of a word associate with the price of the dish and it turns out every time you add an extra letter to the average length of a word in the dish you're paying 18 cents more for that dish alright so we've looked at potato chips and we've looked at menus I want to look at one final kind of data set and that's reviews the web is full of restaurant reviews and I'm showing you here a negative restaurant review so this is a one star review somebody really hated this restaurant you'll notice it's got words like absolutely horrible and we waited ten minutes and we will not return when we first began to look at these we thought oh these one-star restaurant reviews are going to be about greasy food but there's no mention of the food at all what's going on so we looked at all the one-star reviews and looked at the characteristics of their language negative words words like horrible or awful bad stories in the past tense so use words like waited or didn't not wait or do a bad thing happened we use the past tense instinctively to put this bad thing in the past you know this this thing is over and we use the words we in us to say we're going to get through this bad thing together and what that tells you is that these ones to our restaurant reviews they're not about the food they tell you the psychological state of the diner so in summary natural language processing is this is is a very practical thing we use it every day in all sorts of tools but it's also a great window into human psychology into human attitudes and really it's a window into",
            "userFeedbackScore": 0.2823529411764706,
            "videoid": "QIdB6M5WdkI",
            "viewCount": "2272"
        },
        "ReakZVh2Xwk": {
            "NumOfComments": 22,
            "caption_exist": "T",
            "channel_id": "UCfzlCWGWYyIQ0aLC5w48gBQ",
            "channel_title": "sentdex",
            "comment_sentiment": 0.08992201591810967,
            "concepts": [
                [
                    "pickle",
                    18
                ],
                [
                    "classifier",
                    16
                ],
                [
                    "algorithm",
                    9
                ],
                [
                    "accuracy",
                    6
                ],
                [
                    "python",
                    5
                ],
                [
                    "process",
                    5
                ],
                [
                    "training",
                    4
                ],
                [
                    "thing",
                    4
                ],
                [
                    "trained",
                    3
                ],
                [
                    "basically",
                    3
                ],
                [
                    "saving",
                    3
                ],
                [
                    "quick",
                    3
                ],
                [
                    "pretty",
                    2
                ],
                [
                    "import",
                    2
                ],
                [
                    "tweak thing",
                    2
                ],
                [
                    "save underscore classifier",
                    2
                ],
                [
                    "trained algorithm",
                    2
                ],
                [
                    "first",
                    2
                ],
                [
                    "word",
                    2
                ],
                [
                    "voting process",
                    2
                ],
                [
                    "training process",
                    2
                ],
                [
                    "course",
                    2
                ],
                [
                    "class",
                    1
                ],
                [
                    "processing",
                    1
                ],
                [
                    "analysis",
                    1
                ],
                [
                    "nlt",
                    1
                ],
                [
                    "name",
                    1
                ],
                [
                    "good",
                    1
                ],
                [
                    "stable",
                    1
                ],
                [
                    "natural language processing",
                    1
                ],
                [
                    "language",
                    1
                ],
                [
                    "natural",
                    1
                ],
                [
                    "top",
                    1
                ],
                [
                    "natural language processing tutorial",
                    1
                ],
                [
                    "stream",
                    1
                ],
                [
                    "separate",
                    1
                ],
                [
                    "feel free",
                    1
                ],
                [
                    "real quick",
                    1
                ],
                [
                    "question",
                    1
                ],
                [
                    "matter",
                    1
                ],
                [
                    "differentiation",
                    1
                ]
            ],
            "description": "As you will likely find with any form of data analysis, there is going to be some sort of processing bottleneck, that you repeat over and over, often yielding the same object in Python memory. \n\nExamples of this might be loading a massive dataset into memory, some basic pre-processing of a static dataset, or, like in our case, the training of a classifier. \n\nIn our case, we spend much time on training our classifier, and soon we may add more. It is a wise choice to go ahead and pickle the trained classifer. This way, we can load in the trained classifier in a matter of milliseconds, rather than waiting 3-5+ minutes for the classifier to be trained. \n\nTo do this, we use the standard library's \"pickle\" module. What pickle does is serialize, or de-serialize, python objects. This could be lists, dictionaries, or even things like our trained classifier!\n\nPlaylist link: https://www.youtube.com/watch?v=FLZvOKSCkxY&list=PLQVvvaa0QuDf2JswnfiGkliBInZnIC4HL&index=1\n\nsample code: http://pythonprogramming.net\nhttp://hkinsley.com\nhttps://twitter.com/sentdex\nhttp://sentdex.com\nhttp://seaofbtc.com",
            "dislikeCount": "4",
            "duration": "PT6M18S",
            "likeCount": "184",
            "published_time": "2015-05-15T15:00:52.000Z",
            "tags": [
                "Natural Language Toolkit (Software)",
                "Python (Programming Language)",
                "Natural Language Processing (Software Genre)",
                "Pickle",
                "pickling",
                "serializing",
                "serialize",
                "marshalling"
            ],
            "thumbnail": "https://i.ytimg.com/vi/ReakZVh2Xwk/hqdefault.jpg",
            "title": "Save Classifier with Pickle  - Natural Language Processing With Python and NLTK p.14",
            "transcript": "  what is going on everybody welcome to the 14th Python with NLT K for natural language processing tutorial video in this video we're going to be talking about pickle or how to save basically you're trained algorithm so every time you want to use your trained algorithm you don't have to go through and retrain it so the training process for this algorithm actually this specific algorithm the training process is not that slow the longest part of the the process is probably just the loading in of the documents but that said there's going to be times where you you maybe you're writing you've got one algorithm or down the road we're actually going to have many algorithms and we're going to use a voting process on those algorithms to decide what the decision is amongst all of the algos and it adds up pretty quickly to train each algorithm especially if you're doing the same training set every single time why not save it so pickle is a way that we can actually save Python objects so then we can load in those objects and it's actually extremely powerful so anyway let's just go ahead and hop in and show a quick example of using pickles so first of all to use it we need to import it and it's just pickle like the food so import pickle and then what we'll do is near the bottom just somewhere after we do this part you know we can save the classifier by doing something like we can first of all we'll say save underscore classifier is going to eat will open and then we'll save it as some sort of file name but we'll call it naive Bayes dot pickle and we want to open this with the intention to W B which is right in bytes if you're on Python 2 this won't matter but it for Python 3 and beyond which you should be on anyways this you have to write it as bytes so anyway so there's that and then we'll do pickle dump what do we want to dump we want to dump this classifier right the thing that we trained to this word classifier what do we want to dump where do we want to dump it to save underscore classifier okay then of course as usual classifier The Closer save classifier up close will close that file good so let's go ahead and save and run that and while we're waiting let's go over here but we save that naivebayes pickle all the last one I did yes here it is we'll wait on this one it come on okay so we had a 69% accuracy and we've saved it now to this naive Bayes dot pickle file alrighty so we've got that saved now what we could do is instead of saving the classifier let's just comment that part out and then here where we have we're loading in this classifier instead of having it be the training what we can do is something like this so we can say whoops where am i their classifier underscore effort file basically equals open and it's the same file naive Bayes dot pickle we want to open it with the intention to read in bytes again because of Python 3s differentiation between bytes and streams then we're going to say classifier equals pickled load what do we want to load classifier F and then of course classifier close and actually its classifier F dot close cool all right soon run that um actually uh sorry let's comment this out even though it'll redefined it anyways we'll just comment it out to show that our our magical trick is actually working here with pickle um so we might have a separate percentage accuracy who this one was actually really bad haha I assure you this might actually still be bags the one that we've said let's see are we saving every time now sure hope they won't get 56 there we go 67 whew like I said this kind of unreliability in accuracy numbers you know the 50 as you saw earlier we got an 89 percent and now on the same algorithm we got a 56 percent like this is horrible I'll show you later on how we can use this voting process to get at least a stable uh accuracy percent and then as we go on we'll kind of dig into this accuracy and kind of see if we can figure out what's causing this much volatility and our accuracy but anyways that was just a quick tutorial on saving the classifier says you might want to tweak things and maybe you can save like you you might tweak things a little bit maybe you want to add more words or something like that and you want to save that classifier this is how you can do it you can save it actually you could pickle this entire document you could have pickled everything here you could you know make a class out of this basically and pickle that and um and then you wouldn't have to run this every time because as you add on top of this and do more analysis it's only going to take more and more time every time you run it so it makes a lot of sense to pickle it so anyways that's pickling just a real quick example with this although pickle is extremely useful this is not an NL TK thing this is actually pretty your standard library so definitely check out pickle think about using it I know that I definitely under use pickle in my time I didn't really think about looking into it it's a lot more useful than it is so that I thought it was a lot more useful than do this anyway that's it for this tutorial if you have any questions or comments please feel free to leave them below otherwise as always thanks for watching thanks for all sport the subscriptions until next time",
            "userFeedbackScore": 0.41859837885095175,
            "videoid": "ReakZVh2Xwk",
            "viewCount": "31871"
        },
        "Sx3Fpw0XCXk": {
            "NumOfComments": 73,
            "caption_exist": "T",
            "channel_id": "UCK8sQmJBp8GCxrOtXWBpyEA",
            "channel_title": "Google",
            "comment_sentiment": 0.19977875213320426,
            "concepts": [
                [
                    "language",
                    9
                ],
                [
                    "meet",
                    8
                ],
                [
                    "word",
                    7
                ],
                [
                    "model",
                    6
                ],
                [
                    "wave",
                    5
                ],
                [
                    "system",
                    5
                ],
                [
                    "thing",
                    4
                ],
                [
                    "google wave",
                    3
                ],
                [
                    "web",
                    3
                ],
                [
                    "language model",
                    3
                ],
                [
                    "machine",
                    2
                ],
                [
                    "statistical",
                    2
                ],
                [
                    "natural",
                    2
                ],
                [
                    "translation",
                    2
                ],
                [
                    "processing",
                    1
                ],
                [
                    "cost",
                    1
                ],
                [
                    "focus",
                    1
                ],
                [
                    "natural language processing",
                    1
                ],
                [
                    "likelihood",
                    1
                ],
                [
                    "scaling",
                    1
                ],
                [
                    "information",
                    1
                ],
                [
                    "name",
                    1
                ],
                [
                    "mean",
                    1
                ],
                [
                    "sydney",
                    1
                ],
                [
                    "data",
                    1
                ],
                [
                    "list",
                    1
                ],
                [
                    "group",
                    1
                ],
                [
                    "quality",
                    1
                ],
                [
                    "productivity",
                    1
                ],
                [
                    "error",
                    1
                ],
                [
                    "structured data",
                    1
                ],
                [
                    "first",
                    1
                ],
                [
                    "technology",
                    1
                ],
                [
                    "type",
                    1
                ],
                [
                    "good",
                    1
                ],
                [
                    "dictionary",
                    1
                ],
                [
                    "pretty",
                    1
                ]
            ],
            "description": "Casey Whitelaw describes the natural language processing behind Google Wave's spelling correction on the deck of the Sydney office. Birds and boat horns for effect.",
            "dislikeCount": "44",
            "duration": "PT5M5S",
            "likeCount": "357",
            "published_time": "2009-05-28T09:27:15.000Z",
            "tags": [
                "google wave",
                "natural language",
                "sydney",
                "engineering"
            ],
            "thumbnail": "https://i.ytimg.com/vi/Sx3Fpw0XCXk/hqdefault.jpg",
            "title": "Google Wave: Natural Language Processing",
            "transcript": "  hi my name is Casey white log I'm the tech lead for the natural language processing group here in Sydney and today I'm going to talk to you a little bit about some of the cool things they were added to Google Wave so one of the main things that we want to stay focused on you Google Wave is productivity that we want users to be able to stay productive whether they're reading or whether they're writing one of the ways that we've done that is with our spell correction system what we'd like is for users just to be able to focus on what they're typing and not worry about whether there's any mistakes that made we think that if people could just loosen up a little bit and you know it maybe type 5% faster and that's 5% less time that they spend time so I'll start with an example it's probably the easiest way to explain let's say you know you're you want to meet up with one of your friends you're having a chat so you write let's meet oops tomorrow so here you see I've made a mistake they've written met instead of meet here at my finger slipped on the e so now the way that we implemented spelling is have we introduced an automatic participant called smelly who works just like a nother user that's participating on the wave with you so smelly is on your wave with you and it can see that you've typed let's meet tomorrow and it's now going to try and spell check it for each word it doesn't have any kind of dictionary so it's it doesn't know where the met is a well spelled word or a misspelling so to start with it comes up with a list of possible candidate corruptions for this word so some examples of that might be meat the food or meat the correctly spelled version of this and you can imagine lots of others so set or net or me all kinds of different worms that we would evaluate to see whether they're what you actually meant a time we have learned from the web the kind of misspellings that people make in which things are more and less likely so we know that for instance maybe slipping and inserting an A is relatively likely but misspelling the very first letter might be less likely in this case so we've got some suggestions and the next thing that we do is evaluate these suggestions in context so there are other systems at Google that already use the same phone of statistical language models as this such as the Google Translation system that essentially encode information about how languages use and these are learned from the web from looking at billions of web pages so we get a really good idea about the way that people really use language in preface so what we will do is look at the likelihood of let's meet tomorrow and let's meet tomorrow less likely and let's meet tomorrow which is going to be more likely than either of these and we combine that with our error model which tells us how likely the misspellings are you know without any context to get a final determination as to what other most likely words the most likely word that you meant right here and so in this case we would suggest meet once once we think that a word is misspelled we need to get that back to the Google Wave client so that you can actually see that the user can actually see it and either correct it automatically or manually two kinds of ways that this differs from existing spelling system is one of them is just that it's in its hosted and this means that we can do the same kind of spelling for you regardless of which device you're you're connecting from so whether you're on your laptop or your mobile or your desktop and we get the same quality spelling regardless and that applies across languages too so you know we're doing this for other alphabetic languages also so the we like I said we use large statistical language models when I said large we train them from billions of words they end up being many many gigabytes and it's pretty infeasible to run these on a single machine which isn't such a problem in a datacenter where you can have a set of machines running a language model and there was spelling model together and then we can share the we can share that scaling model between many users so that the cost per user is very low so it's very efficient for us to do this once you realize that you've got a system that supports collaborative editing that has structured data and that you can change the user interface by having remote participants then really the sky's the limit I mean there's all kinds of existing natural language technologies like spell checking or translation that we can apply and we're seeing a lot of new applications as the way that we communicate changes as well so you know really it's going to be exciting times",
            "userFeedbackScore": 0.5260152872971854,
            "videoid": "Sx3Fpw0XCXk",
            "viewCount": "134165"
        },
        "UeiUiCRchiU": {
            "NumOfComments": 0,
            "caption_exist": "T",
            "channel_id": "UC5zx8Owijmv-bbhAK6Z9apg",
            "channel_title": "Artificial Intelligence - All in One",
            "comment_sentiment": 0,
            "concepts": [
                [
                    "box cutter",
                    10
                ],
                [
                    "ambiguity",
                    8
                ],
                [
                    "shoot",
                    7
                ],
                [
                    "good",
                    6
                ],
                [
                    "funny",
                    6
                ],
                [
                    "find",
                    6
                ],
                [
                    "interpretation",
                    5
                ],
                [
                    "good thing",
                    5
                ],
                [
                    "thing",
                    5
                ],
                [
                    "mean",
                    3
                ],
                [
                    "word",
                    3
                ],
                [
                    "lexical ambiguity",
                    2
                ],
                [
                    "noun",
                    2
                ],
                [
                    "head",
                    2
                ],
                [
                    "second",
                    2
                ],
                [
                    "first",
                    2
                ],
                [
                    "chronically absent employee",
                    2
                ],
                [
                    "rotation",
                    1
                ],
                [
                    "language",
                    1
                ],
                [
                    "arm",
                    1
                ],
                [
                    "bridge",
                    1
                ],
                [
                    "dropout",
                    1
                ],
                [
                    "money",
                    1
                ],
                [
                    "android",
                    1
                ],
                [
                    "future",
                    1
                ],
                [
                    "natural",
                    1
                ],
                [
                    "eye",
                    1
                ],
                [
                    "construction",
                    1
                ],
                [
                    "category",
                    1
                ],
                [
                    "woman",
                    1
                ],
                [
                    "structural",
                    1
                ],
                [
                    "tree",
                    1
                ],
                [
                    "developer",
                    1
                ]
            ],
            "description": ".\nCopyright Disclaimer Under Section 107 of the Copyright Act 1976, allowance is made for \"FAIR USE\" for purposes such as criticism, comment, news reporting, teaching, scholarship, and research. Fair use is a use permitted by copyright statute that might otherwise be infringing. Non-profit, educational or personal use tips the balance in favor of fair use.\n.",
            "dislikeCount": "0",
            "duration": "PT6M33S",
            "likeCount": "19",
            "published_time": "2016-03-25T03:11:36.000Z",
            "tags": [
                "Natural Language Processing",
                "Language Processing",
                "University of Michigan",
                "Michigan",
                "NLP",
                "Coursera",
                "Dragomir R. Radev",
                "Computational Linguistics",
                "Linguistics",
                "Information Retrieval",
                "Computer Science",
                "Video Lecture",
                "Video Tutorial",
                "Video Course",
                "Course",
                "Data Science",
                "Funny Sentences"
            ],
            "thumbnail": "https://i.ytimg.com/vi/UeiUiCRchiU/hqdefault.jpg",
            "title": "Lecture 3 \u2014 Funny Sentences - Natural Language Processing | University of Michigan",
            "transcript": "  in the next segment we're going to have some show some sentences that have more rotations one is literal and is not funny whereas the second one is not the intended one and can be sometimes very funny so the first such example is children make delicious snacks imagine this is a headline in some newspaper story the point of the author was essentially to report that some kids were preparing some snacks to evade tasty however because of the way that the sentence is constructed syntactically it also may be interpreted as children are very tasty when you eat them let me show you a few more such sentences that have this kind of unintentional meaning that is funny stolen painting found by treats as if the tree found the painting I saw the Rockies flying to San Francisco here the funny interpretation is one where the Rockies are flying to San Francisco court to try shooting defendant instead of trying or putting on trial a defendant of the shooting we actually want to shoot the defendant ban on nude dancing on governor's desk red tape holds up new bridges governor government head seeks arms Blair wins on budget more lies ahead local high school dropouts cut in half hospitals are sued by seven foot doctors dead expected to rise miners refused to work after death patient at death's door doctors put him through and finally in a make a woman has a baby every 15 minutes how does she do that so the previous slide showed you some what I called classic examples of them biggest headlines this slide is going to show you some examples that I actually collected on the internet recently the first one is Vancouver police shoot man holding box cutter so the two possible interpretations here is that the man who was shot was holding the box cutter which is probably what was intended and the other interpretation is that the box cutter was used by the police to shoot the man another example men armed with box cutters shot by officers which is the same story police shoot men with box cutters in a story and police shoot men using box cutters and police shoot man with box cutters in the third fourth and fifth example we have an instance of what is known as a prepositional phrase attachment where a prepositional phrase such as with box cutters can refer to either the nearest noun man or to the verb shoot so in the case when it refers to the nearest noun it modifies it so the man was carrying the box cutters if it modifies the verb then with box cutter modifies the way in which the person was shot and a few more of your headlines just for fun bow ganga doubles money for parties motorola to hire 300 Android developers Massachusetts exhales as bill passes heads to Canada us eyes returns to the moon and finally flesh-eating bug survivor goes home all those are here are headlines as you can see from the URLs so let me now show you some examples of ambiguous recommendations which will illustrate different issues with natural language so a funny example here is a man like him is hard to find this is written in reference to a chronically absent employee so the intended interpretation of the sentence is that this person was so difficult to find that because he was absent all the time but the sentence was made to look like a real recommendation Center specifically somebody who's so valuable that people like him are really difficult to find so here we have an instance of what is known as a lexical ambiguity so the word hard to find I'm sorry the phrase hard to find has multiple interpretations one case it means somebody who is very valuable in the other case it means somebody who's hiding and here's some more examples of lexical ambiguities for dishonest employee he can say he's an unbelievable worker unbelievable here is the ambiguous word it can mean somebody who's so good that it's hard to believe that person exists or it can also mean that this person should never be believed for lazy employee you can say you would indeed be fortunate to get this person to work for you again here the ambiguity comes from the word fortunate in one case it means that you will be happy to work with such a person in the second case it means that it would be a miracle if this person got to work for you there are other categories of linguistic constructions that can cause ambiguity for example we can have structural ambiguity scope ambiguity and others so in structure ambiguous are one example for chronically absent employee you can say it seemed that his career or her career was just taking off so here the ambiguity comes from the fact that taking off can refer to that the fact that the career was just starting meaning that this person had even more good things in their future or it means that her career was just the business of taking off from work in the scope ambiguous we can look at an example of for an employee who is not worth considering as a job candidate you can say oh we know I cannot say enough good things about this candidate or recommend him too highly so here the ambiguity comes from the fact that say enough good things can mean that either you have only a few good things to say about this person or that you have way too many good things to say about this person if you want to find more examples of this kind of funny recommendations you can look up Beatrice and talk in this collection on the Internet okay so now in the next segment we are going to talk about the administration of this class",
            "userFeedbackScore": 0.3,
            "videoid": "UeiUiCRchiU",
            "viewCount": "6647"
        },
        "d4gGtcobq8M": {
            "NumOfComments": 2,
            "caption_exist": "T",
            "channel_id": "UC6bkDYWHnlS-iY1u3CFoOmA",
            "channel_title": "SparkCognition",
            "comment_sentiment": 0.4791666666666667,
            "concepts": [
                [
                    "data",
                    8
                ],
                [
                    "word",
                    6
                ],
                [
                    "text",
                    5
                ],
                [
                    "nlp",
                    4
                ],
                [
                    "algorithm",
                    4
                ],
                [
                    "repair",
                    3
                ],
                [
                    "computer",
                    3
                ],
                [
                    "group",
                    3
                ],
                [
                    "natural",
                    3
                ],
                [
                    "language",
                    3
                ],
                [
                    "spark cognition",
                    3
                ],
                [
                    "injury report",
                    3
                ],
                [
                    "analysis",
                    2
                ],
                [
                    "operation",
                    2
                ],
                [
                    "energy",
                    2
                ],
                [
                    "unstructured data",
                    2
                ],
                [
                    "deep nlp",
                    2
                ],
                [
                    "natural language processing",
                    2
                ],
                [
                    "processing",
                    2
                ],
                [
                    "pattern",
                    2
                ],
                [
                    "find",
                    2
                ],
                [
                    "environmental",
                    1
                ],
                [
                    "solution",
                    1
                ],
                [
                    "stop word",
                    1
                ],
                [
                    "information",
                    1
                ],
                [
                    "field",
                    1
                ],
                [
                    "money",
                    1
                ],
                [
                    "cost",
                    1
                ],
                [
                    "process",
                    1
                ],
                [
                    "stuff",
                    1
                ],
                [
                    "technology",
                    1
                ],
                [
                    "unit of measurement",
                    1
                ],
                [
                    "ai",
                    1
                ],
                [
                    "air",
                    1
                ],
                [
                    "help",
                    1
                ],
                [
                    "measurement",
                    1
                ],
                [
                    "gas",
                    1
                ],
                [
                    "efficiency",
                    1
                ],
                [
                    "gulf",
                    1
                ],
                [
                    "turbine",
                    1
                ],
                [
                    "tool",
                    1
                ],
                [
                    "form",
                    1
                ],
                [
                    "utility",
                    1
                ],
                [
                    "fed",
                    1
                ],
                [
                    "question",
                    1
                ],
                [
                    "power",
                    1
                ],
                [
                    "mean",
                    1
                ],
                [
                    "first",
                    1
                ],
                [
                    "wind",
                    1
                ],
                [
                    "matrix",
                    1
                ],
                [
                    "expensive",
                    1
                ],
                [
                    "human",
                    1
                ],
                [
                    "saving",
                    1
                ],
                [
                    "understand",
                    1
                ],
                [
                    "distance",
                    1
                ],
                [
                    "wind turbine",
                    1
                ],
                [
                    "level",
                    1
                ]
            ],
            "description": "Not sure what natural language processing is and how it applies to you? In this video, we lay out the basics of natural language processing so you can better understand what it is, how it works, and how it's being used in the real world today. \n\nTo learn more on how SparkCognition is taking artificial intelligence into the real world, check out our DeepNLP solution: http://bit.ly/2JA74Cq",
            "dislikeCount": "0",
            "duration": "PT4M11S",
            "likeCount": "65",
            "published_time": "2018-06-06T17:52:48.000Z",
            "tags": [
                "natural language processing",
                "NLP",
                "learningnlp",
                "datascience",
                "deeplearning",
                "AI",
                "artificialintelligence",
                "bigdata",
                "naturallanguage",
                "processing",
                "usingnlp",
                "technology"
            ],
            "thumbnail": "https://i.ytimg.com/vi/d4gGtcobq8M/hqdefault.jpg",
            "title": "The Basics of Natural Language Processing",
            "transcript": "  energy companies want to improve operations and keep their employees safe but it is never easy Machinery breaks and someone has to go fix it picture yourself 200 feet in the air on an icy oil rig these repairs are not only expensive but also dangerous now companies know that by better analyzing their data they can improve their operations thereby saving money and keeping their employees safer but there's a challenge only 20% of the data needed for this analysis is in a structured format like spreadsheets or databases that's data that computers usually use the other 80% of it is in the form of text like repair manuals injury reports and notes shouted down by technicians this information is extremely valuable but due to its size and structure it has largely been invisible to analytics teams imagine you're searching a database of injury reports you want to find lower body injuries what do you search analysis from one large utility company showed that lower body injuries returned only a small number of results far fewer than existed in reality that's because their search tool was looking for the exact keywords lower body injuries but when the analysts use more specific search terms like foot injuries that returned many more results where foot was used in the context of distance so while they had more results than searching for just lower body injuries they weren't the results they wanted this was because a foot can be both a body part and a unit of measurement and while humans can determine context and know the difference until recently computers were largely stumped thanks to this field of AI called natural language processing computers can now analyze and understand textual data in case you're curious here's how natural language processing works at a high level NLP algorithms can't read text like we do but they can look for patterns and they find these patterns by turning huge amounts of text into matrices when analyzing text the algorithm might first remove words that don't really offer as much value stuff like a the is an R these are called stop words then the out of the might split the sentences into groups of words and count how many times each group of words appears in each document and how many documents have that group of words out of all the documents being analyzed without knowing anything at all about the text the algorithm can then tell how often a given word or phrase appears in a given document and how many documents contain that phrase out of all of the documents so tokens that appears lots of times in lots of documents may not mean much but tokens that appear frequently in only a few documents tell us that something's going on for example if we fed easier reports across all of our oil wells into this basic algorithm and we see that falling debris entries are clustered around reports from rigs in the Gulf of Mexico we might then know about a new piece of machinery or process or environmental condition or something else that is causing injuries before management hears about it now this is just a basic example when combining vast amounts of data and advanced NLP algorithms develop its spark-ignition our customers are seeing tremendous improvements in operational efficiency and safety wind turbine operators are finding answers that previous solutions missed spark Commission finds the meaning in the data and makes that available oil and gas operators are able to ask natural language questions when performing Diagnostics before repairs so with advanced technology like deep NLP from spark cognition you can now unlock the value of your unstructured data every email every maintenance log in every injury report become actual insights that can drive revenue and reduce costs and it's not just the energy industry that can harness the power of deep NLP if you want to learn more about how spark cognition can help you generate value from your unstructured data please keep reading at spark cognition calm",
            "userFeedbackScore": 1.0,
            "videoid": "d4gGtcobq8M",
            "viewCount": "5383"
        },
        "fOvTtapxa9c": {
            "NumOfComments": 100,
            "caption_exist": "T",
            "channel_id": "UCX6b17PVsYBQ0ip5gyeme-Q",
            "channel_title": "CrashCourse",
            "comment_sentiment": 0.2014901186544936,
            "concepts": [
                [
                    "computer",
                    22
                ],
                [
                    "language",
                    21
                ],
                [
                    "speech",
                    16
                ],
                [
                    "word",
                    15
                ],
                [
                    "sound",
                    14
                ],
                [
                    "noun",
                    10
                ],
                [
                    "human",
                    10
                ],
                [
                    "rule",
                    8
                ],
                [
                    "natural",
                    7
                ],
                [
                    "data",
                    7
                ],
                [
                    "speech recognition",
                    6
                ],
                [
                    "system",
                    6
                ],
                [
                    "machine",
                    5
                ],
                [
                    "process",
                    5
                ],
                [
                    "information",
                    5
                ],
                [
                    "accuracy",
                    4
                ],
                [
                    "phoneme",
                    4
                ],
                [
                    "text",
                    4
                ],
                [
                    "understand",
                    4
                ],
                [
                    "thing",
                    4
                ],
                [
                    "frequency",
                    3
                ],
                [
                    "technology",
                    3
                ],
                [
                    "science",
                    3
                ],
                [
                    "course",
                    3
                ],
                [
                    "computer science",
                    3
                ],
                [
                    "question",
                    3
                ],
                [
                    "first",
                    3
                ],
                [
                    "type",
                    3
                ],
                [
                    "dictionary",
                    2
                ],
                [
                    "article",
                    2
                ],
                [
                    "machine learning",
                    2
                ],
                [
                    "give computer",
                    2
                ],
                [
                    "language model",
                    2
                ],
                [
                    "vertical axis",
                    2
                ],
                [
                    "noun phrase",
                    2
                ],
                [
                    "research",
                    2
                ],
                [
                    "speech recognition system",
                    2
                ],
                [
                    "bell",
                    2
                ],
                [
                    "digital",
                    2
                ],
                [
                    "processing",
                    2
                ],
                [
                    "pattern",
                    2
                ],
                [
                    "interaction",
                    2
                ],
                [
                    "siri",
                    2
                ],
                [
                    "vocabulary",
                    2
                ],
                [
                    "human language",
                    2
                ],
                [
                    "phrase structure rule",
                    2
                ],
                [
                    "part of speech",
                    2
                ],
                [
                    "amplitude",
                    2
                ],
                [
                    "transcription",
                    2
                ],
                [
                    "crash course computer science",
                    2
                ],
                [
                    "speech synthesis",
                    2
                ],
                [
                    "focus",
                    2
                ],
                [
                    "crash course",
                    2
                ],
                [
                    "help",
                    2
                ],
                [
                    "pretty",
                    2
                ],
                [
                    "nlp",
                    2
                ],
                [
                    "microphone",
                    2
                ],
                [
                    "resonance",
                    2
                ],
                [
                    "recognizer",
                    2
                ],
                [
                    "model",
                    2
                ],
                [
                    "algorithm",
                    2
                ],
                [
                    "communication",
                    1
                ],
                [
                    "shape",
                    1
                ],
                [
                    "heap",
                    1
                ],
                [
                    "exchange",
                    1
                ],
                [
                    "separate",
                    1
                ],
                [
                    "fourier transform",
                    1
                ],
                [
                    "graph",
                    1
                ],
                [
                    "leaf",
                    1
                ],
                [
                    "meet",
                    1
                ],
                [
                    "feedback",
                    1
                ],
                [
                    "physic",
                    1
                ],
                [
                    "programming",
                    1
                ],
                [
                    "modern",
                    1
                ],
                [
                    "knowledge",
                    1
                ],
                [
                    "alexa",
                    1
                ],
                [
                    "funding",
                    1
                ],
                [
                    "parse tree",
                    1
                ],
                [
                    "knowledge graph",
                    1
                ],
                [
                    "web",
                    1
                ],
                [
                    "acoustic",
                    1
                ],
                [
                    "statistic",
                    1
                ],
                [
                    "signature",
                    1
                ],
                [
                    "deep neural network",
                    1
                ],
                [
                    "order",
                    1
                ],
                [
                    "parsing",
                    1
                ],
                [
                    "physical",
                    1
                ],
                [
                    "ambiguity",
                    1
                ],
                [
                    "service",
                    1
                ],
                [
                    "positive feedback",
                    1
                ],
                [
                    "input/output",
                    1
                ],
                [
                    "wave",
                    1
                ],
                [
                    "field",
                    1
                ],
                [
                    "fast fourier transform",
                    1
                ],
                [
                    "walk",
                    1
                ],
                [
                    "voice user interface",
                    1
                ],
                [
                    "development",
                    1
                ],
                [
                    "craft",
                    1
                ],
                [
                    "natural language processing",
                    1
                ],
                [
                    "error",
                    1
                ],
                [
                    "tree",
                    1
                ],
                [
                    "content",
                    1
                ],
                [
                    "chunk",
                    1
                ],
                [
                    "good",
                    1
                ],
                [
                    "software",
                    1
                ],
                [
                    "valve",
                    1
                ],
                [
                    "form",
                    1
                ],
                [
                    "telephone",
                    1
                ],
                [
                    "computer vision",
                    1
                ],
                [
                    "data set",
                    1
                ],
                [
                    "common",
                    1
                ],
                [
                    "method",
                    1
                ],
                [
                    "action",
                    1
                ],
                [
                    "programming language",
                    1
                ],
                [
                    "linguistics",
                    1
                ],
                [
                    "level",
                    1
                ],
                [
                    "neural network",
                    1
                ],
                [
                    "convention",
                    1
                ],
                [
                    "computing",
                    1
                ]
            ],
            "description": "Today we\u2019re going to talk about how computers understand speech and speak themselves. As computers play an increasing role in our daily lives there has been an growing demand for voice user interfaces, but speech is also terribly complicated. Vocabularies are diverse, sentence structures can often dictate the meaning of certain words, and computers also have to deal with accents, mispronunciations, and many common linguistic faux pas. The field of Natural Language Processing, or NLP, attempts to solve these problems, with a number of techniques we\u2019ll discuss today. And even though our virtual assistants like Siri, Alexa, Google Home, Bixby, and Cortana have come a long way from the first speech processing and synthesis models, there is still much room for improvement. \n\nProduced in collaboration with PBS Digital Studios: http://youtube.com/pbsdigitalstudios \n\nWant to know more about Carrie Anne?\nhttps://about.me/carrieannephilbin\n\nThe Latest from PBS Digital Studios: https://www.youtube.com/playlist?list=PL1mtdjDVOoOqJzeaJAV15Tq0tZ1vKj7ZV\n\nWant to find Crash Course elsewhere on the internet?\nFacebook - https://www.facebook.com/YouTubeCrash...\nTwitter - http://www.twitter.com/TheCrashCourse\nTumblr - http://thecrashcourse.tumblr.com \nSupport Crash Course on Patreon: http://patreon.com/crashcourse\nCC Kids: http://www.youtube.com/crashcoursekids",
            "dislikeCount": "60",
            "duration": "PT11M50S",
            "likeCount": "3746",
            "published_time": "2017-11-22T22:01:48.000Z",
            "tags": [
                "John Green",
                "Hank Green",
                "vlogbrothers",
                "Crash Course",
                "crashcourse",
                "education",
                "computing",
                "computers",
                "crash course computer science",
                "compsci",
                "computation",
                "natural language processing",
                "nlp",
                "speech synthesis",
                "speech",
                "parse tree",
                "parts-of-speech",
                "knowledge graph",
                "siri",
                "google home",
                "alexa",
                "bixby",
                "cortana",
                "speech recognition",
                "phonemes",
                "language model",
                "voice user interface"
            ],
            "thumbnail": "https://i.ytimg.com/vi/fOvTtapxa9c/hqdefault.jpg",
            "title": "Natural Language Processing: Crash Course Computer Science #36",
            "transcript": "  hi I'm Kerry Ann and welcome to crash course computer science last episode we talked about computer vision giving computers the ability to see and understand visual information today we're going to talk about how to give computers the ability to understand language you might argue they've always had this capability back in episodes 9 and 12 we talked about machine language instructions as well as higher level programming languages while they certainly meet the definition of a language they also tend to have small vocabularies and follow highly structured conventions code will only compile and run if it's a hundred percent free of spelling and syntactic errors of course this is quite different from human languages what are called natural languages containing large diverse vocabularies words with several different meanings speakers with different accents and all sorts of interesting wordplay people also make linguistic faux pars when writing and speaking like slurring words together leaving out key details so things are ambiguous and mispronouncing things but for the most part humans can roll right through these challenges the skillful use of language is a major part of what makes us human and for this reason the desire for computers to understand and speak our language has been around since they were first conceived this led to the creation of natural language processing or NLP and interdisciplinary field combining computer science and linguistics [Music] there's an essentially infinite number of ways to arrange words in a sentence we can't give computers a dictionary of all possible sentences to help them understand what humans are blabbing on about so an early and fundamental NLP problem was deconstructing sentences into bite-sized pieces which could be more easily processed in school you learned about nine fundamental types of English words nouns pronouns articles verbs adjectives adverbs prepositions conjunctions and interjections these are all called parts of speech there are all sorts of subcategories too like singular versus plural nouns and superlative versus comparative adverbs but we're not going to get into that knowing a words type is definitely useful but unfortunately there are a lot of words that have multiple meanings like rows and leaves which can be used as nouns or verbs a digital dictionary alone isn't enough to resolve this ambiguity so computers also need to know some grammar for this phrase structure rules were developed which encapsulate the grammar of a language for example in English there's a rule that says a sentence can be comprised of a noun phrase followed by a verb phrase noun phrases can be an article like thir followed by a noun or they can be an adjective followed by a noun and you can make rules like this for an entire language then using these rules is fairly easy to construct what's called a parse tree which not only tags every word with a likely part of speech but also reveals how the sentence is constructed we now know for example that the noun focus of this sentence is the mongols and we know it's about them doing the action of rising from something in this case leaves the smaller chunks of data allow computers to more easily access process and respond to information equivalent processes are happening every time you do a voice search like where's the nearest pizza the computer can recognize this is a where question knows that you want the noun pizza and the dimension you care about is nearest the same process applies to what is the biggest giraffe or who sang thriller by treating language almost like Lego computers can be quite adept at natural language tasks they can answer questions and also process commands like set an alarm for 2:20 or play t-swizzle and Spotify but as you've probably experienced they fell when you start getting too fancy and they can no longer parse the sentence correctly or capture your intent hey Siri methinks the mongols doth roam too much or thinkI on this most gentle Midsummer's day I should also know that phrase structure rules and similar methods that codify language can be used by computers to generate natural language text this works particularly well when data is stored in a web of semantic information where entities are linked to one another in meaningful relationships providing all the ingredients you need to craft informational sentences thriller was released in 1983 and sung by Michael Jackson Google's version of this is called knowledge graph at the end of 2016 it contained roughly seventy billion facts about and relationships between different entities these two processes parsing and generating text are fundamental components of natural language chatbots computer programs that chat with you early chatbots were primarily rule-based where experts would encode hundreds of rules mapping what a user might say to how a program should reply obviously this was unwieldy to maintain and limited the possible sophistication a famous early example was eliza created in the mid 1960s at MIT this was a chatbot that took on the role of a therapist and used basic syntactic rules to identify content in written exchanges which it would turn around and ask the user about sometimes it felt very much like human human communication but other times it would make simple and even comical mistakes chatbot some more advanced dialogue systems have come a long way in the last 50 years and can be quite convincing today modern approaches are based on machine learning where gigabytes of real human to human chats are used to train chat BOTS today the technology is finding use in customer service applications where there's already heaps of example conversations to learn from people have also been getting chat BOTS to talk with one another and in the Facebook experiment chat BOTS even started to involve their own language this experiment got a bunch of scary sounding press but it was just the computer's crafting a simplified protocol to negotiate with one another it wasn't evil it was efficient but what about if something has spoken how does a computer get words from the sound that's the domain of speech recognition which has been the focus of research for many decades Bell Labs debuted the first speech recognition system in 1952 nicknamed Audrey the automatic digit recognizer it could recognize all ten numerical digits if you set them slowly enough five nine seven the project didn't go anywhere because it was much faster to enter telephone numbers with a finger ten years later at the 1962 World's Fair IBM demonstrated a shoebox size machine capable of recognizing 16 words to boost research in the area DARPA kicked off an ambitious five-year funding initiative in 1971 which led to the development of harpy at Carnegie Mellon University HAARP he was the first system to recognize over a thousand words but on computers of the era transcription was often 10 or more times slower than the rate of natural speech fortunately thanks to huge advances in computing performance in the 80s and 90s continuous real-time speech recognition became practical there was simultaneous innovation in the algorithms for processing natural language moving from handcrafted rules to machine learning techniques that could learn automatically from existing data sets of human language today the speech recognition systems with the best accuracy are using deep neural networks which we touched on in episode 34 to get a sense of how these techniques work let's look at some speech specifically the acoustic signal let's start by looking at vowel sounds like R and E these are the waveforms of those two sounds as captured by a computer's microphone as we discussed in episode 21 on files and file formats this signal is the magnitude of displacement of a diaphragm inside of a microphone and sound waves caused it to oscillate in this view of sound data the horizontal access is time and the vertical axis is the magnitude of displacement or amplitude although we can see there are differences between the waveforms it's not super obvious what you would point to and say aha this is definitely an e sound to really make this pop out we need to view the data in a totally different way a spectrogram in this view of the data we still have time along the horizontal axes but now instead of amplitude on the vertical axis we plot the magnitude of the different frequencies that make up each sound the brighter the color the louder that frequency component this conversion from waveforms of frequencies is done with a very cool algorithm called a fast Fourier transform if you've ever stared at a stereo systems EQ visualiser it's pretty much the same thing a spectrogram is plotting that information over time you might have noticed that the signals have a sort of ribbed pattern to them that's all the resonances of my vocal tract to make different sounds I squeezed my vocal cords mouth and tongue two different shapes which amplifies or dampens different resonances we can see this in the signal with areas that are brighter and areas that are darker if we work our way up from the bottom labeling where we see peaks in the spectrum what are called formants we can see the two sounds have quite different arrangements and this is true for all vowels it's exactly this type of information the less computers recognize spoken valves and indeed whole words let's see a more complicated example like when I say she was happy we can see our I found here and our sound here we can also see a bunch of other distinctive sounds like the sounding she the once in was and so on these sound pieces that make up words are called phonemes speech recognition software knows what all these phonemes look like in English there are roughly 44 so it mostly boils down to fancy pattern matching then you have two separate words from one another figure out when sentences begin and end and ultimately you end up with speech converted into text allowing for techniques like we discussed at the beginning of the episode because people say words in slightly different ways due to things like accents and mispronunciations transcription accuracy is greatly improved when combined with a language model which contains statistics about sequences of words for example she was is most likely to be followed by an adjective like happy it's uncommon for she was to be followed immediately by a noun so if the speech recognizer was unsure between happy and happy it would pick happy since the language model would report that as a more likely choice finally we need to talk about speech synthesis that is giving computers the ability to output speech this is very much like speech recognition but in Reverse we can take a sentence of text and break it down into its phonetic components and then play those sounds back to back out of a computer speaker you can hear this changing of phonemes very clearly with older speech synthesis technologies like this 1937 hand operated machine from Bell Labs say she saw me with no expression now say it an answer to these questions who saw you did you buy the 1980s this has improved a lot but that discontinuous and awkward blending of phonemes still created that signature robotic sound thriller was released in 1983 and sewn by Michael Jackson today synthesized computer voices like Siri katana and Alexa have got a much better but they're still not quite human but we're so so close and it's likely to be a solved problem pretty soon especially because we're now seeing an explosion of voice user interfaces on our phones in our cars and homes and mainly soon plugs right into our is this ubiquity is creating a positive feedback loop where people are using voice interaction more often which in turn is giving companies like Google Amazon and Microsoft more data to train their systems on which is enabling better accuracy which is leading to people using voice more which is enabling even better accuracy and the loop continues many predict that speech technologies will become as common a form of interaction as screens keyboards trackpads and other physical input/output devices that we use today that's particularly good news for robots who don't want to have to walk around with keyboards in order to communicate with humans but we'll talk more about them next week see you then crash course computer science is produced in association with PBS Digital Studios at their channel you can check out a playlist of shows like Ian's physics girl and it's okay to be smart this episode was filmed at the chad and stacey emigholz studio in indianapolis and it was made with the help of all these nice people and our wonderful graphics team thought cafe thanks for the random access memories I'll see you next time",
            "userFeedbackScore": 0.5848920326912482,
            "videoid": "fOvTtapxa9c",
            "viewCount": "144406"
        },
        "iGmHnICXDss": {
            "NumOfComments": 0,
            "caption_exist": "T",
            "channel_id": "UCiDouKcxRmAdc5OeZdiRwAg",
            "channel_title": "Hugo Larochelle",
            "comment_sentiment": 0,
            "concepts": [
                [
                    "word",
                    34
                ],
                [
                    "model",
                    22
                ],
                [
                    "probability",
                    19
                ],
                [
                    "language",
                    16
                ],
                [
                    "language model",
                    11
                ],
                [
                    "minus",
                    10
                ],
                [
                    "previous word",
                    6
                ],
                [
                    "data",
                    6
                ],
                [
                    "system",
                    6
                ],
                [
                    "assumption",
                    5
                ],
                [
                    "good",
                    5
                ],
                [
                    "processing",
                    4
                ],
                [
                    "training",
                    4
                ],
                [
                    "natural",
                    4
                ],
                [
                    "natural language processing",
                    4
                ],
                [
                    "distribution",
                    4
                ],
                [
                    "engram model",
                    3
                ],
                [
                    "conditional probability",
                    3
                ],
                [
                    "training data",
                    3
                ],
                [
                    "translation",
                    3
                ],
                [
                    "individual word",
                    2
                ],
                [
                    "type",
                    2
                ],
                [
                    "element",
                    2
                ],
                [
                    "order",
                    2
                ],
                [
                    "person smart",
                    2
                ],
                [
                    "machine",
                    1
                ],
                [
                    "neural network",
                    1
                ],
                [
                    "speech",
                    1
                ],
                [
                    "machine translation",
                    1
                ],
                [
                    "form",
                    1
                ],
                [
                    "function",
                    1
                ],
                [
                    "product",
                    1
                ],
                [
                    "sparsity",
                    1
                ],
                [
                    "mean",
                    1
                ],
                [
                    "speech recognition system",
                    1
                ],
                [
                    "course",
                    1
                ],
                [
                    "first",
                    1
                ],
                [
                    "speech recognition",
                    1
                ],
                [
                    "probability distribution",
                    1
                ],
                [
                    "pretty",
                    1
                ]
            ],
            "description": "",
            "dislikeCount": "0",
            "duration": "PT9M23S",
            "likeCount": "56",
            "published_time": "2013-11-16T00:50:27.000Z",
            "tags": [],
            "thumbnail": "https://i.ytimg.com/vi/iGmHnICXDss/hqdefault.jpg",
            "title": "Neural networks [10.5] : Natural language processing - language modeling",
            "transcript": "  in this video we'll describe a very important problem in natural language processing the problem of language modeling so a language model is a probabilistic model that assigns probabilities to any sequence of words so it's a distribution essentially on sequences of words on sentences undocumented so the problem of language modeling or the task of language modeling is to actually learn such a model that will assign high probabilities to well form sentences so sentences that are likely to be written in some document or heard or and so on this is a problem or task that is very important natural language processing a lot of systems actually incorporate in language modeling component such as speech recognition systems and machine translation systems so for instance if we had a sentence wanted to translate in person et de Jonge and we had a system that's pretty good at just translating each individual word so in would be the system would know that you would translate it to AA which is here also and that son would be translated by person and then Anthony John would be translated by smart but imagine our system is also not so good at determining the order of these words then in in the setting language model would be very useful in distinguishing sponsz constants whether we should prefer a person smart or a smart person as a good translation of impersonate any job and in particular if you had a good language model then we would recognize that this has a higher probability of being written by a person in English than a person smart so in this case it would have been able to reorder PacSun and put it after entities out in in the actual translation in English so there are many other types of usage of language modeling and language problems sorry language models and so they're really really important in natural language processing an assumption that one often makes when someone designs a language model is to make the NEF order Markov assumption what this assumption says is that if we have a distribution the probability distribution over a sequence then the then we're assuming that we can write it as the product of the probability of each element of the sequence given only the N minus 1 previous elements in that sequence so in our case we're modeling words so we're assuming that the word at position T is only dependent on the N minus 1 previous words and does not depend on the other words before that in the sequence of words in the sentence or in the document so this is a restrictive assumption it essentially never really holds it really holds I except for perhaps very very large end values and and whenever I'm going to talk about context I'm going to talk about these n minus 1 previous words that the words on which we're conditioning the probability of observing the next word so the model which or type of model that is very frequently used in natural language processing is known as the Engram model so first let's describe what we mean by Engram an Engram is just a sequence of n words that usually extract from some training data from some corpus of words so for instance it would just take that sentence here as our corpus or our training data then uh the Uni Grahams that are contained is well there is is which is right here there's a their sequence and all the other individual words in that in that sentence examples of diagrams taken from that sentence would be is because we is followed by here also a sequence because we have followed by sequence here and so on and then trigrams which corresponds to N equals 3 and then gram for N equals 3 when an example is is a sequence because we have is a sequence here a sequence of because we have a sequence of here and so on so that's what an Engram is we're just a sequence of n words that is usually expect them from some from the data remove some ink and so an Engram model is going to estimate the conditional probability that we need in our markov model for assigning the probability of a whole document or sentence it's going to estimate that conditional probability based on counts of engrams that counts that are extracted from some training corpus some training data so specifically if we had a set of words the N minus 1 previous words or the context and we're wondering what's the probability of some given word that is observed next but what we do is that we would count so imagine we have this function count that just contains the number of times that the context so the word from position t minus 1 t minus n minus 1 up to t WT minus 1 so the number of times we observe this context followed by this word WT the probability conditional probably as model by estimated by this model is going to be proportional to that count and then we're going to normalize that by the number of times we've seen this context followed by any word so by dividing this way then we're guaranteed that this estimate of the probability is a valid probability it's going to sum to 1 now Engram models have one particular problem or challenge that I have to face and the problem data sparsity so if you want a good model that's accurate that that is close to reality really what we want is em to be as large as possible otherwise once it's for N equals one then we're actually assuming that all the words are independent because then the N minus one previous words that's one minus one that's the zero previous words we're conditioning on no words before any word for when we're modeling the probability of each word in the sentence so it effectively we're considering that all the words are independent the sequence which is of course a very crude assumption to make if that N equals two then we're just assuming that if we know one word then the distribution of the next word is not influenced by the third word before or fourth word before and so on so so that again is a very crude approximation so really we want n to be as large as possible to get a realistic model however for large values of n then it means that when we estimating these probabilities in this Ngram model it's going to be much more likely that for some new data and some testing corpora for which we're evaluating the probability of sentences it's going to be much more likely that we will have never seen the exact context for which we have to condition for valuing our conditional probabilities in the end gram model and so there are ways of alleviating this problem essentially they're all variants of this idea of smoothing the counts that is when we're for instance estimating the probability of some were given the three previous words so we have a for Graham here model then instead of just making that proportional to the number of times we've seen W 1 followed by W 2 W 3 and W 4 and then normalizing will actually have this probability be proportional to a combination of the counts of seeing W 1 up to next to W 2 W 3 and W 4 and then combined with the number of times we've seen just W 2 followed by W 3 w 4 and combine that also with W 3 bn4 and then just the number of times we've seen w-4 and so there's a whole literature on ways of combining these different counts to get a model that doesn't over fit as much and generalizes better but actually as we'll see this only partly solves the problem in the sense that there's something better we can do and specifically we will see a neural network language model that they can outperform this approach",
            "userFeedbackScore": 0.3,
            "videoid": "iGmHnICXDss",
            "viewCount": "11056"
        },
        "imPpT2Qo2sk": {
            "NumOfComments": 64,
            "caption_exist": "T",
            "channel_id": "UCfzlCWGWYyIQ0aLC5w48gBQ",
            "channel_title": "sentdex",
            "comment_sentiment": 0.08335130508668198,
            "concepts": [
                [
                    "chunk",
                    39
                ],
                [
                    "noun",
                    12
                ],
                [
                    "stuff",
                    9
                ],
                [
                    "regular expression",
                    8
                ],
                [
                    "np",
                    8
                ],
                [
                    "thing",
                    7
                ],
                [
                    "basically",
                    6
                ],
                [
                    "question",
                    6
                ],
                [
                    "speech",
                    5
                ],
                [
                    "word",
                    5
                ],
                [
                    "question mark",
                    4
                ],
                [
                    "step",
                    4
                ],
                [
                    "good",
                    4
                ],
                [
                    "find",
                    4
                ],
                [
                    "reg",
                    4
                ],
                [
                    "part of speech",
                    4
                ],
                [
                    "python",
                    3
                ],
                [
                    "chunk graham",
                    3
                ],
                [
                    "president george",
                    3
                ],
                [
                    "group",
                    3
                ],
                [
                    "sign",
                    3
                ],
                [
                    "named entity",
                    3
                ],
                [
                    "form",
                    3
                ],
                [
                    "speech tagging",
                    3
                ],
                [
                    "tesla",
                    2
                ],
                [
                    "feel free",
                    2
                ],
                [
                    "state",
                    2
                ],
                [
                    "yeah",
                    2
                ],
                [
                    "text",
                    2
                ],
                [
                    "union",
                    2
                ],
                [
                    "first",
                    2
                ],
                [
                    "moment",
                    1
                ],
                [
                    "c",
                    1
                ],
                [
                    "computer",
                    1
                ],
                [
                    "series",
                    1
                ],
                [
                    "bracket",
                    1
                ],
                [
                    "natural",
                    1
                ],
                [
                    "separate",
                    1
                ],
                [
                    "noun phrase",
                    1
                ],
                [
                    "matter",
                    1
                ],
                [
                    "range",
                    1
                ],
                [
                    "front",
                    1
                ],
                [
                    "dome",
                    1
                ],
                [
                    "video tutorial",
                    1
                ],
                [
                    "pretty",
                    1
                ],
                [
                    "natural language processing",
                    1
                ],
                [
                    "map",
                    1
                ],
                [
                    "processing",
                    1
                ],
                [
                    "language",
                    1
                ],
                [
                    "information",
                    1
                ],
                [
                    "beach",
                    1
                ],
                [
                    "battery",
                    1
                ],
                [
                    "natural language processing tutorial",
                    1
                ],
                [
                    "head",
                    1
                ],
                [
                    "saving",
                    1
                ],
                [
                    "power",
                    1
                ],
                [
                    "mean",
                    1
                ],
                [
                    "space",
                    1
                ],
                [
                    "human",
                    1
                ],
                [
                    "nl ck",
                    1
                ],
                [
                    "capital",
                    1
                ],
                [
                    "create",
                    1
                ]
            ],
            "description": "Chunking in Natural Language Processing (NLP) is the process by which we group various words together by their part of speech tags. \n\nOne of the most popular uses of this is to group things by what are called \"noun phrases.\" We do this to find the main subjects and descriptive words around them, but chunking can be used for any combination of parts of speech.\n\nsample code: http://pythonprogramming.net\nhttp://hkinsley.com\nhttps://twitter.com/sentdex\nhttp://sentdex.com\nhttp://seaofbtc.com",
            "dislikeCount": "16",
            "duration": "PT13M19S",
            "likeCount": "497",
            "published_time": "2015-05-05T14:21:21.000Z",
            "tags": [
                "Natural Language Toolkit (Software)",
                "Python (Programming Language)",
                "Natural Language Processing (Software Genre)",
                "Outline Of Natural Language Processing",
                "chunking",
                "chunk",
                "noun phrase",
                "PunktSentenceTokenizer",
                "RegexpParser",
                "NLTK"
            ],
            "thumbnail": "https://i.ytimg.com/vi/imPpT2Qo2sk/hqdefault.jpg",
            "title": "Chunking - Natural Language Processing With Python and NLTK p.5",
            "transcript": "  hello everybody and welcome to part 5 of our NL ck with Python for natural language processing tutorial video in this video we're going to be talking about chunking so what is chunking besides a really strange sounding term so consider you have a body of text we know how to split it up by sentence and by even by word and not only that but part of speech tags we're that far what would be the next step to figure out the meaning of a sentence well first we want to know who is the sentence talking about who what whatever and so generally we refer to that as the named entity in the context right but also just a noun okay so a person place or thing is generally going to be your subject I say generally now uh so once you know that okay once you know the named entity what's the next step well the next step is going to be or the noun the next step is going to be finding out words that kind of modify or affect that noun so you might have many named entities or many nouns in the same so you might have a sentence like a Apple releases new phone comes with new color case hundred dollars more and Tesla releases home battery okay so these are sentient this is one sentence but it's not about two different things completely and you might even have some opinions in that sentence and you've got to figure out who's where does that opinion apply is that applying to Apple or is that applying the Tesla so um so that's what we're going to talk about now is chunking and how we might be able to do stuff like that so generally what that's going to be is is good you're in it most people will chunk into we're called noun phrases and these are just going to be groups of net will be and down and we'll have a bunch of modifiers around that noun and it would be kind of like a descriptive sentence I suppose or descriptive group of words surrounding that now now the downside of that is you're you're only going to be able to use regular expressions so what's going to happen is you were firt be like you can only group things together as a chunk and it can only be a chunk of things that are touching each other as you'll see in a moment so that's kind of the only downside here but you can at least chunk important words and then kind of break it out from there if you want it so anyways it'll be more useful I think to kind of visualize what chunking is now so we're going to come down here and the way that we do chunking is we use a meshing of these part of speech tags and regular expressions now if you are unfamiliar with regular expressions you can still follow along I'll try to explain as good as I can but you're going to have to brand new concepts being thrown at you here and you may not know which is which like are we looking at regular expressions here are we looking at like chunking code so if you need to learn about regular expressions this is a Python per turn at net it's my website ctrl F when you're on the basics tab anyways if you're not here you clean and start learning basics reg X enter we want this one actually video tutorial text tutorial we very fun now the main thing that we're going to use with chunking is modifiers so with your modifiers you've got things like the plus sign means it met it's one or more question mark means zero or one no more asterisk means zero or more so basically any number or nothing we probably use the dollar sign the caret we won't be using really you might actually probably not though the either/or you'll use that a lot the range you'll use as well so anyways um just keep that in mind that's kind of what you need to know with modifiers I will still hold your hand through this but like I said it might be confusing if you if you're not familiar with either of these concepts so coming down here let's get rid of tagged and then we'll prynt I'd rather keep tagged there now we're ready to create our chunk Graham Cheung Graham is going to be equal to and generally people use triple quotes for the shot Graham's because this one will be really simple but a lot of times people are going to put them and separate them by lines because they can get really long and complicated really fast so chunk Graham use the triple quotes if you want don't if you don't want to it doesn't matter to me and we're going to chunk by now most people are going to put the R in front to denote a form of regular expression here but it e sure it's not necessary well we'll test that if I remember - so now chunk you can call this anything you want you could call this up if you wanted it's just what it's going to be called when it finds it so anyway a chunk equals and then you put the chunk that you want to you know find inside curly braces now to bring up or mention any form of part of speech tag use these little round brackets sideways carrots I don't know what the heck you call these someone tell me what you call these and first let's say we're looking for any our B now what is an RB it's head over up session let's go that's up here RB adverb now we want to find RB RB r or r BS any adverb we want to find how do we do it regular expressions so we come on RB what would be a valid modifier to use we want our be followed by basically any character we could use the WS suppose but we're not going to do that we're going to say R be any character is the period and then we're going to use a question mark what is that so the period is any character except for newline basically in tabs and stuff and actually I'm sorry it's actually any and as long as my own tutorials correct it's actually any character except for a new line I thought it was also tabbed anyways moving on RB so anything and then question mark is 0 or 1 because as you can see we have any power speech tag that is longer well at least for our be then three characters okay but possessive pronouns can be and they use a dirty dollar sign there anyway moving on it's that's fine any adverb then we're gonna say what will say asterisk what is asterisk zero so we're saying any form of an adverb and we're looking for zero or more of these so this chunk will chunk whether or not there is an adverb but if there is one we'll get it then we'll have another one then we're just going to be B and then we'll use the exact same code that we used before which is period question mark and again if we find one great if not no problem then we're going to look for an n NP what's in it NP Harrison let's go up an NP proper noun and hot-diggity there's an example of a proper noun it's me so come on down here n NP we are going to require the existence of a n NP then we're going to do a a plausible and n let's go back up and n is just a noun okay so n NP singular proper noun followed by a possible noun singular anything now that's our chunk sweet so hopefully you guys follow along there if you have any questions as always ask below now we're regular ready to do is a parse via this chunk Graham so we're going to say chunk parser equals n LT k dot reg X parser and the Ray X parser we want to use is the chunk Ram now chunk is going to equal the chunk parser dot the tag part of speech stuff and then we can do the phone print chumps use on a beach but I do reg X Oh fry sure in my original tutorial series I swear I hid this rag X P parser and I'm almost positive they're doing this because it's regular expressions and then parser but no one says reg X they say reggae all right anyway let's try this again I'm almost positive that hit me in the original tutorial this is going to go for a while so ctrl C to break it and so this is your you know basically out of the output and what you're looking for is chunk right this is chunk so chunk state here's another chunk Union address chunk United capital so it's finding all these nouns basically white house chomp-chomp okay now this is kind of ugly the computer loves this stuff but the human hates it so what you can do with a LT k if you grabbed if you have matplotlib if you got map I live this primal work but chumped dot draw saving run that and now you have this look at us wow that's way too many chumps what has happened for them maybe because that's the start of this practice is like this initial yeah right so the initial stuff here that was a little rough but anyway president george w bush and as we can see that really we actually would want to chunk these all together right so maybe any amount of n NP s so we could come down here n NP one or more is what we want so we want to do a plus okay so now let's try that again i don't know how far deep this one was but it was it was a few x's so something like this is rerun it um okay so that's a little better um that's not so hot but this is hard because of the original stuff but president george w bush that is a correct chunk let's get out of this initial stuff mr. speaker vice pres Picchi this is a tough this is a tough one for uh python anyway we'll start on this one so this is the original one that we wanted to chunk for sure so president george w bush good state of the union again you could add this stuff into your chunk like you could have a possible preposition a possible I forget what DT stands for but it's like useless words I'm pretty sure um close close close White House put that together Eric Draper every time Draper I'm right it's almost like he's missing a space there Capitol dome that's a good catch okay have served America alright you get the point so this is how we're chunking a bit of information together obviously again like I was trying to point out our chunk is extremely basic and as you can see as we move forward we can think of a lot of like little stuff that we're like hmm I wouldn't mind adding this or that or possible of this and so as you can imagine these chunks can get really long okay so anyway feel free to mess with it I actually highly encourage you to kind of play around with this try to catch that of the year yeah I think it was of the see if you can catch that and well I'm thinking about let's look at DT again determined dt4 not important anyway um so that's chunking now in the next tutorial video we're going to talk about basically kind of a different task now this can be chinking and that's going to be so chunking is the you know grouping of things shaking is going to be the removal of something get that out so we'll tuck on that and the next video if you have any questions or comments on this video please feel free to leave them below all otherwise as always thanks for watching thanks for all the support of subscriptions and until next",
            "userFeedbackScore": 0.4030519345298855,
            "videoid": "imPpT2Qo2sk",
            "viewCount": "85117"
        },
        "ohM7D21C_8Q": {
            "NumOfComments": 5,
            "caption_exist": "T",
            "channel_id": "UC6KW_xGp6eAcIqIfqmcwoPQ",
            "channel_title": "Zihan Guo",
            "comment_sentiment": 0.039999999999999994,
            "concepts": [
                [
                    "question",
                    30
                ],
                [
                    "find",
                    9
                ],
                [
                    "type",
                    8
                ],
                [
                    "article",
                    6
                ],
                [
                    "first",
                    5
                ],
                [
                    "system",
                    4
                ],
                [
                    "switch",
                    4
                ],
                [
                    "negation",
                    4
                ],
                [
                    "list",
                    3
                ],
                [
                    "top",
                    3
                ],
                [
                    "word",
                    3
                ],
                [
                    "second",
                    3
                ],
                [
                    "wikipedia article",
                    3
                ],
                [
                    "thing",
                    3
                ],
                [
                    "question answering system",
                    2
                ],
                [
                    "semantics",
                    2
                ],
                [
                    "basically",
                    2
                ],
                [
                    "question answering",
                    2
                ],
                [
                    "negation word",
                    2
                ],
                [
                    "irrelevant sentence",
                    2
                ],
                [
                    "top ten",
                    2
                ],
                [
                    "answer system",
                    2
                ],
                [
                    "dictionary",
                    1
                ],
                [
                    "compound",
                    1
                ],
                [
                    "future",
                    1
                ],
                [
                    "parsec",
                    1
                ],
                [
                    "speech",
                    1
                ],
                [
                    "static",
                    1
                ],
                [
                    "noun",
                    1
                ],
                [
                    "tour",
                    1
                ],
                [
                    "noun phrase",
                    1
                ],
                [
                    "overview",
                    1
                ],
                [
                    "named entity",
                    1
                ],
                [
                    "part of speech",
                    1
                ],
                [
                    "interest",
                    1
                ],
                [
                    "synonym",
                    1
                ],
                [
                    "information",
                    1
                ],
                [
                    "good",
                    1
                ],
                [
                    "atom",
                    1
                ],
                [
                    "form",
                    1
                ],
                [
                    "mean",
                    1
                ],
                [
                    "fluent",
                    1
                ],
                [
                    "human",
                    1
                ],
                [
                    "distance",
                    1
                ],
                [
                    "flow",
                    1
                ],
                [
                    "algorithm",
                    1
                ],
                [
                    "create",
                    1
                ],
                [
                    "distribution",
                    1
                ]
            ],
            "description": "We used python to programmed a QA system using packages like  wordnet, stanford parser, and using techniques like name entity recognition, pronoun transformation, synonym antonym random replacement. Here is our project: https://github.com/gzhami/nlp_qa_project",
            "dislikeCount": "0",
            "duration": "PT5M9S",
            "likeCount": "32",
            "published_time": "2017-12-01T04:56:46.000Z",
            "tags": [
                "QA System",
                "NLP",
                "wordnet",
                "CMU",
                "11-411",
                "11411",
                "Question Answering"
            ],
            "thumbnail": "https://i.ytimg.com/vi/ohM7D21C_8Q/hqdefault.jpg",
            "title": "How to create a Question Answering System NLP",
            "transcript": "  and you give a high-level description of how we viewed or question answering system there are two tasks the first task takes a Wikipedia article an integer and return the top and best question the second task take away key B the article a question file and for each question we find the best answer and the I mean fluent in grammar correct that has to be checked my human pen size does not contain read any information like prepositional phrases the tour approaches synthetic approach or semantic approach semantics approach is computationally difficult and implementation le hard the closest thing that we have is pronounced transformation which you see here which has formed the pronoun he and his into Auburn and Albert's three type of question first weaponry finding the verb and found the noun phrase swap the position we get binary question compound question we which requires more work we find the verb find is corresponding auxiliary verb placing it ahead and convert the verb in the original form likes into like transforming adjective makes the question more difficult we use part of speech to identify where the adjectives are and randomly replace them with synonyms or antonyms to make the question more difficult this is an example a study is a static person is tad a bad person that's one unpassable transformation using answer number replacements is tad a respect respectful person they have similar meaning without respectable and that's how we make their questions more difficult WH questions we give in our article tokenized it into sentence passed into pronounced transformation generate binary questions and from binary questions we'll use named entity recognization and parsec treated to to try out different WH question generations the end will be this dictionary with label being being the WH label and the value being a list of a list of questions corresponding to that to that W type how to prioritize which type of question read her first if we can generate 2,000 type of questions from a wiki article and we only want to read return the top ten which which top tens did we choose it relies on distribution and we find the type of question the wh-wh-why the wild type of question is more rare than where and when what a hood in binary so we will return the most rare cases first and then the last rare and the last two rare type of question next sorry for jumping ahead let's see where we go okay so the second part the second task is to create the answer system for the answer system here is the flow chart take a Wikipedia a question fell token as it is Levenstein distance to find the most irrelevant sentence from this list of sentences from the Wikipedia article we find the label to decide what type of question it is and use back engineer but basically back engineer or algorithm to find where that answer lies so let me give one example a simple example of how we how we answer any questions this is the question this is the most irrelevant sentence that we have from the Wikipedia article so we find the most relevance in this first and here is that here's the sentence we get the negation work we have to check if the sentence contains negation word or not so at the very beginning the the switch would be yes we find a negative word we flip this that switch from yes to no so now we have unknown then we we convert this sentence without that negation word into a binary question comparing this to question and and location-wise we check if they have antonyms or synonyms if they have synonyms we don't do anything if they're antonyms would basically flip the switch again so because of anyway yes we see we saw a negation we switch we flipped that is to know and because we saw an atom we flipped that not yes so here is how we get the answer to things - do we want to do in the future interest in his semantics reasoning here is the is an example recurrent the program is only be able to generate - to answer with with because of such go what did they study dissonance because of such go we don't know what that go is to answer that question requires looking one question were several questions I had another thing that we really should have done is a conjunction freeze break down hill extra and he buys her flower should be broken down into a lecture in the sentence he buys her flowers so that's it that's it for our projects and hopefully it gives you a good overview of what we have done and how would you be able to address our question answering system thank you for your attention",
            "userFeedbackScore": 0.3584347826086956,
            "videoid": "ohM7D21C_8Q",
            "viewCount": "3099"
        },
        "yGKTphqxR9Q": {
            "NumOfComments": 46,
            "caption_exist": "T",
            "channel_id": "UCfzlCWGWYyIQ0aLC5w48gBQ",
            "channel_title": "sentdex",
            "comment_sentiment": 0.18441019799443711,
            "concepts": [
                [
                    "word",
                    27
                ],
                [
                    "python",
                    24
                ],
                [
                    "stemming",
                    9
                ],
                [
                    "thing",
                    7
                ],
                [
                    "stuff",
                    5
                ],
                [
                    "data",
                    5
                ],
                [
                    "ride",
                    5
                ],
                [
                    "analysis",
                    4
                ],
                [
                    "nlt",
                    3
                ],
                [
                    "tokenize",
                    3
                ],
                [
                    "import",
                    3
                ],
                [
                    "natural",
                    3
                ],
                [
                    "porter stemmer",
                    3
                ],
                [
                    "processing",
                    3
                ],
                [
                    "python lee",
                    3
                ],
                [
                    "mean",
                    3
                ],
                [
                    "first",
                    3
                ],
                [
                    "python poorly",
                    2
                ],
                [
                    "root",
                    2
                ],
                [
                    "basically",
                    2
                ],
                [
                    "root stem",
                    2
                ],
                [
                    "image processing",
                    2
                ],
                [
                    "quick",
                    2
                ],
                [
                    "text",
                    2
                ],
                [
                    "language",
                    2
                ],
                [
                    "good",
                    2
                ],
                [
                    "find",
                    2
                ],
                [
                    "word net",
                    2
                ],
                [
                    "form",
                    2
                ],
                [
                    "sin set",
                    2
                ],
                [
                    "list",
                    2
                ],
                [
                    "machine",
                    1
                ],
                [
                    "series",
                    1
                ],
                [
                    "wordnet",
                    1
                ],
                [
                    "feel free",
                    1
                ],
                [
                    "english language",
                    1
                ],
                [
                    "help",
                    1
                ],
                [
                    "yeah",
                    1
                ],
                [
                    "synonym",
                    1
                ],
                [
                    "word tokenize",
                    1
                ],
                [
                    "bra",
                    1
                ],
                [
                    "top",
                    1
                ],
                [
                    "nc",
                    1
                ],
                [
                    "question",
                    1
                ],
                [
                    "machine learning",
                    1
                ],
                [
                    "space",
                    1
                ],
                [
                    "order",
                    1
                ],
                [
                    "pretty",
                    1
                ],
                [
                    "algorithm",
                    1
                ]
            ],
            "description": "Another form of data pre-processing with natural language processing is called \"stemming.\" \n\nThis is the process where we remove word affixes from the end of words. \n\nThe reason we would do this is so that we do not need to store the meaning of every single tense of a word. For example:\n\nReader\nReading\nRead\n\nAside from tense, and even one of these is a noun, they all have the same meaning for their \"root\" stem (read).\n\nThis way, we store one single value for the root stem of \"read.\" Then, when we wish to learn more, we can look into the affixes that were on the end, like \"ing\" is an active word, or in the past, then you have reader as someone who reads... then just plain read as either past tense or current. \n\nsample code: http://pythonprogramming.net\nhttp://hkinsley.com\nhttps://twitter.com/sentdex\nhttp://sentdex.com\nhttp://seaofbtc.com",
            "dislikeCount": "7",
            "duration": "PT8M16S",
            "likeCount": "748",
            "published_time": "2015-05-03T21:52:17.000Z",
            "tags": [
                "Natural Language Toolkit (Software)",
                "Natural Language Processing (Software Genre)",
                "Python (Programming Language)",
                "Stemming",
                "Outline Of Natural Language Processing",
                "porter stemmer",
                "PorterStemmer",
                "nltk.stem",
                "NLTK"
            ],
            "thumbnail": "https://i.ytimg.com/vi/yGKTphqxR9Q/hqdefault.jpg",
            "title": "Stemming - Natural Language Processing With Python and NLTK p.3",
            "transcript": "  hello everybody and welcome to part three of our natural language toolkit with Python for natural image processing tutorial video if you're familiar with any of the machine learning video series that I have you'll find that like I say most of your work when it comes to any sort of data analysis is usually organized an organization of data cleaning up a data and all that and like analysis is like right at the very end it's like the cherry on top that you get after like 99% of your work is just about like organizing and structuring and pre-processing your data so so most of what n LT K does for you actually is LT K does not perform the analysis generally for you it's good you can use some stuff in a let's see k to test things stuff like that but for the most part it's a toolkit so most of what we're going to cover at least in these first few videos is just the toolkit aspect of it and then I'll show you guys how we can actually use it for analysis but moving on the next topic that we're going to be talking about is called stemming so the idea of stemming is kind of a it's a form of data pre-processing and it's a form of not really normalization but it's the best word I can think of to compare it to and the idea of it is you take words and you take the root stem of the word so for example if you've got a writing the stem of writing would be rid basically so you get rid of the ing and you have a stem of our ID our ID is applicable to rid bra or sorry ride riding ridden that kind of stuff okay so uh what we're going to do now is well actually first why might you want a stem okay so why are we even doing this and the reason why is a lot of times you're going to have different variations of words based on their stems or at least sort of fixes at the end and but really the actual meaning of that word is unchanged so for example let's say we have two sentences we have I was taking a ride in the are and they've another one I was riding in the car okay these mean the exact same thing but the word ride here is ride and the word ride ish here is riding but the sentences mean the exact same thing and the use of the word here is identical so if you can imagine all the words in the English language plus they're all they're fixations at the end of them you could imagine that you'd have a huge database here and a lot of times you would have two words that basically mean the exact same thing taking up space in this database or table or whatever you're using to get values on words or meaning even from words that have the exact same definition it would be very redundant very inefficient so we use stemming to kind of help this problem so with naturalness processing has had a stemming algorithm around for a really long time and it's called the porter stemmer this one's actually been around since like 1979 so if you think natural image processing is like a new thing adds your problem so anyway um so to use it with n ltk we're going to go from NLT k dot stem import porter order stemmer strimmer stammer and just to just like everything else there's there are multiple stammers you can train some stammers yourself porter stemmer is actually pretty darn good so we'll just use it use it for now anyway next we're gonna say from NLT K dot tokenize import um we don't really need the cent tokenizer so we'll just do the word tokenize tokenize cool now we're going to say PS vehicle's porter stemmer and now let's make an example list of words so example words that's an equal list and we'll have some stuff in here let's do five for now okay so let's think of some example words you've got a Python you might be a Python yourself it might be the act of being a Python something like that then you've got a Python ER this is someone who pythons they've got Python in this is someone who is actively doing the Python then you have a Python this is generally what happens when you solve a problem with Python that problem was Python and then you've got Python Lee this is just how you kind of handle yourself he handles himself Python okay so those are example words and now let us stem those words and see what we get so we're going to do for W in example words and we're going to do print PS stem W cool now let's go ahead and run really quick so here are the stems so as you can see the first one two through four are all the same right they all stem down to Python as the root stem except for the last one who is python leap okay and so this one has its own little stem probably because of the meaning changes slightly but you could argue the same thing with all of these other ones but anyway we'll close that for now and um so whenever you're stemming like what would a sentence look like that was stemmed okay so let's see let me comment this off and let's make a sentence with our words just to really drive the point home so new text equals it is very important to be Python Lee wow you are Python in with Python all Python errs have pythons poorly at least once okay now we're going to say what happened there we are we're going to say words equals word underscore tokenize new text and then again we'll just use this exact same thing up here for W in example words only now it is words wads save and run that it is very import to be Python Lee while you are Python with Python all Python have Python poorly at least well not quite sure why once stemmed let's see is there any other iterate once I can't think of more like I don't sure why it stems down to o NC anyway Oh fine it's just not meaningful so as you can see we can stem all this and important important important the meaning does not need to necessarily change that kind of stuff so anyways that's just a quick example of stemming with NLT k and kind of why you might want to stem with n LC k it really depends on what you do and kind of like what your goal is because a lot of times as you'll see as we move forward you won't actually have to stem and you'll feed words through n LT K and they'll actually you'll use word net instead and word net will actually find you the synonym using sin set and yeah so really stemming is something you should know you should know how to do it but moving you may or may not actually ever utilize stemming because you just don't need to now that you've got wordnet and sin set and nowadays even imaging it what anyway so that's it for this tutorial if you have any questions or comments on stemming please feel free to leave them below otherwise as always thanks for watching thanks for all the support and subscriptions until next time",
            "userFeedbackScore": 0.5638363318585016,
            "videoid": "yGKTphqxR9Q",
            "viewCount": "98102"
        }
    }
}