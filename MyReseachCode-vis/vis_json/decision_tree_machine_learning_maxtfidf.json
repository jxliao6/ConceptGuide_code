{
    "concept_relationship": {
        "links": [
            {
                "prerequisite": null,
                "similarity": 0.9999999999999999,
                "source": 0,
                "target": 0
            },
            {
                "prerequisite": 0.15451003169010596,
                "similarity": 0.8080840395142804,
                "source": 0,
                "target": 1
            },
            {
                "prerequisite": null,
                "similarity": 0.5636778767007926,
                "source": 0,
                "target": 2
            },
            {
                "prerequisite": null,
                "similarity": 0.43068029549822884,
                "source": 0,
                "target": 3
            },
            {
                "prerequisite": null,
                "similarity": 0.10715690887447457,
                "source": 0,
                "target": 4
            },
            {
                "prerequisite": null,
                "similarity": 0.5366039489251923,
                "source": 0,
                "target": 5
            },
            {
                "prerequisite": null,
                "similarity": 0.13932932507848195,
                "source": 0,
                "target": 6
            },
            {
                "prerequisite": null,
                "similarity": 0.39999842005088027,
                "source": 0,
                "target": 7
            },
            {
                "prerequisite": null,
                "similarity": 0.28017899266183993,
                "source": 0,
                "target": 8
            },
            {
                "prerequisite": null,
                "similarity": 0.047289390214026186,
                "source": 0,
                "target": 9
            },
            {
                "prerequisite": null,
                "similarity": 0.4099604475607465,
                "source": 0,
                "target": 10
            },
            {
                "prerequisite": null,
                "similarity": 0.2701458833066819,
                "source": 0,
                "target": 11
            },
            {
                "prerequisite": null,
                "similarity": 0.43176771352481347,
                "source": 0,
                "target": 12
            },
            {
                "prerequisite": null,
                "similarity": 0.385315037099065,
                "source": 0,
                "target": 13
            },
            {
                "prerequisite": null,
                "similarity": 0.29270856401431344,
                "source": 0,
                "target": 14
            },
            {
                "prerequisite": null,
                "similarity": 0.2941270800014457,
                "source": 0,
                "target": 15
            },
            {
                "prerequisite": null,
                "similarity": 0.41738219556119155,
                "source": 0,
                "target": 16
            },
            {
                "prerequisite": null,
                "similarity": 0.21376051715988154,
                "source": 0,
                "target": 17
            },
            {
                "prerequisite": null,
                "similarity": 0.18068345454326543,
                "source": 0,
                "target": 18
            },
            {
                "prerequisite": null,
                "similarity": 0.19590265820293312,
                "source": 0,
                "target": 19
            },
            {
                "prerequisite": null,
                "similarity": 0.23931335956471958,
                "source": 0,
                "target": 20
            },
            {
                "prerequisite": null,
                "similarity": 0.18068345454326543,
                "source": 0,
                "target": 21
            },
            {
                "prerequisite": null,
                "similarity": 0.338678725346757,
                "source": 0,
                "target": 22
            },
            {
                "prerequisite": null,
                "similarity": 0.42160575613866025,
                "source": 0,
                "target": 23
            },
            {
                "prerequisite": null,
                "similarity": 0.18068345454326543,
                "source": 0,
                "target": 24
            },
            {
                "prerequisite": null,
                "similarity": 0.4481838986177358,
                "source": 0,
                "target": 25
            },
            {
                "prerequisite": null,
                "similarity": 0.18068345454326543,
                "source": 0,
                "target": 26
            },
            {
                "prerequisite": null,
                "similarity": 0.18068345454326543,
                "source": 0,
                "target": 27
            },
            {
                "prerequisite": null,
                "similarity": 0.25092260702528774,
                "source": 0,
                "target": 28
            },
            {
                "prerequisite": null,
                "similarity": 0.3034888889981533,
                "source": 0,
                "target": 29
            },
            {
                "prerequisite": null,
                "similarity": 0.8080840395142804,
                "source": 1,
                "target": 0
            },
            {
                "prerequisite": null,
                "similarity": 1.0,
                "source": 1,
                "target": 1
            },
            {
                "prerequisite": null,
                "similarity": 0.5619572074629301,
                "source": 1,
                "target": 2
            },
            {
                "prerequisite": null,
                "similarity": 0.35009537009537356,
                "source": 1,
                "target": 3
            },
            {
                "prerequisite": null,
                "similarity": 0.11122439674672152,
                "source": 1,
                "target": 4
            },
            {
                "prerequisite": null,
                "similarity": 0.43625685696872657,
                "source": 1,
                "target": 5
            },
            {
                "prerequisite": null,
                "similarity": 0.10820061342845128,
                "source": 1,
                "target": 6
            },
            {
                "prerequisite": null,
                "similarity": 0.5134220650755373,
                "source": 1,
                "target": 7
            },
            {
                "prerequisite": null,
                "similarity": 0.4742125011134046,
                "source": 1,
                "target": 8
            },
            {
                "prerequisite": null,
                "similarity": 0.1020364852592947,
                "source": 1,
                "target": 9
            },
            {
                "prerequisite": null,
                "similarity": 0.44956208065461184,
                "source": 1,
                "target": 10
            },
            {
                "prerequisite": null,
                "similarity": 0.1065659959483148,
                "source": 1,
                "target": 11
            },
            {
                "prerequisite": null,
                "similarity": 0.29843981604105574,
                "source": 1,
                "target": 12
            },
            {
                "prerequisite": null,
                "similarity": 0.3602979739882123,
                "source": 1,
                "target": 13
            },
            {
                "prerequisite": null,
                "similarity": 0.3446944963337696,
                "source": 1,
                "target": 14
            },
            {
                "prerequisite": null,
                "similarity": 0.19108273398696052,
                "source": 1,
                "target": 15
            },
            {
                "prerequisite": null,
                "similarity": 0.1662212322234179,
                "source": 1,
                "target": 16
            },
            {
                "prerequisite": null,
                "similarity": 0.2515189698117507,
                "source": 1,
                "target": 17
            },
            {
                "prerequisite": null,
                "similarity": 0.44738267821834815,
                "source": 1,
                "target": 18
            },
            {
                "prerequisite": null,
                "similarity": 0.2888989734783358,
                "source": 1,
                "target": 19
            },
            {
                "prerequisite": null,
                "similarity": 0.1516031634885187,
                "source": 1,
                "target": 20
            },
            {
                "prerequisite": null,
                "similarity": 0.44738267821834815,
                "source": 1,
                "target": 21
            },
            {
                "prerequisite": null,
                "similarity": 0.23563737698663853,
                "source": 1,
                "target": 22
            },
            {
                "prerequisite": null,
                "similarity": 0.3729009798950151,
                "source": 1,
                "target": 23
            },
            {
                "prerequisite": null,
                "similarity": 0.44738267821834815,
                "source": 1,
                "target": 24
            },
            {
                "prerequisite": null,
                "similarity": 0.400393738570729,
                "source": 1,
                "target": 25
            },
            {
                "prerequisite": null,
                "similarity": 0.44738267821834815,
                "source": 1,
                "target": 26
            },
            {
                "prerequisite": null,
                "similarity": 0.44738267821834815,
                "source": 1,
                "target": 27
            },
            {
                "prerequisite": null,
                "similarity": 0.3479012333384208,
                "source": 1,
                "target": 28
            },
            {
                "prerequisite": null,
                "similarity": 0.470625121977445,
                "source": 1,
                "target": 29
            },
            {
                "prerequisite": null,
                "similarity": 0.5636778767007926,
                "source": 2,
                "target": 0
            },
            {
                "prerequisite": null,
                "similarity": 0.5619572074629301,
                "source": 2,
                "target": 1
            },
            {
                "prerequisite": null,
                "similarity": 1.0,
                "source": 2,
                "target": 2
            },
            {
                "prerequisite": null,
                "similarity": 0.2841477574492909,
                "source": 2,
                "target": 3
            },
            {
                "prerequisite": null,
                "similarity": 0.12385518793711235,
                "source": 2,
                "target": 4
            },
            {
                "prerequisite": null,
                "similarity": 0.35938940812026143,
                "source": 2,
                "target": 5
            },
            {
                "prerequisite": null,
                "similarity": 0.0641296639649016,
                "source": 2,
                "target": 6
            },
            {
                "prerequisite": null,
                "similarity": 0.22298106782893545,
                "source": 2,
                "target": 7
            },
            {
                "prerequisite": null,
                "similarity": 0.27451440705264296,
                "source": 2,
                "target": 8
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 2,
                "target": 9
            },
            {
                "prerequisite": 0.0841386092197392,
                "similarity": 0.8311096058843884,
                "source": 10,
                "target": 2
            },
            {
                "prerequisite": null,
                "similarity": 0.12617199629457684,
                "source": 2,
                "target": 11
            },
            {
                "prerequisite": null,
                "similarity": 0.18075300732861488,
                "source": 2,
                "target": 12
            },
            {
                "prerequisite": null,
                "similarity": 0.03250915319841993,
                "source": 2,
                "target": 13
            },
            {
                "prerequisite": null,
                "similarity": 0.1820783990476476,
                "source": 2,
                "target": 14
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 2,
                "target": 15
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 2,
                "target": 16
            },
            {
                "prerequisite": null,
                "similarity": 0.044722002501839275,
                "source": 2,
                "target": 17
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 2,
                "target": 18
            },
            {
                "prerequisite": null,
                "similarity": 0.04575520755786226,
                "source": 2,
                "target": 19
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 2,
                "target": 20
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 2,
                "target": 21
            },
            {
                "prerequisite": null,
                "similarity": 0.07602801576176127,
                "source": 2,
                "target": 22
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 2,
                "target": 23
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 2,
                "target": 24
            },
            {
                "prerequisite": null,
                "similarity": 0.20241500180625424,
                "source": 2,
                "target": 25
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 2,
                "target": 26
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 2,
                "target": 27
            },
            {
                "prerequisite": null,
                "similarity": 0.30929754203825727,
                "source": 2,
                "target": 28
            },
            {
                "prerequisite": null,
                "similarity": 0.5811631630324262,
                "source": 2,
                "target": 29
            },
            {
                "prerequisite": null,
                "similarity": 0.43068029549822884,
                "source": 3,
                "target": 0
            },
            {
                "prerequisite": null,
                "similarity": 0.35009537009537356,
                "source": 3,
                "target": 1
            },
            {
                "prerequisite": null,
                "similarity": 0.2841477574492909,
                "source": 3,
                "target": 2
            },
            {
                "prerequisite": null,
                "similarity": 0.9999999999999998,
                "source": 3,
                "target": 3
            },
            {
                "prerequisite": null,
                "similarity": 0.2242393196444807,
                "source": 3,
                "target": 4
            },
            {
                "prerequisite": null,
                "similarity": 0.11142716232199068,
                "source": 3,
                "target": 5
            },
            {
                "prerequisite": null,
                "similarity": 0.3291856997220141,
                "source": 3,
                "target": 6
            },
            {
                "prerequisite": null,
                "similarity": 0.2166229739348111,
                "source": 3,
                "target": 7
            },
            {
                "prerequisite": null,
                "similarity": 0.16381766603135145,
                "source": 3,
                "target": 8
            },
            {
                "prerequisite": null,
                "similarity": 0.1552160876586293,
                "source": 3,
                "target": 9
            },
            {
                "prerequisite": null,
                "similarity": 0.126439960342016,
                "source": 3,
                "target": 10
            },
            {
                "prerequisite": 0.7883239891131839,
                "similarity": 0.777747176077368,
                "source": 3,
                "target": 11
            },
            {
                "prerequisite": null,
                "similarity": 0.15753287312630496,
                "source": 3,
                "target": 12
            },
            {
                "prerequisite": null,
                "similarity": 0.04368298023176616,
                "source": 3,
                "target": 13
            },
            {
                "prerequisite": null,
                "similarity": 0.01039103118458794,
                "source": 3,
                "target": 14
            },
            {
                "prerequisite": null,
                "similarity": 0.1699691181678925,
                "source": 3,
                "target": 15
            },
            {
                "prerequisite": null,
                "similarity": 0.06321319596111141,
                "source": 3,
                "target": 16
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 3,
                "target": 17
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 3,
                "target": 18
            },
            {
                "prerequisite": null,
                "similarity": 0.24791546671754017,
                "source": 3,
                "target": 19
            },
            {
                "prerequisite": null,
                "similarity": 0.18479193010682446,
                "source": 3,
                "target": 20
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 3,
                "target": 21
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 3,
                "target": 22
            },
            {
                "prerequisite": null,
                "similarity": 0.04483396794843901,
                "source": 3,
                "target": 23
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 3,
                "target": 24
            },
            {
                "prerequisite": null,
                "similarity": 0.015058288113059337,
                "source": 3,
                "target": 25
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 3,
                "target": 26
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 3,
                "target": 27
            },
            {
                "prerequisite": null,
                "similarity": 0.2694350359172675,
                "source": 3,
                "target": 28
            },
            {
                "prerequisite": null,
                "similarity": 0.16685655585466486,
                "source": 3,
                "target": 29
            },
            {
                "prerequisite": null,
                "similarity": 0.10715690887447457,
                "source": 4,
                "target": 0
            },
            {
                "prerequisite": null,
                "similarity": 0.11122439674672152,
                "source": 4,
                "target": 1
            },
            {
                "prerequisite": null,
                "similarity": 0.12385518793711235,
                "source": 4,
                "target": 2
            },
            {
                "prerequisite": null,
                "similarity": 0.2242393196444807,
                "source": 4,
                "target": 3
            },
            {
                "prerequisite": null,
                "similarity": 0.9999999999999999,
                "source": 4,
                "target": 4
            },
            {
                "prerequisite": null,
                "similarity": 0.014695586842224058,
                "source": 4,
                "target": 5
            },
            {
                "prerequisite": null,
                "similarity": 0.02555272585721188,
                "source": 4,
                "target": 6
            },
            {
                "prerequisite": null,
                "similarity": 0.0022270017061608856,
                "source": 4,
                "target": 7
            },
            {
                "prerequisite": null,
                "similarity": 0.1614487829505727,
                "source": 4,
                "target": 8
            },
            {
                "prerequisite": null,
                "similarity": 0.2309890076799626,
                "source": 4,
                "target": 9
            },
            {
                "prerequisite": null,
                "similarity": 0.015395650357018845,
                "source": 4,
                "target": 10
            },
            {
                "prerequisite": null,
                "similarity": 0.07757075682239839,
                "source": 4,
                "target": 11
            },
            {
                "prerequisite": null,
                "similarity": 0.059193651040345116,
                "source": 4,
                "target": 12
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 4,
                "target": 13
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 4,
                "target": 14
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 4,
                "target": 15
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 4,
                "target": 16
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 4,
                "target": 17
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 4,
                "target": 18
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 4,
                "target": 19
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 4,
                "target": 20
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 4,
                "target": 21
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 4,
                "target": 22
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 4,
                "target": 23
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 4,
                "target": 24
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 4,
                "target": 25
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 4,
                "target": 26
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 4,
                "target": 27
            },
            {
                "prerequisite": null,
                "similarity": 0.3165282637037626,
                "source": 4,
                "target": 28
            },
            {
                "prerequisite": null,
                "similarity": 0.022579443930242065,
                "source": 4,
                "target": 29
            },
            {
                "prerequisite": null,
                "similarity": 0.5366039489251923,
                "source": 5,
                "target": 0
            },
            {
                "prerequisite": null,
                "similarity": 0.43625685696872657,
                "source": 5,
                "target": 1
            },
            {
                "prerequisite": null,
                "similarity": 0.35938940812026143,
                "source": 5,
                "target": 2
            },
            {
                "prerequisite": null,
                "similarity": 0.11142716232199068,
                "source": 5,
                "target": 3
            },
            {
                "prerequisite": null,
                "similarity": 0.014695586842224058,
                "source": 5,
                "target": 4
            },
            {
                "prerequisite": null,
                "similarity": 0.9999999999999998,
                "source": 5,
                "target": 5
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 5,
                "target": 6
            },
            {
                "prerequisite": null,
                "similarity": 0.26075872854091564,
                "source": 5,
                "target": 7
            },
            {
                "prerequisite": null,
                "similarity": 0.05737373752846754,
                "source": 5,
                "target": 8
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 5,
                "target": 9
            },
            {
                "prerequisite": null,
                "similarity": 0.306868656217072,
                "source": 5,
                "target": 10
            },
            {
                "prerequisite": null,
                "similarity": 0.11611707251722568,
                "source": 5,
                "target": 11
            },
            {
                "prerequisite": null,
                "similarity": 0.26374672536013033,
                "source": 5,
                "target": 12
            },
            {
                "prerequisite": null,
                "similarity": 0.5918245304043911,
                "source": 5,
                "target": 13
            },
            {
                "prerequisite": null,
                "similarity": 0.20045875833631033,
                "source": 5,
                "target": 14
            },
            {
                "prerequisite": null,
                "similarity": 0.04811781096642555,
                "source": 5,
                "target": 15
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 5,
                "target": 16
            },
            {
                "prerequisite": null,
                "similarity": 0.17404052573396037,
                "source": 5,
                "target": 17
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 5,
                "target": 18
            },
            {
                "prerequisite": null,
                "similarity": 0.029676892554581388,
                "source": 5,
                "target": 19
            },
            {
                "prerequisite": null,
                "similarity": 0.03586819391877206,
                "source": 5,
                "target": 20
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 5,
                "target": 21
            },
            {
                "prerequisite": 1.5288682809626195,
                "similarity": 0.8492885260050134,
                "source": 22,
                "target": 5
            },
            {
                "prerequisite": null,
                "similarity": 0.15295611329019268,
                "source": 5,
                "target": 23
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 5,
                "target": 24
            },
            {
                "prerequisite": null,
                "similarity": 0.05137303986179206,
                "source": 5,
                "target": 25
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 5,
                "target": 26
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 5,
                "target": 27
            },
            {
                "prerequisite": null,
                "similarity": 0.1490865225172359,
                "source": 5,
                "target": 28
            },
            {
                "prerequisite": null,
                "similarity": 0.10000468371583315,
                "source": 5,
                "target": 29
            },
            {
                "prerequisite": null,
                "similarity": 0.13932932507848195,
                "source": 6,
                "target": 0
            },
            {
                "prerequisite": null,
                "similarity": 0.10820061342845128,
                "source": 6,
                "target": 1
            },
            {
                "prerequisite": null,
                "similarity": 0.0641296639649016,
                "source": 6,
                "target": 2
            },
            {
                "prerequisite": null,
                "similarity": 0.3291856997220141,
                "source": 6,
                "target": 3
            },
            {
                "prerequisite": null,
                "similarity": 0.02555272585721188,
                "source": 6,
                "target": 4
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 6,
                "target": 5
            },
            {
                "prerequisite": null,
                "similarity": 1.0,
                "source": 6,
                "target": 6
            },
            {
                "prerequisite": null,
                "similarity": 0.08715319526399362,
                "source": 6,
                "target": 7
            },
            {
                "prerequisite": null,
                "similarity": 0.13752090971817152,
                "source": 6,
                "target": 8
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 6,
                "target": 9
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 6,
                "target": 10
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 6,
                "target": 11
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 6,
                "target": 12
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 6,
                "target": 13
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 6,
                "target": 14
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 6,
                "target": 15
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 6,
                "target": 16
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 6,
                "target": 17
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 6,
                "target": 18
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 6,
                "target": 19
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 6,
                "target": 20
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 6,
                "target": 21
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 6,
                "target": 22
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 6,
                "target": 23
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 6,
                "target": 24
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 6,
                "target": 25
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 6,
                "target": 26
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 6,
                "target": 27
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 6,
                "target": 28
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 6,
                "target": 29
            },
            {
                "prerequisite": null,
                "similarity": 0.39999842005088027,
                "source": 7,
                "target": 0
            },
            {
                "prerequisite": null,
                "similarity": 0.5134220650755373,
                "source": 7,
                "target": 1
            },
            {
                "prerequisite": null,
                "similarity": 0.22298106782893545,
                "source": 7,
                "target": 2
            },
            {
                "prerequisite": null,
                "similarity": 0.2166229739348111,
                "source": 7,
                "target": 3
            },
            {
                "prerequisite": null,
                "similarity": 0.0022270017061608856,
                "source": 7,
                "target": 4
            },
            {
                "prerequisite": null,
                "similarity": 0.26075872854091564,
                "source": 7,
                "target": 5
            },
            {
                "prerequisite": null,
                "similarity": 0.08715319526399362,
                "source": 7,
                "target": 6
            },
            {
                "prerequisite": null,
                "similarity": 1.0,
                "source": 7,
                "target": 7
            },
            {
                "prerequisite": null,
                "similarity": 0.0958098465362987,
                "source": 7,
                "target": 8
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 7,
                "target": 9
            },
            {
                "prerequisite": null,
                "similarity": 0.37559792272882914,
                "source": 7,
                "target": 10
            },
            {
                "prerequisite": null,
                "similarity": 0.013129832080939798,
                "source": 7,
                "target": 11
            },
            {
                "prerequisite": null,
                "similarity": 0.09894664427015754,
                "source": 7,
                "target": 12
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 7,
                "target": 13
            },
            {
                "prerequisite": null,
                "similarity": 0.13408159247910978,
                "source": 7,
                "target": 14
            },
            {
                "prerequisite": null,
                "similarity": 0.23769137182119826,
                "source": 7,
                "target": 15
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 7,
                "target": 16
            },
            {
                "prerequisite": null,
                "similarity": 0.12155577234188585,
                "source": 7,
                "target": 17
            },
            {
                "prerequisite": null,
                "similarity": 0.09827913508492898,
                "source": 7,
                "target": 18
            },
            {
                "prerequisite": null,
                "similarity": 0.3913329780716614,
                "source": 7,
                "target": 19
            },
            {
                "prerequisite": null,
                "similarity": 0.1993285864803108,
                "source": 7,
                "target": 20
            },
            {
                "prerequisite": null,
                "similarity": 0.09827913508492898,
                "source": 7,
                "target": 21
            },
            {
                "prerequisite": null,
                "similarity": 0.05203636626376938,
                "source": 7,
                "target": 22
            },
            {
                "prerequisite": null,
                "similarity": 0.1888923647303508,
                "source": 7,
                "target": 23
            },
            {
                "prerequisite": null,
                "similarity": 0.09827913508492898,
                "source": 7,
                "target": 24
            },
            {
                "prerequisite": null,
                "similarity": 0.3498923038977671,
                "source": 7,
                "target": 25
            },
            {
                "prerequisite": null,
                "similarity": 0.09827913508492898,
                "source": 7,
                "target": 26
            },
            {
                "prerequisite": null,
                "similarity": 0.09827913508492898,
                "source": 7,
                "target": 27
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 7,
                "target": 28
            },
            {
                "prerequisite": null,
                "similarity": 0.06268112424851277,
                "source": 7,
                "target": 29
            },
            {
                "prerequisite": null,
                "similarity": 0.28017899266183993,
                "source": 8,
                "target": 0
            },
            {
                "prerequisite": null,
                "similarity": 0.4742125011134046,
                "source": 8,
                "target": 1
            },
            {
                "prerequisite": null,
                "similarity": 0.27451440705264296,
                "source": 8,
                "target": 2
            },
            {
                "prerequisite": null,
                "similarity": 0.16381766603135145,
                "source": 8,
                "target": 3
            },
            {
                "prerequisite": null,
                "similarity": 0.1614487829505727,
                "source": 8,
                "target": 4
            },
            {
                "prerequisite": null,
                "similarity": 0.05737373752846754,
                "source": 8,
                "target": 5
            },
            {
                "prerequisite": null,
                "similarity": 0.13752090971817152,
                "source": 8,
                "target": 6
            },
            {
                "prerequisite": null,
                "similarity": 0.0958098465362987,
                "source": 8,
                "target": 7
            },
            {
                "prerequisite": null,
                "similarity": 1.0,
                "source": 8,
                "target": 8
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 8,
                "target": 9
            },
            {
                "prerequisite": null,
                "similarity": 0.38563023079330794,
                "source": 8,
                "target": 10
            },
            {
                "prerequisite": null,
                "similarity": 0.09660935122760975,
                "source": 8,
                "target": 11
            },
            {
                "prerequisite": null,
                "similarity": 0.09928615892519664,
                "source": 8,
                "target": 12
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 8,
                "target": 13
            },
            {
                "prerequisite": null,
                "similarity": 0.2676356117022354,
                "source": 8,
                "target": 14
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 8,
                "target": 15
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 8,
                "target": 16
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 8,
                "target": 17
            },
            {
                "prerequisite": 2.4251687626683647,
                "similarity": 0.8529222379329148,
                "source": 18,
                "target": 8
            },
            {
                "prerequisite": null,
                "similarity": 0.15830414322575426,
                "source": 8,
                "target": 19
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 8,
                "target": 20
            },
            {
                "prerequisite": 0.07535158222595434,
                "similarity": 0.8529222379329148,
                "source": 21,
                "target": 8
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 8,
                "target": 22
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 8,
                "target": 23
            },
            {
                "prerequisite": 1.3650556353072512,
                "similarity": 0.8529222379329148,
                "source": 24,
                "target": 8
            },
            {
                "prerequisite": 0.6239206989888467,
                "similarity": 0.6247164155370181,
                "source": 25,
                "target": 8
            },
            {
                "prerequisite": 0.5215291226783142,
                "similarity": 0.8529222379329148,
                "source": 26,
                "target": 8
            },
            {
                "prerequisite": 1.0,
                "similarity": 0.8529222379329148,
                "source": 8,
                "target": 27
            },
            {
                "prerequisite": null,
                "similarity": 0.1703507289957843,
                "source": 8,
                "target": 28
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 8,
                "target": 29
            },
            {
                "prerequisite": null,
                "similarity": 0.047289390214026186,
                "source": 9,
                "target": 0
            },
            {
                "prerequisite": null,
                "similarity": 0.1020364852592947,
                "source": 9,
                "target": 1
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 9,
                "target": 2
            },
            {
                "prerequisite": null,
                "similarity": 0.1552160876586293,
                "source": 9,
                "target": 3
            },
            {
                "prerequisite": null,
                "similarity": 0.2309890076799626,
                "source": 9,
                "target": 4
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 9,
                "target": 5
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 9,
                "target": 6
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 9,
                "target": 7
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 9,
                "target": 8
            },
            {
                "prerequisite": null,
                "similarity": 1.0,
                "source": 9,
                "target": 9
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 9,
                "target": 10
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 9,
                "target": 11
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 9,
                "target": 12
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 9,
                "target": 13
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 9,
                "target": 14
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 9,
                "target": 15
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 9,
                "target": 16
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 9,
                "target": 17
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 9,
                "target": 18
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 9,
                "target": 19
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 9,
                "target": 20
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 9,
                "target": 21
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 9,
                "target": 22
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 9,
                "target": 23
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 9,
                "target": 24
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 9,
                "target": 25
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 9,
                "target": 26
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 9,
                "target": 27
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 9,
                "target": 28
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 9,
                "target": 29
            },
            {
                "prerequisite": null,
                "similarity": 0.4099604475607465,
                "source": 10,
                "target": 0
            },
            {
                "prerequisite": null,
                "similarity": 0.44956208065461184,
                "source": 10,
                "target": 1
            },
            {
                "prerequisite": null,
                "similarity": 0.8311096058843884,
                "source": 10,
                "target": 2
            },
            {
                "prerequisite": null,
                "similarity": 0.126439960342016,
                "source": 10,
                "target": 3
            },
            {
                "prerequisite": null,
                "similarity": 0.015395650357018845,
                "source": 10,
                "target": 4
            },
            {
                "prerequisite": null,
                "similarity": 0.306868656217072,
                "source": 10,
                "target": 5
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 10,
                "target": 6
            },
            {
                "prerequisite": null,
                "similarity": 0.37559792272882914,
                "source": 10,
                "target": 7
            },
            {
                "prerequisite": null,
                "similarity": 0.38563023079330794,
                "source": 10,
                "target": 8
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 10,
                "target": 9
            },
            {
                "prerequisite": null,
                "similarity": 1.0,
                "source": 10,
                "target": 10
            },
            {
                "prerequisite": null,
                "similarity": 0.04666711798944105,
                "source": 10,
                "target": 11
            },
            {
                "prerequisite": null,
                "similarity": 0.15705015188313087,
                "source": 10,
                "target": 12
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 10,
                "target": 13
            },
            {
                "prerequisite": null,
                "similarity": 0.40465890975190966,
                "source": 10,
                "target": 14
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 10,
                "target": 15
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 10,
                "target": 16
            },
            {
                "prerequisite": null,
                "similarity": 0.2047739888390984,
                "source": 10,
                "target": 17
            },
            {
                "prerequisite": null,
                "similarity": 0.08278097421155041,
                "source": 10,
                "target": 18
            },
            {
                "prerequisite": null,
                "similarity": 0.015364321171543266,
                "source": 10,
                "target": 19
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 10,
                "target": 20
            },
            {
                "prerequisite": null,
                "similarity": 0.08278097421155041,
                "source": 10,
                "target": 21
            },
            {
                "prerequisite": null,
                "similarity": 0.08766094837976357,
                "source": 10,
                "target": 22
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 10,
                "target": 23
            },
            {
                "prerequisite": null,
                "similarity": 0.08278097421155041,
                "source": 10,
                "target": 24
            },
            {
                "prerequisite": null,
                "similarity": 0.3280551897618716,
                "source": 10,
                "target": 25
            },
            {
                "prerequisite": null,
                "similarity": 0.08278097421155041,
                "source": 10,
                "target": 26
            },
            {
                "prerequisite": null,
                "similarity": 0.08278097421155041,
                "source": 10,
                "target": 27
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 10,
                "target": 28
            },
            {
                "prerequisite": null,
                "similarity": 0.36506402408916916,
                "source": 10,
                "target": 29
            },
            {
                "prerequisite": null,
                "similarity": 0.2701458833066819,
                "source": 11,
                "target": 0
            },
            {
                "prerequisite": null,
                "similarity": 0.1065659959483148,
                "source": 11,
                "target": 1
            },
            {
                "prerequisite": null,
                "similarity": 0.12617199629457684,
                "source": 11,
                "target": 2
            },
            {
                "prerequisite": null,
                "similarity": 0.777747176077368,
                "source": 11,
                "target": 3
            },
            {
                "prerequisite": null,
                "similarity": 0.07757075682239839,
                "source": 11,
                "target": 4
            },
            {
                "prerequisite": null,
                "similarity": 0.11611707251722568,
                "source": 11,
                "target": 5
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 11,
                "target": 6
            },
            {
                "prerequisite": null,
                "similarity": 0.013129832080939798,
                "source": 11,
                "target": 7
            },
            {
                "prerequisite": null,
                "similarity": 0.09660935122760975,
                "source": 11,
                "target": 8
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 11,
                "target": 9
            },
            {
                "prerequisite": null,
                "similarity": 0.04666711798944105,
                "source": 11,
                "target": 10
            },
            {
                "prerequisite": null,
                "similarity": 1.0,
                "source": 11,
                "target": 11
            },
            {
                "prerequisite": null,
                "similarity": 0.23402442046455327,
                "source": 11,
                "target": 12
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 11,
                "target": 13
            },
            {
                "prerequisite": null,
                "similarity": 0.020766965767108122,
                "source": 11,
                "target": 14
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 11,
                "target": 15
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 11,
                "target": 16
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 11,
                "target": 17
            },
            {
                "prerequisite": null,
                "similarity": 0.05775141196466608,
                "source": 11,
                "target": 18
            },
            {
                "prerequisite": null,
                "similarity": 0.02289871548917163,
                "source": 11,
                "target": 19
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 11,
                "target": 20
            },
            {
                "prerequisite": null,
                "similarity": 0.05775141196466608,
                "source": 11,
                "target": 21
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 11,
                "target": 22
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 11,
                "target": 23
            },
            {
                "prerequisite": null,
                "similarity": 0.05775141196466608,
                "source": 11,
                "target": 24
            },
            {
                "prerequisite": null,
                "similarity": 0.04613311477395592,
                "source": 11,
                "target": 25
            },
            {
                "prerequisite": null,
                "similarity": 0.05775141196466608,
                "source": 11,
                "target": 26
            },
            {
                "prerequisite": null,
                "similarity": 0.05775141196466608,
                "source": 11,
                "target": 27
            },
            {
                "prerequisite": null,
                "similarity": 0.12462731905152899,
                "source": 11,
                "target": 28
            },
            {
                "prerequisite": null,
                "similarity": 0.07366616374383199,
                "source": 11,
                "target": 29
            },
            {
                "prerequisite": null,
                "similarity": 0.43176771352481347,
                "source": 12,
                "target": 0
            },
            {
                "prerequisite": null,
                "similarity": 0.29843981604105574,
                "source": 12,
                "target": 1
            },
            {
                "prerequisite": null,
                "similarity": 0.18075300732861488,
                "source": 12,
                "target": 2
            },
            {
                "prerequisite": null,
                "similarity": 0.15753287312630496,
                "source": 12,
                "target": 3
            },
            {
                "prerequisite": null,
                "similarity": 0.059193651040345116,
                "source": 12,
                "target": 4
            },
            {
                "prerequisite": null,
                "similarity": 0.26374672536013033,
                "source": 12,
                "target": 5
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 12,
                "target": 6
            },
            {
                "prerequisite": null,
                "similarity": 0.09894664427015754,
                "source": 12,
                "target": 7
            },
            {
                "prerequisite": null,
                "similarity": 0.09928615892519664,
                "source": 12,
                "target": 8
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 12,
                "target": 9
            },
            {
                "prerequisite": null,
                "similarity": 0.15705015188313087,
                "source": 12,
                "target": 10
            },
            {
                "prerequisite": null,
                "similarity": 0.23402442046455327,
                "source": 12,
                "target": 11
            },
            {
                "prerequisite": null,
                "similarity": 1.0,
                "source": 12,
                "target": 12
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 12,
                "target": 13
            },
            {
                "prerequisite": 2.4150361318478972,
                "similarity": 0.7048348175692629,
                "source": 14,
                "target": 12
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 12,
                "target": 15
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 12,
                "target": 16
            },
            {
                "prerequisite": 0.3387950789584278,
                "similarity": 0.7198856845739285,
                "source": 12,
                "target": 17
            },
            {
                "prerequisite": null,
                "similarity": 0.11640704686727354,
                "source": 12,
                "target": 18
            },
            {
                "prerequisite": null,
                "similarity": 0.021605390269134196,
                "source": 12,
                "target": 19
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 12,
                "target": 20
            },
            {
                "prerequisite": null,
                "similarity": 0.11640704686727354,
                "source": 12,
                "target": 21
            },
            {
                "prerequisite": null,
                "similarity": 0.308173231339218,
                "source": 12,
                "target": 22
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 12,
                "target": 23
            },
            {
                "prerequisite": null,
                "similarity": 0.11640704686727354,
                "source": 12,
                "target": 24
            },
            {
                "prerequisite": null,
                "similarity": 0.06465589588435626,
                "source": 12,
                "target": 25
            },
            {
                "prerequisite": null,
                "similarity": 0.11640704686727354,
                "source": 12,
                "target": 26
            },
            {
                "prerequisite": null,
                "similarity": 0.11640704686727354,
                "source": 12,
                "target": 27
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 12,
                "target": 28
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 12,
                "target": 29
            },
            {
                "prerequisite": null,
                "similarity": 0.385315037099065,
                "source": 13,
                "target": 0
            },
            {
                "prerequisite": null,
                "similarity": 0.3602979739882123,
                "source": 13,
                "target": 1
            },
            {
                "prerequisite": null,
                "similarity": 0.03250915319841993,
                "source": 13,
                "target": 2
            },
            {
                "prerequisite": null,
                "similarity": 0.04368298023176616,
                "source": 13,
                "target": 3
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 13,
                "target": 4
            },
            {
                "prerequisite": null,
                "similarity": 0.5918245304043911,
                "source": 13,
                "target": 5
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 13,
                "target": 6
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 13,
                "target": 7
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 13,
                "target": 8
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 13,
                "target": 9
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 13,
                "target": 10
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 13,
                "target": 11
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 13,
                "target": 12
            },
            {
                "prerequisite": null,
                "similarity": 0.9999999999999998,
                "source": 13,
                "target": 13
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 13,
                "target": 14
            },
            {
                "prerequisite": null,
                "similarity": 0.23674292606999442,
                "source": 13,
                "target": 15
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 13,
                "target": 16
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 13,
                "target": 17
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 13,
                "target": 18
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 13,
                "target": 19
            },
            {
                "prerequisite": null,
                "similarity": 0.17647397108528262,
                "source": 13,
                "target": 20
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 13,
                "target": 21
            },
            {
                "prerequisite": null,
                "similarity": 0.466774421472994,
                "source": 13,
                "target": 22
            },
            {
                "prerequisite": 1.7625895963204377,
                "similarity": 0.7525545550249654,
                "source": 13,
                "target": 23
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 13,
                "target": 24
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 13,
                "target": 25
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 13,
                "target": 26
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 13,
                "target": 27
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 13,
                "target": 28
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 13,
                "target": 29
            },
            {
                "prerequisite": null,
                "similarity": 0.29270856401431344,
                "source": 14,
                "target": 0
            },
            {
                "prerequisite": null,
                "similarity": 0.3446944963337696,
                "source": 14,
                "target": 1
            },
            {
                "prerequisite": null,
                "similarity": 0.1820783990476476,
                "source": 14,
                "target": 2
            },
            {
                "prerequisite": null,
                "similarity": 0.01039103118458794,
                "source": 14,
                "target": 3
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 14,
                "target": 4
            },
            {
                "prerequisite": null,
                "similarity": 0.20045875833631033,
                "source": 14,
                "target": 5
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 14,
                "target": 6
            },
            {
                "prerequisite": null,
                "similarity": 0.13408159247910978,
                "source": 14,
                "target": 7
            },
            {
                "prerequisite": null,
                "similarity": 0.2676356117022354,
                "source": 14,
                "target": 8
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 14,
                "target": 9
            },
            {
                "prerequisite": null,
                "similarity": 0.40465890975190966,
                "source": 14,
                "target": 10
            },
            {
                "prerequisite": null,
                "similarity": 0.020766965767108122,
                "source": 14,
                "target": 11
            },
            {
                "prerequisite": null,
                "similarity": 0.7048348175692629,
                "source": 14,
                "target": 12
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 14,
                "target": 13
            },
            {
                "prerequisite": null,
                "similarity": 0.9999999999999999,
                "source": 14,
                "target": 14
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 14,
                "target": 15
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 14,
                "target": 16
            },
            {
                "prerequisite": 0.2718629029943541,
                "similarity": 0.948104406073371,
                "source": 14,
                "target": 17
            },
            {
                "prerequisite": null,
                "similarity": 0.1916381246318516,
                "source": 14,
                "target": 18
            },
            {
                "prerequisite": null,
                "similarity": 0.03556843494051529,
                "source": 14,
                "target": 19
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 14,
                "target": 20
            },
            {
                "prerequisite": null,
                "similarity": 0.1916381246318516,
                "source": 14,
                "target": 21
            },
            {
                "prerequisite": null,
                "similarity": 0.40587054962693236,
                "source": 14,
                "target": 22
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 14,
                "target": 23
            },
            {
                "prerequisite": null,
                "similarity": 0.1916381246318516,
                "source": 14,
                "target": 24
            },
            {
                "prerequisite": null,
                "similarity": 0.1997281605353719,
                "source": 14,
                "target": 25
            },
            {
                "prerequisite": null,
                "similarity": 0.1916381246318516,
                "source": 14,
                "target": 26
            },
            {
                "prerequisite": null,
                "similarity": 0.1916381246318516,
                "source": 14,
                "target": 27
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 14,
                "target": 28
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 14,
                "target": 29
            },
            {
                "prerequisite": null,
                "similarity": 0.2941270800014457,
                "source": 15,
                "target": 0
            },
            {
                "prerequisite": null,
                "similarity": 0.19108273398696052,
                "source": 15,
                "target": 1
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 15,
                "target": 2
            },
            {
                "prerequisite": null,
                "similarity": 0.1699691181678925,
                "source": 15,
                "target": 3
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 15,
                "target": 4
            },
            {
                "prerequisite": null,
                "similarity": 0.04811781096642555,
                "source": 15,
                "target": 5
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 15,
                "target": 6
            },
            {
                "prerequisite": null,
                "similarity": 0.23769137182119826,
                "source": 15,
                "target": 7
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 15,
                "target": 8
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 15,
                "target": 9
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 15,
                "target": 10
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 15,
                "target": 11
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 15,
                "target": 12
            },
            {
                "prerequisite": null,
                "similarity": 0.23674292606999442,
                "source": 15,
                "target": 13
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 15,
                "target": 14
            },
            {
                "prerequisite": null,
                "similarity": 1.0000000000000002,
                "source": 15,
                "target": 15
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 15,
                "target": 16
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 15,
                "target": 17
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 15,
                "target": 18
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 15,
                "target": 19
            },
            {
                "prerequisite": 3.4775831066741865,
                "similarity": 0.9888991029987373,
                "source": 20,
                "target": 15
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 15,
                "target": 21
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 15,
                "target": 22
            },
            {
                "prerequisite": null,
                "similarity": 0.5294005803959675,
                "source": 15,
                "target": 23
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 15,
                "target": 24
            },
            {
                "prerequisite": null,
                "similarity": 0.44777485164345704,
                "source": 15,
                "target": 25
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 15,
                "target": 26
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 15,
                "target": 27
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 15,
                "target": 28
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 15,
                "target": 29
            },
            {
                "prerequisite": null,
                "similarity": 0.41738219556119155,
                "source": 16,
                "target": 0
            },
            {
                "prerequisite": null,
                "similarity": 0.1662212322234179,
                "source": 16,
                "target": 1
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 16,
                "target": 2
            },
            {
                "prerequisite": null,
                "similarity": 0.06321319596111141,
                "source": 16,
                "target": 3
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 16,
                "target": 4
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 16,
                "target": 5
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 16,
                "target": 6
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 16,
                "target": 7
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 16,
                "target": 8
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 16,
                "target": 9
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 16,
                "target": 10
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 16,
                "target": 11
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 16,
                "target": 12
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 16,
                "target": 13
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 16,
                "target": 14
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 16,
                "target": 15
            },
            {
                "prerequisite": null,
                "similarity": 1.0,
                "source": 16,
                "target": 16
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 16,
                "target": 17
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 16,
                "target": 18
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 16,
                "target": 19
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 16,
                "target": 20
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 16,
                "target": 21
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 16,
                "target": 22
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 16,
                "target": 23
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 16,
                "target": 24
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 16,
                "target": 25
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 16,
                "target": 26
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 16,
                "target": 27
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 16,
                "target": 28
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 16,
                "target": 29
            },
            {
                "prerequisite": null,
                "similarity": 0.21376051715988154,
                "source": 17,
                "target": 0
            },
            {
                "prerequisite": null,
                "similarity": 0.2515189698117507,
                "source": 17,
                "target": 1
            },
            {
                "prerequisite": null,
                "similarity": 0.044722002501839275,
                "source": 17,
                "target": 2
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 17,
                "target": 3
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 17,
                "target": 4
            },
            {
                "prerequisite": null,
                "similarity": 0.17404052573396037,
                "source": 17,
                "target": 5
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 17,
                "target": 6
            },
            {
                "prerequisite": null,
                "similarity": 0.12155577234188585,
                "source": 17,
                "target": 7
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 17,
                "target": 8
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 17,
                "target": 9
            },
            {
                "prerequisite": null,
                "similarity": 0.2047739888390984,
                "source": 17,
                "target": 10
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 17,
                "target": 11
            },
            {
                "prerequisite": null,
                "similarity": 0.7198856845739285,
                "source": 17,
                "target": 12
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 17,
                "target": 13
            },
            {
                "prerequisite": null,
                "similarity": 0.948104406073371,
                "source": 17,
                "target": 14
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 17,
                "target": 15
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 17,
                "target": 16
            },
            {
                "prerequisite": null,
                "similarity": 1.0,
                "source": 17,
                "target": 17
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 17,
                "target": 18
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 17,
                "target": 19
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 17,
                "target": 20
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 17,
                "target": 21
            },
            {
                "prerequisite": null,
                "similarity": 0.4280863447390447,
                "source": 17,
                "target": 22
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 17,
                "target": 23
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 17,
                "target": 24
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 17,
                "target": 25
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 17,
                "target": 26
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 17,
                "target": 27
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 17,
                "target": 28
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 17,
                "target": 29
            },
            {
                "prerequisite": null,
                "similarity": 0.18068345454326543,
                "source": 18,
                "target": 0
            },
            {
                "prerequisite": null,
                "similarity": 0.44738267821834815,
                "source": 18,
                "target": 1
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 18,
                "target": 2
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 18,
                "target": 3
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 18,
                "target": 4
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 18,
                "target": 5
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 18,
                "target": 6
            },
            {
                "prerequisite": null,
                "similarity": 0.09827913508492898,
                "source": 18,
                "target": 7
            },
            {
                "prerequisite": null,
                "similarity": 0.8529222379329148,
                "source": 18,
                "target": 8
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 18,
                "target": 9
            },
            {
                "prerequisite": null,
                "similarity": 0.08278097421155041,
                "source": 18,
                "target": 10
            },
            {
                "prerequisite": null,
                "similarity": 0.05775141196466608,
                "source": 18,
                "target": 11
            },
            {
                "prerequisite": null,
                "similarity": 0.11640704686727354,
                "source": 18,
                "target": 12
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 18,
                "target": 13
            },
            {
                "prerequisite": null,
                "similarity": 0.1916381246318516,
                "source": 18,
                "target": 14
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 18,
                "target": 15
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 18,
                "target": 16
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 18,
                "target": 17
            },
            {
                "prerequisite": null,
                "similarity": 1.0,
                "source": 18,
                "target": 18
            },
            {
                "prerequisite": null,
                "similarity": 0.18560208209532628,
                "source": 18,
                "target": 19
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 18,
                "target": 20
            },
            {
                "prerequisite": 3.4157080741606745,
                "similarity": 1.0,
                "source": 18,
                "target": 21
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 18,
                "target": 22
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 18,
                "target": 23
            },
            {
                "prerequisite": 0.9566578701729609,
                "similarity": 1.0,
                "source": 18,
                "target": 24
            },
            {
                "prerequisite": null,
                "similarity": 0.5554293973119723,
                "source": 18,
                "target": 25
            },
            {
                "prerequisite": 0.5184316833712974,
                "similarity": 1.0,
                "source": 18,
                "target": 26
            },
            {
                "prerequisite": 5.175050301810865,
                "similarity": 1.0,
                "source": 18,
                "target": 27
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 18,
                "target": 28
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 18,
                "target": 29
            },
            {
                "prerequisite": null,
                "similarity": 0.19590265820293312,
                "source": 19,
                "target": 0
            },
            {
                "prerequisite": null,
                "similarity": 0.2888989734783358,
                "source": 19,
                "target": 1
            },
            {
                "prerequisite": null,
                "similarity": 0.04575520755786226,
                "source": 19,
                "target": 2
            },
            {
                "prerequisite": null,
                "similarity": 0.24791546671754017,
                "source": 19,
                "target": 3
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 19,
                "target": 4
            },
            {
                "prerequisite": null,
                "similarity": 0.029676892554581388,
                "source": 19,
                "target": 5
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 19,
                "target": 6
            },
            {
                "prerequisite": null,
                "similarity": 0.3913329780716614,
                "source": 19,
                "target": 7
            },
            {
                "prerequisite": null,
                "similarity": 0.15830414322575426,
                "source": 19,
                "target": 8
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 19,
                "target": 9
            },
            {
                "prerequisite": null,
                "similarity": 0.015364321171543266,
                "source": 19,
                "target": 10
            },
            {
                "prerequisite": null,
                "similarity": 0.02289871548917163,
                "source": 19,
                "target": 11
            },
            {
                "prerequisite": null,
                "similarity": 0.021605390269134196,
                "source": 19,
                "target": 12
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 19,
                "target": 13
            },
            {
                "prerequisite": null,
                "similarity": 0.03556843494051529,
                "source": 19,
                "target": 14
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 19,
                "target": 15
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 19,
                "target": 16
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 19,
                "target": 17
            },
            {
                "prerequisite": null,
                "similarity": 0.18560208209532628,
                "source": 19,
                "target": 18
            },
            {
                "prerequisite": null,
                "similarity": 1.0,
                "source": 19,
                "target": 19
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 19,
                "target": 20
            },
            {
                "prerequisite": null,
                "similarity": 0.18560208209532628,
                "source": 19,
                "target": 21
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 19,
                "target": 22
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 19,
                "target": 23
            },
            {
                "prerequisite": null,
                "similarity": 0.18560208209532628,
                "source": 19,
                "target": 24
            },
            {
                "prerequisite": null,
                "similarity": 0.10308885259805427,
                "source": 19,
                "target": 25
            },
            {
                "prerequisite": null,
                "similarity": 0.18560208209532628,
                "source": 19,
                "target": 26
            },
            {
                "prerequisite": null,
                "similarity": 0.18560208209532628,
                "source": 19,
                "target": 27
            },
            {
                "prerequisite": null,
                "similarity": 0.13109732931235313,
                "source": 19,
                "target": 28
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 19,
                "target": 29
            },
            {
                "prerequisite": null,
                "similarity": 0.23931335956471958,
                "source": 20,
                "target": 0
            },
            {
                "prerequisite": null,
                "similarity": 0.1516031634885187,
                "source": 20,
                "target": 1
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 20,
                "target": 2
            },
            {
                "prerequisite": null,
                "similarity": 0.18479193010682446,
                "source": 20,
                "target": 3
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 20,
                "target": 4
            },
            {
                "prerequisite": null,
                "similarity": 0.03586819391877206,
                "source": 20,
                "target": 5
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 20,
                "target": 6
            },
            {
                "prerequisite": null,
                "similarity": 0.1993285864803108,
                "source": 20,
                "target": 7
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 20,
                "target": 8
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 20,
                "target": 9
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 20,
                "target": 10
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 20,
                "target": 11
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 20,
                "target": 12
            },
            {
                "prerequisite": null,
                "similarity": 0.17647397108528262,
                "source": 20,
                "target": 13
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 20,
                "target": 14
            },
            {
                "prerequisite": null,
                "similarity": 0.9888991029987373,
                "source": 20,
                "target": 15
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 20,
                "target": 16
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 20,
                "target": 17
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 20,
                "target": 18
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 20,
                "target": 19
            },
            {
                "prerequisite": null,
                "similarity": 1.0,
                "source": 20,
                "target": 20
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 20,
                "target": 21
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 20,
                "target": 22
            },
            {
                "prerequisite": null,
                "similarity": 0.4213161898605461,
                "source": 20,
                "target": 23
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 20,
                "target": 24
            },
            {
                "prerequisite": null,
                "similarity": 0.3755051247996588,
                "source": 20,
                "target": 25
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 20,
                "target": 26
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 20,
                "target": 27
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 20,
                "target": 28
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 20,
                "target": 29
            },
            {
                "prerequisite": null,
                "similarity": 0.18068345454326543,
                "source": 21,
                "target": 0
            },
            {
                "prerequisite": null,
                "similarity": 0.44738267821834815,
                "source": 21,
                "target": 1
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 21,
                "target": 2
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 21,
                "target": 3
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 21,
                "target": 4
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 21,
                "target": 5
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 21,
                "target": 6
            },
            {
                "prerequisite": null,
                "similarity": 0.09827913508492898,
                "source": 21,
                "target": 7
            },
            {
                "prerequisite": null,
                "similarity": 0.8529222379329148,
                "source": 21,
                "target": 8
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 21,
                "target": 9
            },
            {
                "prerequisite": null,
                "similarity": 0.08278097421155041,
                "source": 21,
                "target": 10
            },
            {
                "prerequisite": null,
                "similarity": 0.05775141196466608,
                "source": 21,
                "target": 11
            },
            {
                "prerequisite": null,
                "similarity": 0.11640704686727354,
                "source": 21,
                "target": 12
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 21,
                "target": 13
            },
            {
                "prerequisite": null,
                "similarity": 0.1916381246318516,
                "source": 21,
                "target": 14
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 21,
                "target": 15
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 21,
                "target": 16
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 21,
                "target": 17
            },
            {
                "prerequisite": null,
                "similarity": 1.0,
                "source": 21,
                "target": 18
            },
            {
                "prerequisite": null,
                "similarity": 0.18560208209532628,
                "source": 21,
                "target": 19
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 21,
                "target": 20
            },
            {
                "prerequisite": null,
                "similarity": 1.0,
                "source": 21,
                "target": 21
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 21,
                "target": 22
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 21,
                "target": 23
            },
            {
                "prerequisite": 1.5000040578959553,
                "similarity": 1.0,
                "source": 21,
                "target": 24
            },
            {
                "prerequisite": null,
                "similarity": 0.5554293973119723,
                "source": 21,
                "target": 25
            },
            {
                "prerequisite": 0.22839382365862076,
                "similarity": 1.0,
                "source": 26,
                "target": 21
            },
            {
                "prerequisite": 0.42857142857142855,
                "similarity": 1.0,
                "source": 21,
                "target": 27
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 21,
                "target": 28
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 21,
                "target": 29
            },
            {
                "prerequisite": null,
                "similarity": 0.338678725346757,
                "source": 22,
                "target": 0
            },
            {
                "prerequisite": null,
                "similarity": 0.23563737698663853,
                "source": 22,
                "target": 1
            },
            {
                "prerequisite": null,
                "similarity": 0.07602801576176127,
                "source": 22,
                "target": 2
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 22,
                "target": 3
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 22,
                "target": 4
            },
            {
                "prerequisite": null,
                "similarity": 0.8492885260050134,
                "source": 22,
                "target": 5
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 22,
                "target": 6
            },
            {
                "prerequisite": null,
                "similarity": 0.05203636626376938,
                "source": 22,
                "target": 7
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 22,
                "target": 8
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 22,
                "target": 9
            },
            {
                "prerequisite": null,
                "similarity": 0.08766094837976357,
                "source": 22,
                "target": 10
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 22,
                "target": 11
            },
            {
                "prerequisite": null,
                "similarity": 0.308173231339218,
                "source": 22,
                "target": 12
            },
            {
                "prerequisite": null,
                "similarity": 0.466774421472994,
                "source": 22,
                "target": 13
            },
            {
                "prerequisite": null,
                "similarity": 0.40587054962693236,
                "source": 22,
                "target": 14
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 22,
                "target": 15
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 22,
                "target": 16
            },
            {
                "prerequisite": null,
                "similarity": 0.4280863447390447,
                "source": 22,
                "target": 17
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 22,
                "target": 18
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 22,
                "target": 19
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 22,
                "target": 20
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 22,
                "target": 21
            },
            {
                "prerequisite": null,
                "similarity": 1.0,
                "source": 22,
                "target": 22
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 22,
                "target": 23
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 22,
                "target": 24
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 22,
                "target": 25
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 22,
                "target": 26
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 22,
                "target": 27
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 22,
                "target": 28
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 22,
                "target": 29
            },
            {
                "prerequisite": null,
                "similarity": 0.42160575613866025,
                "source": 23,
                "target": 0
            },
            {
                "prerequisite": null,
                "similarity": 0.3729009798950151,
                "source": 23,
                "target": 1
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 23,
                "target": 2
            },
            {
                "prerequisite": null,
                "similarity": 0.04483396794843901,
                "source": 23,
                "target": 3
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 23,
                "target": 4
            },
            {
                "prerequisite": null,
                "similarity": 0.15295611329019268,
                "source": 23,
                "target": 5
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 23,
                "target": 6
            },
            {
                "prerequisite": null,
                "similarity": 0.1888923647303508,
                "source": 23,
                "target": 7
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 23,
                "target": 8
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 23,
                "target": 9
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 23,
                "target": 10
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 23,
                "target": 11
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 23,
                "target": 12
            },
            {
                "prerequisite": null,
                "similarity": 0.7525545550249654,
                "source": 23,
                "target": 13
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 23,
                "target": 14
            },
            {
                "prerequisite": null,
                "similarity": 0.5294005803959675,
                "source": 23,
                "target": 15
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 23,
                "target": 16
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 23,
                "target": 17
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 23,
                "target": 18
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 23,
                "target": 19
            },
            {
                "prerequisite": null,
                "similarity": 0.4213161898605461,
                "source": 23,
                "target": 20
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 23,
                "target": 21
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 23,
                "target": 22
            },
            {
                "prerequisite": null,
                "similarity": 1.0000000000000002,
                "source": 23,
                "target": 23
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 23,
                "target": 24
            },
            {
                "prerequisite": null,
                "similarity": 0.35584485017546313,
                "source": 23,
                "target": 25
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 23,
                "target": 26
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 23,
                "target": 27
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 23,
                "target": 28
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 23,
                "target": 29
            },
            {
                "prerequisite": null,
                "similarity": 0.18068345454326543,
                "source": 24,
                "target": 0
            },
            {
                "prerequisite": null,
                "similarity": 0.44738267821834815,
                "source": 24,
                "target": 1
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 24,
                "target": 2
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 24,
                "target": 3
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 24,
                "target": 4
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 24,
                "target": 5
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 24,
                "target": 6
            },
            {
                "prerequisite": null,
                "similarity": 0.09827913508492898,
                "source": 24,
                "target": 7
            },
            {
                "prerequisite": null,
                "similarity": 0.8529222379329148,
                "source": 24,
                "target": 8
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 24,
                "target": 9
            },
            {
                "prerequisite": null,
                "similarity": 0.08278097421155041,
                "source": 24,
                "target": 10
            },
            {
                "prerequisite": null,
                "similarity": 0.05775141196466608,
                "source": 24,
                "target": 11
            },
            {
                "prerequisite": null,
                "similarity": 0.11640704686727354,
                "source": 24,
                "target": 12
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 24,
                "target": 13
            },
            {
                "prerequisite": null,
                "similarity": 0.1916381246318516,
                "source": 24,
                "target": 14
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 24,
                "target": 15
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 24,
                "target": 16
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 24,
                "target": 17
            },
            {
                "prerequisite": null,
                "similarity": 1.0,
                "source": 24,
                "target": 18
            },
            {
                "prerequisite": null,
                "similarity": 0.18560208209532628,
                "source": 24,
                "target": 19
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 24,
                "target": 20
            },
            {
                "prerequisite": null,
                "similarity": 1.0,
                "source": 24,
                "target": 21
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 24,
                "target": 22
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 24,
                "target": 23
            },
            {
                "prerequisite": null,
                "similarity": 1.0,
                "source": 24,
                "target": 24
            },
            {
                "prerequisite": null,
                "similarity": 0.5554293973119723,
                "source": 24,
                "target": 25
            },
            {
                "prerequisite": 0.5731702182814662,
                "similarity": 1.0,
                "source": 24,
                "target": 26
            },
            {
                "prerequisite": 0.42857142857142855,
                "similarity": 1.0,
                "source": 24,
                "target": 27
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 24,
                "target": 28
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 24,
                "target": 29
            },
            {
                "prerequisite": null,
                "similarity": 0.4481838986177358,
                "source": 25,
                "target": 0
            },
            {
                "prerequisite": null,
                "similarity": 0.400393738570729,
                "source": 25,
                "target": 1
            },
            {
                "prerequisite": null,
                "similarity": 0.20241500180625424,
                "source": 25,
                "target": 2
            },
            {
                "prerequisite": null,
                "similarity": 0.015058288113059337,
                "source": 25,
                "target": 3
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 25,
                "target": 4
            },
            {
                "prerequisite": null,
                "similarity": 0.05137303986179206,
                "source": 25,
                "target": 5
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 25,
                "target": 6
            },
            {
                "prerequisite": null,
                "similarity": 0.3498923038977671,
                "source": 25,
                "target": 7
            },
            {
                "prerequisite": null,
                "similarity": 0.6247164155370181,
                "source": 25,
                "target": 8
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 25,
                "target": 9
            },
            {
                "prerequisite": null,
                "similarity": 0.3280551897618716,
                "source": 25,
                "target": 10
            },
            {
                "prerequisite": null,
                "similarity": 0.04613311477395592,
                "source": 25,
                "target": 11
            },
            {
                "prerequisite": null,
                "similarity": 0.06465589588435626,
                "source": 25,
                "target": 12
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 25,
                "target": 13
            },
            {
                "prerequisite": null,
                "similarity": 0.1997281605353719,
                "source": 25,
                "target": 14
            },
            {
                "prerequisite": null,
                "similarity": 0.44777485164345704,
                "source": 25,
                "target": 15
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 25,
                "target": 16
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 25,
                "target": 17
            },
            {
                "prerequisite": null,
                "similarity": 0.5554293973119723,
                "source": 25,
                "target": 18
            },
            {
                "prerequisite": null,
                "similarity": 0.10308885259805427,
                "source": 25,
                "target": 19
            },
            {
                "prerequisite": null,
                "similarity": 0.3755051247996588,
                "source": 25,
                "target": 20
            },
            {
                "prerequisite": null,
                "similarity": 0.5554293973119723,
                "source": 25,
                "target": 21
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 25,
                "target": 22
            },
            {
                "prerequisite": null,
                "similarity": 0.35584485017546313,
                "source": 25,
                "target": 23
            },
            {
                "prerequisite": null,
                "similarity": 0.5554293973119723,
                "source": 25,
                "target": 24
            },
            {
                "prerequisite": null,
                "similarity": 1.0,
                "source": 25,
                "target": 25
            },
            {
                "prerequisite": null,
                "similarity": 0.5554293973119723,
                "source": 25,
                "target": 26
            },
            {
                "prerequisite": null,
                "similarity": 0.5554293973119723,
                "source": 25,
                "target": 27
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 25,
                "target": 28
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 25,
                "target": 29
            },
            {
                "prerequisite": null,
                "similarity": 0.18068345454326543,
                "source": 26,
                "target": 0
            },
            {
                "prerequisite": null,
                "similarity": 0.44738267821834815,
                "source": 26,
                "target": 1
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 26,
                "target": 2
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 26,
                "target": 3
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 26,
                "target": 4
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 26,
                "target": 5
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 26,
                "target": 6
            },
            {
                "prerequisite": null,
                "similarity": 0.09827913508492898,
                "source": 26,
                "target": 7
            },
            {
                "prerequisite": null,
                "similarity": 0.8529222379329148,
                "source": 26,
                "target": 8
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 26,
                "target": 9
            },
            {
                "prerequisite": null,
                "similarity": 0.08278097421155041,
                "source": 26,
                "target": 10
            },
            {
                "prerequisite": null,
                "similarity": 0.05775141196466608,
                "source": 26,
                "target": 11
            },
            {
                "prerequisite": null,
                "similarity": 0.11640704686727354,
                "source": 26,
                "target": 12
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 26,
                "target": 13
            },
            {
                "prerequisite": null,
                "similarity": 0.1916381246318516,
                "source": 26,
                "target": 14
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 26,
                "target": 15
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 26,
                "target": 16
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 26,
                "target": 17
            },
            {
                "prerequisite": null,
                "similarity": 1.0,
                "source": 26,
                "target": 18
            },
            {
                "prerequisite": null,
                "similarity": 0.18560208209532628,
                "source": 26,
                "target": 19
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 26,
                "target": 20
            },
            {
                "prerequisite": null,
                "similarity": 1.0,
                "source": 26,
                "target": 21
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 26,
                "target": 22
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 26,
                "target": 23
            },
            {
                "prerequisite": null,
                "similarity": 1.0,
                "source": 26,
                "target": 24
            },
            {
                "prerequisite": null,
                "similarity": 0.5554293973119723,
                "source": 26,
                "target": 25
            },
            {
                "prerequisite": null,
                "similarity": 1.0,
                "source": 26,
                "target": 26
            },
            {
                "prerequisite": 1.0,
                "similarity": 1.0,
                "source": 26,
                "target": 27
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 26,
                "target": 28
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 26,
                "target": 29
            },
            {
                "prerequisite": null,
                "similarity": 0.18068345454326543,
                "source": 27,
                "target": 0
            },
            {
                "prerequisite": null,
                "similarity": 0.44738267821834815,
                "source": 27,
                "target": 1
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 27,
                "target": 2
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 27,
                "target": 3
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 27,
                "target": 4
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 27,
                "target": 5
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 27,
                "target": 6
            },
            {
                "prerequisite": null,
                "similarity": 0.09827913508492898,
                "source": 27,
                "target": 7
            },
            {
                "prerequisite": null,
                "similarity": 0.8529222379329148,
                "source": 27,
                "target": 8
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 27,
                "target": 9
            },
            {
                "prerequisite": null,
                "similarity": 0.08278097421155041,
                "source": 27,
                "target": 10
            },
            {
                "prerequisite": null,
                "similarity": 0.05775141196466608,
                "source": 27,
                "target": 11
            },
            {
                "prerequisite": null,
                "similarity": 0.11640704686727354,
                "source": 27,
                "target": 12
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 27,
                "target": 13
            },
            {
                "prerequisite": null,
                "similarity": 0.1916381246318516,
                "source": 27,
                "target": 14
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 27,
                "target": 15
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 27,
                "target": 16
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 27,
                "target": 17
            },
            {
                "prerequisite": null,
                "similarity": 1.0,
                "source": 27,
                "target": 18
            },
            {
                "prerequisite": null,
                "similarity": 0.18560208209532628,
                "source": 27,
                "target": 19
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 27,
                "target": 20
            },
            {
                "prerequisite": null,
                "similarity": 1.0,
                "source": 27,
                "target": 21
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 27,
                "target": 22
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 27,
                "target": 23
            },
            {
                "prerequisite": null,
                "similarity": 1.0,
                "source": 27,
                "target": 24
            },
            {
                "prerequisite": null,
                "similarity": 0.5554293973119723,
                "source": 27,
                "target": 25
            },
            {
                "prerequisite": null,
                "similarity": 1.0,
                "source": 27,
                "target": 26
            },
            {
                "prerequisite": null,
                "similarity": 1.0,
                "source": 27,
                "target": 27
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 27,
                "target": 28
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 27,
                "target": 29
            },
            {
                "prerequisite": null,
                "similarity": 0.25092260702528774,
                "source": 28,
                "target": 0
            },
            {
                "prerequisite": null,
                "similarity": 0.3479012333384208,
                "source": 28,
                "target": 1
            },
            {
                "prerequisite": null,
                "similarity": 0.30929754203825727,
                "source": 28,
                "target": 2
            },
            {
                "prerequisite": null,
                "similarity": 0.2694350359172675,
                "source": 28,
                "target": 3
            },
            {
                "prerequisite": null,
                "similarity": 0.3165282637037626,
                "source": 28,
                "target": 4
            },
            {
                "prerequisite": null,
                "similarity": 0.1490865225172359,
                "source": 28,
                "target": 5
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 28,
                "target": 6
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 28,
                "target": 7
            },
            {
                "prerequisite": null,
                "similarity": 0.1703507289957843,
                "source": 28,
                "target": 8
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 28,
                "target": 9
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 28,
                "target": 10
            },
            {
                "prerequisite": null,
                "similarity": 0.12462731905152899,
                "source": 28,
                "target": 11
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 28,
                "target": 12
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 28,
                "target": 13
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 28,
                "target": 14
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 28,
                "target": 15
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 28,
                "target": 16
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 28,
                "target": 17
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 28,
                "target": 18
            },
            {
                "prerequisite": null,
                "similarity": 0.13109732931235313,
                "source": 28,
                "target": 19
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 28,
                "target": 20
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 28,
                "target": 21
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 28,
                "target": 22
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 28,
                "target": 23
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 28,
                "target": 24
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 28,
                "target": 25
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 28,
                "target": 26
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 28,
                "target": 27
            },
            {
                "prerequisite": null,
                "similarity": 1.0,
                "source": 28,
                "target": 28
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 28,
                "target": 29
            },
            {
                "prerequisite": null,
                "similarity": 0.3034888889981533,
                "source": 29,
                "target": 0
            },
            {
                "prerequisite": null,
                "similarity": 0.470625121977445,
                "source": 29,
                "target": 1
            },
            {
                "prerequisite": null,
                "similarity": 0.5811631630324262,
                "source": 29,
                "target": 2
            },
            {
                "prerequisite": null,
                "similarity": 0.16685655585466486,
                "source": 29,
                "target": 3
            },
            {
                "prerequisite": null,
                "similarity": 0.022579443930242065,
                "source": 29,
                "target": 4
            },
            {
                "prerequisite": null,
                "similarity": 0.10000468371583315,
                "source": 29,
                "target": 5
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 29,
                "target": 6
            },
            {
                "prerequisite": null,
                "similarity": 0.06268112424851277,
                "source": 29,
                "target": 7
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 29,
                "target": 8
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 29,
                "target": 9
            },
            {
                "prerequisite": null,
                "similarity": 0.36506402408916916,
                "source": 29,
                "target": 10
            },
            {
                "prerequisite": null,
                "similarity": 0.07366616374383199,
                "source": 29,
                "target": 11
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 29,
                "target": 12
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 29,
                "target": 13
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 29,
                "target": 14
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 29,
                "target": 15
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 29,
                "target": 16
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 29,
                "target": 17
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 29,
                "target": 18
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 29,
                "target": 19
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 29,
                "target": 20
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 29,
                "target": 21
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 29,
                "target": 22
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 29,
                "target": 23
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 29,
                "target": 24
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 29,
                "target": 25
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 29,
                "target": 26
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 29,
                "target": 27
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 29,
                "target": 28
            },
            {
                "prerequisite": null,
                "similarity": 1.0,
                "source": 29,
                "target": 29
            }
        ],
        "nodes": [
            {
                "count": 0.6807928660466254,
                "group": 1,
                "index": 0,
                "name": "entropy",
                "videos_id": [
                    "AmCV4g7_-QM",
                    "H_S8DaHAAnc"
                ]
            },
            {
                "count": 0.548845744983891,
                "group": 1,
                "index": 1,
                "name": "forest",
                "videos_id": [
                    "D_2LkhMJcfY"
                ]
            },
            {
                "count": 0.4024145679624972,
                "group": 1,
                "index": 2,
                "name": "implies",
                "videos_id": [
                    "Zze7SKuz9QQ"
                ]
            },
            {
                "count": 0.33481110421555427,
                "group": 1,
                "index": 3,
                "name": "subset",
                "videos_id": [
                    "HDvSevhTgDM",
                    "Q4NVG1IHQOU",
                    "AmCV4g7_-QM",
                    "eKD5gxPPeY0",
                    "w4MnOA14pYs",
                    "D_2LkhMJcfY"
                ]
            },
            {
                "count": 0.2203698824556532,
                "group": 1,
                "index": 4,
                "name": "link",
                "videos_id": [
                    "VS4eMbpc43w"
                ]
            },
            {
                "count": 0.2087714675895662,
                "group": 1,
                "index": 5,
                "name": "threshold",
                "videos_id": [
                    "WOOTNBxbi8c"
                ]
            },
            {
                "count": 0.18888847067627418,
                "group": 1,
                "index": 6,
                "name": "group",
                "videos_id": [
                    "LgLMX5nYfN4"
                ]
            },
            {
                "count": 0.18888847067627418,
                "group": 1,
                "index": 7,
                "name": "animal",
                "videos_id": [
                    "LgLMX5nYfN4"
                ]
            },
            {
                "count": 0.18888847067627418,
                "group": 1,
                "index": 8,
                "name": "shape",
                "videos_id": [
                    "LgLMX5nYfN4"
                ]
            },
            {
                "count": 0.15904041823988746,
                "group": 1,
                "index": 9,
                "name": "mean",
                "videos_id": [
                    "DCZ3tsQIoGU",
                    "IX0iGf2wYM0",
                    "eKD5gxPPeY0",
                    "w4MnOA14pYs",
                    "CBBV9TcKSqo",
                    "LDRbO9a6XPU",
                    "NsUqRe-9tb4"
                ]
            },
            {
                "count": 0.14588418558141972,
                "group": 1,
                "index": 10,
                "name": "range",
                "videos_id": [
                    "LgLMX5nYfN4",
                    "JFJIQ0_2ijg"
                ]
            },
            {
                "count": 0.14084967333570947,
                "group": 1,
                "index": 11,
                "name": "scale",
                "videos_id": [
                    "CBBV9TcKSqo",
                    "NsUqRe-9tb4",
                    "VS4eMbpc43w"
                ]
            },
            {
                "count": 0.12540921216648362,
                "group": 1,
                "index": 12,
                "name": "classification",
                "videos_id": [
                    "p17C9q2M00Q",
                    "JFJIQ0_2ijg"
                ]
            },
            {
                "count": 0.1207282914306081,
                "group": 1,
                "index": 13,
                "name": "type",
                "videos_id": [
                    "LgLMX5nYfN4",
                    "tNa99PG8hR8",
                    "JFJIQ0_2ijg"
                ]
            },
            {
                "count": 0.1207282914306081,
                "group": 1,
                "index": 14,
                "name": "work",
                "videos_id": [
                    "LgLMX5nYfN4",
                    "tNa99PG8hR8",
                    "JFJIQ0_2ijg"
                ]
            },
            {
                "count": 0.11346547767443756,
                "group": 1,
                "index": 15,
                "name": "binary tree",
                "videos_id": [
                    "p17C9q2M00Q",
                    "WOOTNBxbi8c"
                ]
            },
            {
                "count": 0.1101849412278266,
                "group": 1,
                "index": 16,
                "name": "flow",
                "videos_id": [
                    "VS4eMbpc43w"
                ]
            },
            {
                "count": 0.1101849412278266,
                "group": 1,
                "index": 17,
                "name": "location",
                "videos_id": [
                    "VS4eMbpc43w"
                ]
            },
            {
                "count": 0.10625871713674917,
                "group": 1,
                "index": 18,
                "name": "node",
                "videos_id": [
                    "DCZ3tsQIoGU",
                    "HDvSevhTgDM",
                    "Q4NVG1IHQOU",
                    "p17C9q2M00Q",
                    "eKD5gxPPeY0",
                    "w4MnOA14pYs",
                    "D_2LkhMJcfY",
                    "LDRbO9a6XPU",
                    "NsUqRe-9tb4",
                    "O__7lAqni7A",
                    "WOOTNBxbi8c"
                ]
            },
            {
                "count": 0.1017091765179938,
                "group": 1,
                "index": 19,
                "name": "uncertainty",
                "videos_id": [
                    "AmCV4g7_-QM"
                ]
            },
            {
                "count": 0.09540138878008003,
                "group": 1,
                "index": 20,
                "name": "leaf",
                "videos_id": [
                    "DCZ3tsQIoGU",
                    "HDvSevhTgDM",
                    "p17C9q2M00Q",
                    "w4MnOA14pYs",
                    "LDRbO9a6XPU",
                    "NsUqRe-9tb4",
                    "O__7lAqni7A",
                    "WOOTNBxbi8c",
                    "JFJIQ0_2ijg"
                ]
            },
            {
                "count": 0.09475780307973117,
                "group": 1,
                "index": 21,
                "name": "depth",
                "videos_id": [
                    "Q4NVG1IHQOU",
                    "w4MnOA14pYs",
                    "CBBV9TcKSqo",
                    "WOOTNBxbi8c"
                ]
            },
            {
                "count": 0.08895768842255335,
                "group": 1,
                "index": 22,
                "name": "functions",
                "videos_id": [
                    "CBBV9TcKSqo",
                    "LDRbO9a6XPU",
                    "WOOTNBxbi8c"
                ]
            },
            {
                "count": 0.08439697625961186,
                "group": 1,
                "index": 23,
                "name": "cluster",
                "videos_id": [
                    "CBBV9TcKSqo"
                ]
            },
            {
                "count": 0.0842739610368673,
                "group": 1,
                "index": 24,
                "name": "list",
                "videos_id": [
                    "eKD5gxPPeY0",
                    "CBBV9TcKSqo",
                    "D_2LkhMJcfY",
                    "LDRbO9a6XPU"
                ]
            },
            {
                "count": 0.0837054832841513,
                "group": 1,
                "index": 25,
                "name": "target",
                "videos_id": [
                    "DCZ3tsQIoGU",
                    "CBBV9TcKSqo",
                    "tNa99PG8hR8",
                    "D_2LkhMJcfY",
                    "O__7lAqni7A",
                    "WOOTNBxbi8c",
                    "VS4eMbpc43w"
                ]
            },
            {
                "count": 0.07555538827050967,
                "group": 1,
                "index": 26,
                "name": "name",
                "videos_id": [
                    "tNa99PG8hR8"
                ]
            },
            {
                "count": 0.07345662748521774,
                "group": 1,
                "index": 27,
                "name": "two-dimensional",
                "videos_id": [
                    "NsUqRe-9tb4"
                ]
            },
            {
                "count": 0.07345662748521774,
                "group": 1,
                "index": 28,
                "name": "base",
                "videos_id": [
                    "H_S8DaHAAnc"
                ]
            },
            {
                "count": 0.073064017839119,
                "group": 1,
                "index": 29,
                "name": "algorithm",
                "videos_id": [
                    "DCZ3tsQIoGU",
                    "HDvSevhTgDM",
                    "Q4NVG1IHQOU",
                    "Zze7SKuz9QQ",
                    "IX0iGf2wYM0",
                    "LgLMX5nYfN4",
                    "eKD5gxPPeY0",
                    "w4MnOA14pYs",
                    "D_2LkhMJcfY",
                    "LDRbO9a6XPU",
                    "NsUqRe-9tb4",
                    "O__7lAqni7A",
                    "JFJIQ0_2ijg",
                    "H_S8DaHAAnc",
                    "VS4eMbpc43w"
                ]
            }
        ]
    },
    "search_info": {
        "NumOfVideos": 21,
        "key": "decision_tree_machine_learning_50",
        "similarity_threshold": 0.6,
        "time_delta": 6.483333333333333,
        "voclist_SelectMethod": 2
    },
    "videos_info": {
        "AmCV4g7_-QM": {
            "caption_exist": "T",
            "channel_id": "UCs7alOMRnxhzfKAJ4JjZ7Wg",
            "channel_title": "Victor Lavrenko",
            "concepts": [
                [
                    "subset",
                    16
                ],
                [
                    "entropy",
                    5
                ],
                [
                    "uncertainty",
                    2
                ],
                [
                    "posterior",
                    1
                ],
                [
                    "tree",
                    1
                ],
                [
                    "graph",
                    1
                ]
            ],
            "description": "Full lecture: http://bit.ly/D-Tree \nWhich attribute do we select at each step of the ID3 algorithm? The attribute that results in the most pure subsets. We can measure purity of a subset as the entropy (degree of uncertainty) about the class within the subset.",
            "dislikeCount": "27",
            "duration": "PT7M8S",
            "likeCount": "429",
            "published_time": "2014-01-19T21:23:27.000Z",
            "tags": [
                "decision",
                "tree",
                "decision tree",
                "ID3",
                "C4.5",
                "attribute",
                "value",
                "classification",
                "prediction",
                "machine learning",
                "machine",
                "learning",
                "applied",
                "entropy",
                "pure subset",
                "killer feature"
            ],
            "thumbnail": "https://i.ytimg.com/vi/AmCV4g7_-QM/hqdefault.jpg",
            "title": "Decision Tree 3: which attribute to split on?",
            "transcript": "  so and for example let's say we have two we're at the start of the tree right we haven't split the data yet and we are look we could split on Outlook or we could split on wind so how would you pick which one to split on well see if we split on output we get with we end up with three sets and one of them is pure and two of them are impure and you have sort of two two three and three two two as the ratio of positives and negatives in them and if we split on wind we don't have any pure sets but we have one set which is three and three so that's totally 5050 whether it's whether John will play or not and for the week you have it's mostly positive but there are two negatives as well so so which one is better how do we know how do we know whether we should pick Outlook or the wind and the basic idea is you want to somehow measure the purity of the split so ideally you want to pick an attribute that has all pure subsets the magic attribute the killer feature that is going to just determine give all the answers for you and of course it doesn't happen but but but it does it does happen for some subsets so how do we take all of this information and turn it into a some sort of a number that we can use for predicting how good the split is so at the core of it what we want to do is we want to measure the purity of the subset and it's not just whether it's totally pure or totally impure right so these are both impure but this one is a lot better than this one right because this one is you you're almost certain that John will play it's six out of eight and this one it's 50/50 so splits that generate subsets like that are good and splits that generate subsets like that are bad so what we want to what we want to do is we want to come up with some kind of a metric that will measure the purity of the subset and one way to measure it is to measure uncertainty so that's that's that's the metric that we're going to use so after you have split the data suppose you're looking at a particular subset how I'm sorry you are whether a random item from that subset is positive or negative so that's related to those numbers the three and three so here you would be totally uncertain completely uncertain because there is a 50/50 chance of it being positive or negative and here you would be totally certain because you know that it must be positive because you've never seen a negative example in this in this branch so we can't use the posterior probability right so we can't use something like that probably that he will play in the subset because it needs to be symmetric and what I need by that is a pure subset which is four yeses and zero knows is just as good as a pure subset which has say six knows and zero yeses right so it can't be the probability of the positive it must be something that is symmetric on the positive side and on the negative side so so what can we do there is a measure like that and it's called entropy it is a way to measure the uncertainty of the class of a class in a subset of examples so entropy is defined like that so we have two numbers P plus and P minus P plus is the probability of positive once you're on the subset and P minus is one minus that is probability of the negative once you're in the subset and not what you're doing is you'll have some expression in terms of P plus and some expression in terms of P minus and you're adding them together so that's what makes it that's what makes it symmetric if you flip the pluses and the minuses it won't change anything so entropy does actually have an interpretation it's a funny interpretation in in this context but the interpretation is in terms of the bits the number of bits that you need to convey information so the question that you're trying to answer is this suppose that you know that item X belongs to a certain subset how many bits do you need to spend to say whether X is positive or negative zooming that it is in the subset so I guess so if the if if the subset is pure then how many bits do you need zero you don't need it right you already know if it's in the pure subset it's not you know you know that it's positive so if the subset is were all positive pure you know that it split if the subset has ten positives and ten negatives how many bits do you need one right because for this item you need to say whether it's positive or negative because if you did anything else you'd be betting on a coin toss but it's either positive or negative and to be certain you need to spend exactly one bit but and for cases where you are less certain than the pure set but not as uncertain as a coin flip entropy will give a number between 0 and 1 so that the interpretation is the number of bits except it's a fractional number of bits so it's kind of strange and fun so you can have like half a bit okay so here's an example for an appeal for an impure set right so you have three positives three negatives so you have 1/2 1/2 and by the way logarithms are based - so so that that ends up - so this is 1/2 this is 1/2 log of 1/2 is minus 1 so you're multiplying it by that - so you get 1/2 so oh sorry yes minus 1 so it's minus 1/2 times minus 1 that's 1/2 you get the same number here 1/2 so 1/2 plus 1/2 you get 1 bit right so if the set is totally pure so for positives and 0 negatives what you get is 1 1 0 0 so 0 log 0 is 0 by l'hopital's rule and log 0 log 1 is 0 so 1 times 0 is 0 so you need 0 bits for the pure set okay in general though this is what this is what entropy looks like so if you are if you are totally certain that it's positive if the P plus is 1 then you need 0 bits if you totally totally certain that it's negative you need 0 bits if you're totally uncertain half and half you need one bit exactly and if you are somewhere on the side say you're two-thirds certain that it's one another you need a little bit less than one bit so that's what I was talking about when I said symmetric right this graph is symmetric it must be symmetric about a point 1/2 sorry about 0.5",
            "videoid": "AmCV4g7_-QM",
            "viewCount": "142524"
        },
        "CBBV9TcKSqo": {
            "caption_exist": "T",
            "channel_id": "UC40lkNGGObJc9hEGmFTQbrQ",
            "channel_title": "Sundog Education",
            "concepts": [
                [
                    "tree",
                    23
                ],
                [
                    "decision tree",
                    22
                ],
                [
                    "list",
                    11
                ],
                [
                    "cluster",
                    6
                ],
                [
                    "degree",
                    5
                ],
                [
                    "array",
                    4
                ],
                [
                    "scale",
                    3
                ],
                [
                    "prior",
                    3
                ],
                [
                    "string",
                    3
                ],
                [
                    "target",
                    2
                ],
                [
                    "mean",
                    2
                ],
                [
                    "depth",
                    2
                ],
                [
                    "walk",
                    2
                ],
                [
                    "functions",
                    2
                ],
                [
                    "binary function",
                    1
                ],
                [
                    "root",
                    1
                ],
                [
                    "dictionary",
                    1
                ],
                [
                    "head",
                    1
                ]
            ],
            "description": "Full course: https://www.udemy.com/data-science-and-machine-learning-with-python-hands-on/?couponCode=DATATUBE\n\nWe'll take the same problem for our earlier Decision Tree lecture - predicting hiring decisions for job candidates - but implement it using Spark and MLLib!",
            "dislikeCount": "1",
            "duration": "PT16M1S",
            "likeCount": "10",
            "published_time": "2017-09-20T14:30:00.000Z",
            "tags": [
                "data science",
                "spark",
                "machine learning",
                "python",
                "decision trees"
            ],
            "thumbnail": "https://i.ytimg.com/vi/CBBV9TcKSqo/hqdefault.jpg",
            "title": "Decision Trees in Spark",
            "transcript": "  so let's make this real let's look at some actual spark code to make a decision tree using ml Lib that I can actually scale up to a cluster if you wanted to it's actually pretty simple let's take a look all right let's actually build some decision trees using spark and the ML Lib library this is very cool stuff I'm in Florida and we're currently getting a Florida thunderstorm so if you can hear any rain in the background I'm not sure if the mics picking that up but apologies if so just think of it as soothing relaxing background noise so wherever you put the course materials for this course I want you to go to that folder now and make sure you're completely closed out of canopy or whatever environment you're using for Python development cuz I want to make sure you're starting it from this directory okay and find the SPARC decision tree script and double click that and up should come up canopy or whatever you're using to edit your Python files and here we have it now up until this point we've been using ipython notebooks for the course but you can't really use those very well with SPARC with SPARC scripts you need to actually submit them to the SPARC infrastructure and run them in a very special way and we'll see how that works shortly so we are just looking at a raw Python script file here without any of the usual embellishment of the ipython notebook stuff so let's walk through what's going on in this script go through it a little bit slowly here because this is your first SPARC script that you've seen in this course so we're going to import from PI SPARC ml lib the bits that we need from the machine learning library what library for SPARC we need the labeled point class which is a data type required by the decision tree class and the decision tree class itself imported from ml Lib top tree and now pretty much every spark script you see is going to include this line where we import spark conf and spark context and this is needed to create the spark context object that is kind of the root of everything you do in spark and finally we're going to import the array library from numpy and yes you can still use numpy and scikit-learn and whatever you want within spark scripts you just have to make sure first of all that these libraries are installed on every machine that you intend to run it on so if you're running on a cluster you need to make sure that those Python libraries are already in place somehow and you also need to understand that spark will not magically make the scikit-learn methods for example magically scalable you know you can still call these functions in the context of a given map function or something like that but it's only going to run on that one machine within that one process okay so don't lean on that stuff you have Eveleigh but for simple things like managing arrays totally an okay thing to do so we'll start by setting up our spark context and we start by giving it a spark conf a configuration so this configuration object says I'm going to set the masternode to local and this means I'm just running on my own local desktop I'm not actually running on a cluster at all and I'm just going to run in one process even I'm also going to give it an an app name of spark decision tree and you can call that whatever you want Fred Bob Tim whatever floats your boat it just is what this job will appear as if you were to look at it in the spark console later on and then we will create our spark context object using that configuration and that gives us an SC object we can use for creating rdd's now let's skip down a bit past these functions that I have we'll get back to those later and go to the first bit of Python code that actually gets executed in this script so the first thing we're going to do is load up this past highest CSV file and that's the same file we used in the decision tree exercise that we did earlier in this course so if you remember right we have a bunch of attributes of job candidates and we have a field of whether or not we hired those people or not and what we're trying to do is fill up a decision tree that will predict what we hire or not hire a person and given those attributes so again make sure you change the path to that file to wherever you actually installed it otherwise it won't work now let's take a quick peek at that file now you can see that Excel actually imported this into a table but it's comma separated values if you would look at the raw text the first line is the actual headings of each column so what we have here are the number of years of prior experience is the candidate currently employed or not number of previous employers the level of education whether they went to a top-tier school whether they had an internship while they were in school and finally the target that we're trying to predict on whether or not they got a job offer in the end of the day so we need to read that information into an RDD so we can do something with it so let's go back to our script here so the first thing we need to do is read that in and we're going to throw away that first row because that's our header information remember so here's a little trick for doing that we start off by importing every single line from that file into a raw data RDD and I could call that anything I want again this is an arbitrary name I made for it but we're calling SC text file spark context has a text file function that will take a text file and create a new RTD where each entry each line of the RTD consists of one line of input now I'm going to extract the first line the first row from that RTD by using the first function so now the header RTD will contain one entry that is just that row of column headers and now look what's going on here I'm using filter on my original data that contains all of the information in that CSV file and I'm defining a filter function that will only let lines through if that line is not equal to the contents of that initial header row so what I've done here is I've taken my raw CSV file and I've stripped out the first line by only allowing lines that do not equal that first line to survive and I'm returning that back to the raw data RTD variable again so I'm taking raw data filtering out that first line and creating a new raw data that only contains the data itself with me so far okay it's not that complicated now we're going to use a map function and what we need to do next is start to make more structure out of this information so right now every row of my RDD is just a line of text it is common to limited text but it's still just a giant line of text and I want to take that comma separated value list and actually split it up into individual fields so at the end of the day I want each RDD to be transformed from a line of text that has a bunch of information separated by commas into a Python list that has actual individual fields for each column of information that I have so that's what this lambda function does it calls the built-in Python function split which will take a row of input and split it on comma characters and divide that into a list of every field delimited by commas okay so the output of this map function where I passed in a lambda function that just splits every line into fields based on commas is a new RDD called CSV data and at this point CSV data CSV data is an RDD that contains on every row a list where every element is a column from my source data now we're getting close now it turns out that in order to use a decision tree with ML Lib a couple of things need to be true first of all the input has to be in the form of labelled point datatypes and it all has to be numeric in nature okay so before so the next thing we're gonna do is transform all of our raw data into data that can actually be consumed by ML Lib and that's what this create labeled points function does so we're going to call a map on csv data and we are going to pass it the create label points function which will transform every input row into something even closer to what we want at the end of the day so let's look at what create label points does it takes in a list of fields and just to remind you again what that looks like let's pull up that CSV file again so at this point every RTD entry has a field it's a list of Python lists where the first element is the years of experience second element is employed so on and so forth the problems here are that we want to convert those lists to labelled points and we want to convert everything to numerical data so all these yes and noes need to be converted to ones and euros these levels of experience need to be converted from names of degrees to some numeric ordinal value so maybe we'll assign the value 0 to no education one can mean BS to kameen m/s and three communed PhD for example again all these yes/no values need to be converted to zeros and ones because at the end of the day everything going into our decision tree needs to be numeric and that's what create label points does so it takes in this list of string fields and converts it into label points where the label is the target value was this person hired or not 0 or 1 followed by an array remember that's why we imported NP dot array that consists of all the other fields that we care about so this is how you create a label point that the decision tree ml Lib class can consume so you see here that we're converting years of experience from a string to an integer value and for all the yes/no fields we're calling this binary function that I defined up here and all it does is convert the character yes to one otherwise it returns zero so wise will become one's ends will become zeros similarly I have a map education function i defined that converts different types of degrees to an ordinal numeric value okay so at this point after mapping our RDD using that create label points function we now have a CS training data RTD and this is exactly what ml Lib wants for constructing a decision tree let's create a little test candidate we can use so we can use our model to actually predict whether someone new has been hired would be hired or not so what we're going to do is create a test candidate here that consists of an array of these values for each fields so again we need to map these back to their original column representations so that 10 1 3 1 0 0 means 10 years of prior experience currently employed three previous employers a BS degree did not go to a top-tier school and did not do an internship and we will we could actually create an entire RTD with more than one value if we wanted to but we'll just do one for now so we use parallel eyes to convert that list into an RDD all right now for the magic we are going to call a decision tree clean train classifier and this is what will actually build our decision tree itself we passed it in our training data which is just an RDD full of labeled point arrays numb classes - because we have basically a yes or no prediction that we're trying to make would this person be hired or not the next parameter is called categorical features info and this is a Python dictionary that map's fields to the number of categories in each field so if you have a continuous range available to a given field like the number of years of experience you wouldn't specify that at all in here but for fields that are categorical in nature such as what degree do they have if any you know for example that would say field ID 3 which maps to year 2 the degree attained has four different possibilities no education vsms and ph.d and for all the yes/no fields were mapping those to two possible categories yes or no or zero and one is what we converted those to we're going to use the Gini impurity metric as we measure the entropy as we go down our decision tree a max depth of five which is just an upper bound on how far we're gonna go that can be larger if you wish and max spins is just a way to trade off computational expense if you can so it just needs to at least be the maximum number of categorical feet categories you have in each feature and now remember nothing really happens until we call an action so we're going to actually use this model to make a prediction for our test candidate so we use our decision tree model which contains a decision tree that was trained on our tester training data and we're telling you to make a prediction on our test data and we'll get back a list of predictions that we can then iterate through so predict returns a plain old Python object it is the action that I can collect let me let me rephrase that a little bit collect will return a Python object on our predictions and then we can iterate through every item in that list and print the result of the prediction we can also print out the decision tree itself by using the to debug string and that will actually print out a little representation of the decision tree that it created internally that you can fall through follow through in your own head so that's kind of cool too alright feel free to take some time stare at this script a little bit more and digest what's going on but if you're ready let's move on and actually run this beast so to do so you can't just run it okay we're gonna go to the Tools menu and open up a canap\u00e9 command prompt and this just opens up a Windows command prompt with all the necessary environment variables in place for running python scripts in canopy and you can see my working directory here is the directory that I installed all of my course materials into so make sure that's the case here all I need to do is call SPARC - submit so this is a script that lets you run SPARC scripts from Python and then the name of the script SPARC decision tree py that's all I have to do in return and off it will go and again if I were doing this on a cluster and I created my SPARC conf accordingly this would actually get distributed to the entire cluster but for now we're just going to run it on my computer here so you can see in the test person that we put in there we have a prediction that this person would be hired and I've also printed out the decision tree itself so it's kind of cool we can walk through this and see what it means so we actually ended up with a depth of four with nine different nodes and again if we remind ourselves what these different fields correlate to the way to read this is if feature one in zero so that means if the employed is know if this person is not currently employed feature five one two three four five zero one two okay so this person is not currently employed did not do an internship has no prior years of experience and has a bachelor's degree we would not hire this person but if that person had an advanced degree we would just based on the data that we had that we trained it on so you can work out what these different feature IDs mean back to your original source data remember you always start counting at zero and interpret that accordingly note that all the features are expressed as in you know this list of possible categories that it saw whereas continuous data is expressed numerically as less than or greater than relationships so there you have it a working decision tree created using spark and ml lib pretty cool stuff and what's really cool is that this could scale up to a massive data set potentially and it's pretty easy to use still so there you have it and there you have it an actual decision tree built using spark in ml lib that actually works and actually makes sense pretty awesome stuff so you can see it's pretty easy to do and you can scale that up to as large of a data set as you can imagine if you have a large enough cluster so there you have it",
            "videoid": "CBBV9TcKSqo",
            "viewCount": "792"
        },
        "DCZ3tsQIoGU": {
            "caption_exist": "T",
            "channel_id": "UCFJPdVHPZOYhSyxmX_C_Pew",
            "channel_title": "Augmented Startups",
            "concepts": [
                [
                    "tree",
                    18
                ],
                [
                    "decision tree",
                    12
                ],
                [
                    "node",
                    8
                ],
                [
                    "algorithm",
                    6
                ],
                [
                    "terminal node",
                    2
                ],
                [
                    "mean",
                    2
                ],
                [
                    "mode",
                    2
                ],
                [
                    "root",
                    1
                ],
                [
                    "internal node",
                    1
                ],
                [
                    "height",
                    1
                ],
                [
                    "target",
                    1
                ],
                [
                    "reduction",
                    1
                ],
                [
                    "graph",
                    1
                ],
                [
                    "leaf",
                    1
                ],
                [
                    "optimal",
                    1
                ]
            ],
            "description": "Decision Tree (CART) - Machine Learning Fun and Easy\n\nhttps://www.udemy.com/machine-learning-fun-and-easy-using-python-and-keras/?couponCode=YOUTUBE_ML\n\nDecision tree is a type of supervised learning algorithm (having a pre-defined target variable) that is mostly used in classification problems.  A tree has many analogies in real life, and turns out that it has influenced a wide area of machine learning, covering both classification and regression (CART).\n\nSo a decision tree is a flow-chart-like structure, where each internal node denotes a test on an attribute, each branch represents the outcome of a test, and each leaf (or terminal) node holds a class label. The topmost node in a tree is the root node. \n\nTo learn more on Augmented Reality, IoT, Machine Learning FPGAs, Arduinos, PCB Design and Image Processing then Check out \nhttp://www.arduinostartups.com/\nPlease like and Subscribe for more videos :)\n\n--------------------------------------------------\nSupport us on Patreon\nhttp://bit.ly/PatreonArduinoStartups\n--------------------------------------------------",
            "dislikeCount": "40",
            "duration": "PT8M46S",
            "likeCount": "668",
            "published_time": "2017-06-26T18:43:32.000Z",
            "tags": [
                "machine learning",
                "fun and easy machine learning",
                "scikit learn regression",
                "machine learning python",
                "Dependent Variable",
                "Decision Tree (CART) - Machine Learning Fun and Easy",
                "CART",
                "regression tree",
                "random forrest",
                "random forrests",
                "data mining",
                "ID3",
                "deep learning",
                "decision tree algorithm",
                "decision tree classification",
                "leaf node",
                "terminal node",
                "python decision tree",
                "decision tree regression"
            ],
            "thumbnail": "https://i.ytimg.com/vi/DCZ3tsQIoGU/hqdefault.jpg",
            "title": "Decision Tree (CART) - Machine Learning Fun and Easy",
            "transcript": "  and welcome to another fun and easy machine learning tutorial on decision trees a decision tree is a type of supervised learning algorithm that is mostly used in classification problems a3 has many analogies in life and turns out it is influenced in wide area of machine learning covering both classification and regression trees otherwise known as caught please join our notification brigade by subscribing and clicking that Dow icon so a decision tree is a flowchart like structure where each internal node denotes a test on an attribute each branch represents an outcome of a test and each leaf or terminal node holds a class label the topmost node in a tree is the roast node in decision analysis a decision tree can be used to visually and explicitly represent decisions and decision-making as the name goes it uses a tree like model of decisions so the advantages of of god it is simple to understand interpret and visualize decision trees implicitly perform variable screening or feature selection it can handle both numerical as well as categorical data it can also handle multi output problems decision trees requires relatively little effort from the user for data preparation and nonlinear relationships between parameters do not affect the clip performance the disadvantages of cost however is that decision tree learners can create over complex trees that do not generalize the data well this is also known as overfitting decision trees can become unstable because small variations in the data might result in a completely different reading generated this is called variance which needs to be lowered by methods of bagging and posting greedy algorithms cannot guarantee to return the globally optimal decision tree this can be mitigated by training multiple trees where features and samples are randomly sampled with replacement decision tree learners also create bias trees if some classes dominate it is therefore recommended to balance data set priority setting what the decision tree if you look at some applications of the decision tree we can predict whether a customer will pay his renewal premium was an insurance company so you can predict yes if you all or no if you want you need to predict that dem excel file statistics so if male or female as well as age what are the chances of survival he needed to determine if a person is male or female based on the height and weight also he needed to determine a price of a home based on how many rooms as well as the floor size a decision tree is drawn as a down whether its fruit at the top so in image let's look at the primary differences and similarities between regression trees are used when the dependent variable is continuous classification trees I use when the dependent variable is categorical in the case of regression trees the value obtained by terminal nodes in the training data is the mean or average response of the observation falling in that region thus if an unseen data observation falls in that region will make its position with a mean value the user of classification tree the value or class obtained by the terminal node in the training data is the mode of is an unscented observation falls in that region will make its prediction with a mode value so the splitting process is continued until a user-defined stopping criteria is reached for example we can tell the algorithm to stop once the number of observations per node becomes less than 50 so in both cases the student process results in fully grown trees until the stopping criteria is reached but fully grown trees is likely to over data leading to poor accuracy on data and this brings pruning pruning is one of the techniques used to tackle overfitting we'll learn more about it in in future lectures so how can an algorithm be represented as a tree for this let's consider a basic example that used the Titanic data set for predicting whether a passenger or survived or not this model over here uses three features from the data set namely six age and number of spouses or children along we can abbreviate this to si be SP in this case where the passenger diet or survived is represented as red and green text respectively although a real deal set will have a lot more features and this will just be a branch in a much bigger tree but you can't ignore the simplicity of the algorithm so what's actually going on in the background going a tree involves deciding on which features to choose and what conditions to use for splitting along with knowing when to stop as it regenerates arbitrarily you need to trim it down for it to look beautiful so let's started calming techniques use for splitting so how does it read said with split so the decision for making strategic splits heavily affects a tree's accuracy the decision criteria is different for classification and regression trees decision trees use multiple algorithms they decide to split a node in two or more sub nodes the creation of sub nodes increases homogeneity of resultant sub nodes in other words we can group our data in regions based on data that have similar traits decision tree splits the nodes on all available variables and then selects the split which results in the most homogeneous subnodes most ethical ignore example shown in this lecture the algorithm selection is also based on the type of target variables so let's look at the four most commonly used algorithms in decision tree one beauty index to chi-squared three information gain for reduction in variance so we will not go into detail on these algorithms as some involves quite a lot of math and most of the hard work is done within scikit-learn libraries let's gain an intuition of our splitting the data would work if we tweet manually so via we have arbitrarily generated data we have X 1 and X 2 which are our independent variables if you have to look at this data we can split it into five regions so we can draw a line here at X 1 equals 20 as well as X 2 equals 50 and then another one over here at x 1 equals scream 5 and then a last blood over here between 5 x 2 equals 30 so we have regions r1 r2 r3 r4 and r5 and we do this empirically the elements I mentioned earlier will do this for you now remember you can split it a bit further into more regions to say for example we can split r4 over here and that will result in more sub nodes in our 3 but for now let's just have 5 regions so we start off over here at our root node 3 also solves is x1 less than printing so we go either yes or no so if yes is X cubed s6 of T so if you look at our graph over there and then we separate that into r1 so if you sv f r1 if no we have asked you then you go to our other branch and we ask is x1 less than 25 so we look at X less than 25 if yes then it's r3 if no then we ask ourselves is x2 less than 50 and if yes you got our 5 and if no we got our 4 so as you can see that is really simple so this is all the basics to get you on par with decision tree learning decision trees are also very useful when you use with other advanced machine learning algorithms like random forests and boosting which we shall cover in future lectures a popular library for implementing the algorithm scikit-learn it is a wonderful API that can get your model up and running in just a few lines of code in Python so thank you for watching please don't forget to smash that like button and click the doll icon to become a part of our notification brigade and also support us on patreon see you in the next lecture",
            "videoid": "DCZ3tsQIoGU",
            "viewCount": "50300"
        },
        "D_2LkhMJcfY": {
            "caption_exist": "T",
            "channel_id": "UCFJPdVHPZOYhSyxmX_C_Pew",
            "channel_title": "Augmented Startups",
            "concepts": [
                [
                    "forest",
                    22
                ],
                [
                    "algorithm",
                    12
                ],
                [
                    "tree",
                    10
                ],
                [
                    "decision tree",
                    3
                ],
                [
                    "node",
                    2
                ],
                [
                    "control",
                    1
                ],
                [
                    "target",
                    1
                ],
                [
                    "subset",
                    1
                ],
                [
                    "list",
                    1
                ]
            ],
            "description": "Random Forest - Fun and Easy Machine Learning\n\nhttps://www.udemy.com/machine-learning-fun-and-easy-using-python-and-keras/?couponCode=YOUTUBE_ML\n\nHey Guys, and welcome to another Fun and Easy Machine Learning Algorithm on Random Forests.\n\nRandom forest algorithm is a one of the most popular and most powerful supervised Machine Learning algorithm in Machine Learning that is capable of performing both regression and classification tasks. As the name suggest, this algorithm creates the forest with a number of decision trees.\nIn general, the more trees in the forest the more robust the prediction. In the same way in the random forest classifier, the higher the number of trees in the forest gives the high accuracy results.\n\n\nTo model multiple decision trees to create the forest you are not going to use the same method of constructing the decision with information gain or gini index approach, amongst other algorithms. If you are not aware of the concepts of decision tree classifier, Please check out my lecture here on Decision Tree CART for Machine learning. You will need to know how the decision tree classifier works before you can learn the working nature of the random forest algorithm. \n\nTo learn more on Augmented Reality, IoT, Machine Learning FPGAs, Arduinos, PCB Design and Image Processing then Check out \nhttp://www.arduinostartups.com/\nPlease like and Subscribe for more videos :)",
            "dislikeCount": "101",
            "duration": "PT7M38S",
            "likeCount": "1058",
            "published_time": "2017-07-12T13:28:50.000Z",
            "tags": [
                "random forest",
                "machine learning",
                "fun and easy",
                "fun and easy machine learning",
                "machine learning tutorial",
                "random forest tutorial",
                "random forest lecture",
                "fun and easy random forest",
                "linear regression",
                "decision tree",
                "gini index",
                "information gain",
                "arduino startups"
            ],
            "thumbnail": "https://i.ytimg.com/vi/D_2LkhMJcfY/hqdefault.jpg",
            "title": "Random Forest - Fun and Easy Machine Learning",
            "transcript": "  hey guys and welcome to yet another fun and easy machine learning algorithm unfriend enforce the random forest algorithm is one of the most popular and most powerful supervised machine learning algorithm that is capable of performing both regression and classification tasks as the name suggests this algorithm creates the forest with a number of decision trees in general the more trees in the forest the more robust the prediction and thus higher accuracy to model multiple decision trees to create the forest you are going to use the same method of constructing the decision what the information gain or Gini index approach amongst other algorithms if you're not aware of the concepts of decision tree classifier please check out my other lecture on intuition tree God for machine learning you'll need to know how the decision tree classifier works before you can learn the working nature of the random forest algorithm so how does it work in random forests we grow multiple trees as opposed to a single tree in court model to classify a new object based on attributes each tree gives a classification and we save the tree votes for that class the forest choose the classification having the most votes over all the other trees in the forest and in the case of regression takes the average of the outputs by different trees so let's look at the advantages of the random forest so the same random forest algorithm or random forest classifier can be used for both classification and regression tasks then enforce classifier will handle the missing values and maintain accuracy when a large proportion of the data are missing when we have more trees in the forest random classifies won't over fit the model it has the power to handle large data sets with higher dimensionality if you look at the disadvantages of rainforests however it surely does a good job at classification but it's not as good as for regression problems as it does not give precise continuous nature predictions in the case of regression it doesn't predict beyond the range train data and they may over fit data sets that are particularly noisy random phones can feel like a black box approach for statistical models you have very little control of what the model does you can try at best try different parameters and random seeds so what are the applications our friend Forrest so let's check a few of them down below so we can use them in the banking sector so these are for finding loyal customers and finding the fraud customers it can be using medicine where we identify the correct combination of components to validate listen learn enforce algorithms also help for identifying disease by analyzing the patient's medical records in the stock market random forest algorithm is used to identify the stock behavior as well as the expected loss or profit by purchasing a particular stock in e-commerce the rain forest is used in a small segment of the recommendation engine for identifying the likelihood of a customer liking the recommended products and this is based on some Lacan's customers in computer vision the random forest is used for image classification Microsoft have used random forests for body parts classification for Xbox Kinect and other applications involves lip-reading as well as for its classification let's take a look at the random forest serial code and how it works so it works in the following manner where each tree is planted and grown as follows so assume announced cases in the training set is in then the sample of these end cases is taken at random but what replacement the sample will be the training set for growing the tree if they are M input variables or features a number of M smaller than M is specified such that each node M variables are selected at random out of DM the best list on these M input variables is used to split the node the value of M is held constant while we grow the forest each tree is grown to the largest extent possible and is no pruning and then we predict data by aggregating that protections of the entries which means majority vote for classification and average for regression so to perform the predictions using the Train random forest algorithm we need to pass the test features through the rules of each randomly created trees suppose let's say we formed a thousand random decision trees deform the random forest say we're taking if an image contains a hand each random forest will predict a different outcome or class for the same test feature it small subset of the forest look at a random set of features for example a finger suppose 100 random decision trees predict some three unique targets such as a finger thumb or maybe the meal then the votes of finger is tell it out of hundreds and M decisions and likewise for the other two targets if finger is getting highest votes then the final random forest returns the finger as its predicted target this concept of voting is known as majority voting to select elections the same applies to the rest of the fingers of the hand if the algorithm predicts the rest of the fingers to be fingers then the high-level decision tree can vote that the image is a hand and this is why random forests are known as ensemble machine learning algorithm ensembles are a divide-and-conquer approach used to improve performance the main principle behind ensemble methods is that a group of weak learners can come together to form a strong learner each classifier is individually a weak learner while the classifiers taking together are a strong learner and thus ensemble methods reduce the variance and improve performance before we end the lecture let's take a look at some terms and definitions that you might come across such as guiding and boosting bootstrap aggregating is also known as baggy which which is a machine learning ensemble meta algorithm designed to improve the stability and accuracy of the machine learning algorithms used in physical classification and progression it also reduces variance and helps to avoid overfitting boosting is a machine learning ensemble after algorithm for primarily reducing bias and also fails in supervised learning and a family of machine learning algorithms which also convert weak learners into strong ones algorithms that achieve hypothesis boosting quickly becomes simply known as boosting so that is it thank you for watching please don't forget to like scribe and share click the bell icon to get notified of more fun and easy machine learning algorithms also animating these videos takes a lot of time and effort so please don't forget to support us on patreon so in the following lecture we'll learn how to implement a simple random forest in Python see in next lecture thank you for",
            "videoid": "D_2LkhMJcfY",
            "viewCount": "80405"
        },
        "HDvSevhTgDM": {
            "caption_exist": "T",
            "channel_id": "UCoSiBUY5b9ixwuKfS99_rSw",
            "channel_title": "Cognitive Class",
            "concepts": [
                [
                    "node",
                    14
                ],
                [
                    "tree",
                    10
                ],
                [
                    "decision tree",
                    5
                ],
                [
                    "root",
                    2
                ],
                [
                    "algorithm",
                    2
                ],
                [
                    "terminal node",
                    1
                ],
                [
                    "leaf",
                    1
                ],
                [
                    "best case",
                    1
                ],
                [
                    "subset",
                    1
                ]
            ],
            "description": "Enroll in the course for free at: https://bigdatauniversity.com/courses/machine-learning-with-python/\n\nMachine Learning can be an incredibly beneficial tool to uncover hidden insights and predict future trends. \n\nThis free Machine Learning with Python course will give you all the tools you need to get started with supervised and unsupervised learning.\n\nThis Machine Learning with Python course dives into the basics of machine learning using an approachable, and well-known, programming language. You'll learn about Supervised vs Unsupervised Learning, look into how Statistical Modeling relates to Machine Learning, and do a comparison of each.\n\nLook at real-life examples of Machine learning and how it affects society in ways you may not have guessed!\n\nExplore many algorithms and models:\n\nPopular algorithms: Classification, Regression, Clustering, and Dimensional Reduction.\n\nPopular models: Train/Test Split, Root Mean Squared Error, and Random Forests.\n\nGet ready to do more learning than your machine!\n\nConnect with Big Data University:\nhttps://www.facebook.com/bigdatauniversity\nhttps://twitter.com/bigdatau\nhttps://www.linkedin.com/groups/4060416/profile\n\nABOUT THIS COURSE\n\u2022This course is free.\n\u2022It is self-paced.\n\u2022It can be taken at any time.\n\u2022It can be audited as many times as you wish.\n\nhttps://bigdatauniversity.com/courses/machine-learning-with-python/",
            "dislikeCount": "0",
            "duration": "PT6M48S",
            "likeCount": "22",
            "published_time": "2017-03-23T04:56:02.000Z",
            "tags": [
                "machine learning python",
                "machine learning",
                "python",
                "machine learning with python",
                "machine learning python course",
                "machine learning with python course",
                "machine learning course",
                "python course",
                "machine learning python free course",
                "free course",
                "course",
                "python machine learning",
                "machine learning tutorial",
                "machine learning python tutorial",
                "scikit-learn",
                "machine learning (software genre)",
                "data science",
                "data mining",
                "data analysis",
                "bdu",
                "big data",
                "big data university",
                "Supervised Learning"
            ],
            "thumbnail": "https://i.ytimg.com/vi/HDvSevhTgDM/hqdefault.jpg",
            "title": "Machine Learning - Supervised Learning Decision Trees",
            "transcript": "  hello in this video we'll be covering decision trees the first algorithm we'll cover is the decision tree algorithm what exactly is a decision tree how do they help us classify how can I grow my own decision tree these may be some questions you have in mind from hearing the term decision tree hopefully you'll soon be able to answer these questions and a lot more by going through this lesson decision trees are built by splitting the training set into distinct nodes where one node contains all of or most of one category of the data these categories can be called subsets with the data set divided inputed out-of-sample data will be classified more easily when it falls into a node that is strictly one subset of the data if so there's a higher probability that the data point is the same classification as the node it fell under so if we look at the diagram here we can see that it's a weather classifier so to make this more interesting let's say that we want to go out for a run but the decision to go out will be influenced by the weather forecast we start with the outlook which can be sunny overcast or raining our preference is to run on an overcast day so if the forecast says is going to be overcast then we'll definitely go out for the run on the other hand if it's sunny or raining we'll need more details to help us determine if we want to go out running the additional forecast variables can be things such as humidity wind and rain for example if the forecast is calling for high humidity then we're not going on our run but if it's normal then we are similarly with the wind on a rainy day if it's a strong wind then we're definitely not going but if there's only going to be a slight breeze than we are next let's quickly review some terminology that can apply to decision trees if some of these definitions don't make sense right now don't worry as will be going through some examples that will clarify this first we have a node in a node we have a data set that gets tested for a certain attribute the goal of the node is to split the data set on an attribute next is a leaf node which is the terminal node in the tree that predicts the outcome there is also a root node which appears at the top of the tree and contains the entire data set for that tree next we have the term entropy entropy is the amount of information disorder or the amount of randomness in the data if we're talking about a decision tree we can say for example that the entropy in the node depends on how much random data is in that node next we have information gained this is the information collected that can increase the level of certainty in a particular prediction we can think of information gain and entropy as opposites as entropy or the amount of randomness decreases the information gained or amount of certainty increases and vice versa there are also mathematical formulas that you can use to calculate the entropy and information gain in each node to ensure you've made the best however examining those formulas is outside the scope of this lesson let's take a more in-depth look at entropy and information gain here we have our data set at the root node consisting of four colors red blue green and yellow there are four dots for each color totaling sixteen dots will be using histograms to look at the chance that an out-of-sample data point would be a certain color so we start off having an equal chance of 25% for all colors let's say we make a binary split on the node note that in these examples the attribute used in the split will be ignored for simplification from this split all of the red dots go to the left side and the rest of the dots go to the right side now let's take a look at what the histogram looks like for this split a split that looks like this is ideal this would produce the most information gained and greatly decrease the entropy let's say we just stopped here and we wanted to put an out-of-sample point into our tree we'll want to decide what color this point is let's say that this point fulfills the condition be on the left-hand side we can most likely say that this point should be read now back to the tree the split we saw here is the best case scenario most likely though we'd see a split that would look like this we can see that the split isn't perfect but the right split has the highest chance of being blue but it's not a hundred percent like the first split with red let's also look at the histogram for this split and now let's throw in another out-of-sample point let's say that the point has the attributes to move to the right side twice what do you think this point will end up being well it has a high possibility of being blue remember though that the point could also be green since there was an equivalent number of green in the split it could even be yellow but the chance of that would be a lot lower since there'd be a higher chance of the point going to the left nonetheless having more splits to increase the information gained would present a more clear conclusion to what the point should be in the second split there is less information gained than the first split we can see that there is a lot more entropy in the data sets here than here since there is more randomness in the data sets all right so we just looked at an example of information gain that demonstrated good gain as well as a more realistic example of it that still presented a decent amount of gain this time we'll look at a quick example that demonstrates bad information gain we'll still be using the same data set as before which consists of four data points or four colors as well as the histogram take a couple of seconds to look at the split now how do you think this split differs from the previous example we can see that a split like this provides zero information gained insofar as they're still the same probability of a point being red blue green or yellow after the split then was the case before the split let's look at the histogram for this split as you can see there's no information gained therefore the entropy in the splits remains the same thus a split like this is pointless thanks for watching",
            "videoid": "HDvSevhTgDM",
            "viewCount": "3369"
        },
        "H_S8DaHAAnc": {
            "caption_exist": "T",
            "channel_id": "UCxP77kNgVfiiG6CXZ5WMuAQ",
            "channel_title": "Minsuk Heo \ud5c8\ubbfc\uc11d",
            "concepts": [
                [
                    "entropy",
                    12
                ],
                [
                    "algorithm",
                    2
                ],
                [
                    "decision tree",
                    1
                ],
                [
                    "base",
                    1
                ],
                [
                    "edge",
                    1
                ],
                [
                    "tree",
                    1
                ]
            ],
            "description": "Describe ID3 algorithm with mathematical calculation.\nThe tutorial will cover Shannon Entropy and Information Gain.\nSubtitle (English) is also available, please click 'CC' button for subtitle.",
            "dislikeCount": "4",
            "duration": "PT3M8S",
            "likeCount": "30",
            "published_time": "2016-11-29T07:14:26.000Z",
            "tags": [
                "ID3",
                "ID3 algorithm",
                "mathematical calculation",
                "Shannon Entropy",
                "Information Gain",
                "Entropy",
                "Machine Learning",
                "Decision Tree"
            ],
            "thumbnail": "https://i.ytimg.com/vi/H_S8DaHAAnc/hqdefault.jpg",
            "title": "[Machine Learning] Decision Tree - ID3 algorithm (entropy + Information Gain)",
            "transcript": "  hi when we build the decision tree we need to select one tribute and that split the data there are some ways to select one attribute and split the data one our popular algorithm is auditory algorithm and the idea I didn't use in trophy and information gained consume what is the entropy so the entropy is you like this when we have the room is very messy we can say it's high entropy and when we have very organized room like the right one it is low entropy so as you remember we hit the example have the eight pictures and I find the winter family photo from here so before you select the first all tribute here we need to know first the basis Taylor's entropy so we have eight pictures and just have one winter family photo here so 2/3 eighth and one photo winter family photo and seven photos are nothing with the family photo so according to this formula we can calculate the entropy we can denote entropy 1 plus and Sigma minus because we have just one winter family photo and 7 which is not the winter family photo so here the denominator 8 is total picture count and the one means one winter family photo and the 7 stands for the other pictures so after we calculate this one the basis terrace entropy is zero point 5 4 3 and the next concept is information gain and the information gained use the entropy so here you can see the example we already know the entropy of the base terrorists and we are going to do the minus e here and trophy of the when we select the cartoon as your first attribute so the zero point 5 4 3 we already know this value from the the previous stage I'm going to do - e here so total a pictures we just have for cartoon pictures here multiplied by the entropy of the cultural pictures so the carton pictures has an oriental picture and just for pictures of the carton pictures are not doing the family photo here and plus something others a pictures only thing remaining is full and that the remaining is like this picture and this picture and this picture and that this picture so we have a one payment picture one family winter picture so we say one policy here and the others are not giving to a family picture so after we calculate this one the lizard's see is 0.138 so same thing to the here when we select winter as a first so tribute the lizard is 0.093 and when we select more than one percentage of personal tribute the entropy and the lizard the information gain is 0.093 and we want to select the highest information gain so we are going to select the cuttin edge of first attribute so that's it and see you on the next video",
            "videoid": "H_S8DaHAAnc",
            "viewCount": "7161"
        },
        "IX0iGf2wYM0": {
            "caption_exist": "T",
            "channel_id": "UCBVCi5JbYmfG3q5MEuoWdOw",
            "channel_title": "Udacity",
            "concepts": [
                [
                    "algorithm",
                    3
                ],
                [
                    "mean",
                    2
                ],
                [
                    "tree",
                    1
                ]
            ],
            "description": "Watch on Udacity: https://www.udacity.com/course/viewer#!/c-ud262/l-313488098/m-313175589\nCheck out the full Advanced Operating Systems course for free at: https://www.udacity.com/course/ud262 \nGeorgia Tech online Master's program: https://www.udacity.com/georgia-tech",
            "dislikeCount": "4",
            "duration": "PT7M12S",
            "likeCount": "70",
            "published_time": "2015-02-23T20:00:07.000Z",
            "tags": [
                "machine learning",
                "supervised learning",
                "computer science",
                "Georgia Tech",
                "Udacity"
            ],
            "thumbnail": "https://i.ytimg.com/vi/IX0iGf2wYM0/hqdefault.jpg",
            "title": "ID3 - Georgia Tech - Machine Learning",
            "transcript": " So, now we have a clear idea of \u200b\u200bthe best, and about our own desire for how  Partition. We have verified Michael's proposal, the high-level algorithm  How to build a decision tree. I think  That we have enough information now and then we can actually do it  With a specific real algorithm. So let's write it down. And the specific algorithm  Suggested by Michael is a generic version  Of something called ID3. So let me write  Define the algorithm, and then we can talk about it. Well, here's an algorithm  ID3. You will simply continue to repeat  Until the problem is resolved. At each step, you will choose the best feature,  We will define what we mean by the best. There  Several different ways to determine the best  In a moment. Given the best attribute that divides  Data the way we want, and do all things  Which we talked about, set it as the resolution attribute of the node.  For each value A can take,  Create the child node. Categorize training examples  On those papers in accordance with the values  Which you take exactly, and if the training package is ideally classified,  Then stop. Otherwise, you repeat  Each of these papers, so choose the best attribute sequentially  For examples of training that have been classified in this paper,  And continue to do so. It is the building of the tree  Until it ends. So this is an ID3 algorithm.  And the essential element to be addressed in some detail  In this case, it is exactly what it means to have  The best feature. Well, then, what exactly is it  Better? So there are a lot of possibilities  Which you can reach. The most common possibility,  And the possibility that I want you  Think about it mostly, is what it's called getting information.  So, getting information is simply a mathematical way to discover  The amount of information I want to get by choosing  Specific attribute. But  What he is already talking about is reducing  Randomize the names you have  With data set, according to find out  The value for the selected attribute. So the formula is simply  As follows. Obtain the information for the symbols S and A  Where the S symbol refers to a set of training examples  Which you look at. The symbol A indicates  A specific feature, known simply as entropy, for special labels  Set the training examples that you have  Minus expected or average introspection  Entropy for each group  Of the examples that you have  Which have value  Specific. Does that sound logical to you, Michael?  &gt;&gt; So what we do is select a feature  This feature can have a different set of  Values, such as true or false, short, medium, long?  &gt;&gt; True.  &gt;&gt; and.  &gt;&gt; The value is denoted by v.  &gt;&gt; Well, each value of these v represents different. What we say next, well, with those papers,  We will use this entropy again.  &gt;&gt; Umm.  &gt;&gt; Well. So what is entropy?  &gt;&gt; Entropy. So, we'll talk about entropy later in the lesson  In some detail and we define it accurately and mathematically. and some  You probably already know what Entropy is  But for those who do not know, it is exactly  Measurement of randomness. So, if I have a coin,  Let's say it's a two-sided coin. Facial can be engraving or writing,  I do not know anything about this coin except  It is sound and balanced. If you throw a coin,  What would be the probability of inscription versus writing?  &gt;&gt; Half.  &gt;&gt; be  Exactly half, if the coin is sound and balanced, the probability is half.  This means that there is no basis for guessing  Whether the face will be engraving or writing when throwing the coin. And so  It will have a lot of entropy. In fact it will be  What is exactly called one bit of entropy. On the other hand, imagine  I have a coin with two engravings on both sides. Before I throw  Coin, I already know what  The output will be. It will be engraving. So  What is the probability that the resulting embossing? it will be.  &gt;&gt; One.  &gt;&gt; One. So that is not so  Information, not random, nor interoptic anyway. and then  The integer bit count is zero. So, look at this set of examples  I have a set of labels that I have, I can calculate the resulting number,  Let's say red x symbols. Versus the resulting number of green o codes.  If these symbols are evenly divided, they will be their entropy  To the maximum, because if I close my eyes,  I wanted to choose an example, it is impossible to know in advance  Whether I will probably get x or I will probably get o  . On the other hand, if I have  All x codes with each other, so I already know before  I chose to pick x. So  The more I have an increase than the other one, the lower the amount  Intubation. it's mean  I have more information inside. Is this a concept, Michael?  &gt;&gt; I think so. So, can we clarify the formula  What about it?  &gt;&gt; Of course. What is the formula for it? You have to remember.  &gt;&gt; I'm not sure what concept you're supposed to use with  These S codes, but it relates to the logarithmic P logarithm, do not wait, is P (logarithm) P.  &gt;&gt; MM-UMM. So the actual formula of introspection, using the same concept  We use it with information, it is simply the sum of all  Possible values \u200b\u200bthat you can see, are potential  See this value, multiplied by the logarithm of probability  One negative. and I do not want  Enter details here. We will move  To a lot of details about it later when  We arrive at the lesson of random compliance, where it will be for introspection  Of great importance. But now I just want you  You have to be introspective  Is a measure of information. Is a measure  For randomness in the variable you do not see. Is a possibility  Know in advance what you will get if  I closed your eyes and chose one of the training examples, vs.  Do not know what you will get. If it is closed  Your eyes have chosen one of the training examples. OK?  &gt;&gt; Well. So in practice,  The trees made by us before,  We preferred divisions  That makes things less random, I think, right? So, if all things are mixed up  With each other, red and green, after  Partition, if all the red things in one side are all green things  on the other side. Each of these two aspects,  What? They will have an extremely low entropy, though  It was high at the beginning before partition.  &gt;&gt; True, this is absolutely true. So, if you remember  The three examples we mentioned before. It was the case with an example of them, that  All samples landed to the left side  From the tree. So, no amount has ever changed  The entropy we had. There was therefore no return from using that attribute.  In another case, we divided the data into two halves. but in  Each case, we had half symbols x and half symbols o  With each other, on both sides of the partition. Which means  That the total amount of entropy has not changed at all. Though  From dividing the data. In the latter case, the best case, we still divide  The data is in two halves, but as long as all x symbols end up in one side  And all the symbols o ended up on the other side, there will be no interop  Or random leftover anyway. And so on  He gave us the maximum \"access to information\".  &gt;&gt; So this is how we choose  For the best feature? Attribute that offers maximum \"access to information\"?  &gt;&gt; Exactly. So the goal is to increase access to information at the expense of intrapsychic increase.  This is the best feature. ",
            "videoid": "IX0iGf2wYM0",
            "viewCount": "17394"
        },
        "JFJIQ0_2ijg": {
            "caption_exist": "T",
            "channel_id": "UCioEIe1o73G-oGR4b34E7Dg",
            "channel_title": "Melvin L",
            "concepts": [
                [
                    "tree",
                    22
                ],
                [
                    "decision tree",
                    10
                ],
                [
                    "classification",
                    7
                ],
                [
                    "type",
                    3
                ],
                [
                    "species",
                    3
                ],
                [
                    "leaf",
                    3
                ],
                [
                    "work",
                    2
                ],
                [
                    "algorithm",
                    2
                ],
                [
                    "window",
                    1
                ],
                [
                    "theory",
                    1
                ],
                [
                    "scientists",
                    1
                ],
                [
                    "plant",
                    1
                ],
                [
                    "range",
                    1
                ]
            ],
            "description": "This video covers how you can can use rpart library in R to build decision trees for classification. The video provides a brief overview of decision tree and the shows a demo of using rpart to create decision tree models, visualise it and predict using the decision tree model",
            "dislikeCount": "16",
            "duration": "PT19M21S",
            "likeCount": "226",
            "published_time": "2015-12-09T15:45:22.000Z",
            "tags": [
                "R language",
                "Machine Learning (Software Genre)"
            ],
            "thumbnail": "https://i.ytimg.com/vi/JFJIQ0_2ijg/hqdefault.jpg",
            "title": "Decision Tree Classification in R",
            "transcript": "  hi everyone in today's video we're going to take a look at how you can use decision trees in our and specifically for how you can use decision trees for classification the agenda is basically broken down into three parts so one will take a look at what our decision trees and it's really not intended to be a very deep dive you know conceptual overview it's really a kind of like a 1 minute overview of what decision trees are if you have not come across that in the past then we'll spend the bulk of our time or demo where I'll be using the our pot library and we'll go through a couple of demos and then finally we'll wrap up when we discuss around some of the limitations which you will see as we go through the demo and a couple of considerations and alternatives to decision tree so that's really the agenda feel free to click around on the video if you want to move ahead or skip certain sections so first things first what are decision trees so as I mentioned it's this is not a very detailed overview of decision trees itself so I'm best if you want to take a look at Google and various other resources but I just wanted to just emphasize on some of the terminologies and some of the core concepts that we will be using so decision tree is a probably one of the oldest and one of the simplest but yet very very useful machine learning algorithms that have been there for a long long time now the idea of a decision tree is that it can be used for broadly two purposes one is for classification and the others for regression regression is really about determining a you know a set of continuous numbers so if you wanted to predict a number or you know something numerical that's obviously when you use regression and classification is when you want to predict the class so that's what we are really going to be focusing on today while some of these terminologies are used differently across different platforms and by different data scientists and statisticians you it's possible that you may have also come across the term cart analysis so that's basically combining classification and regression tree analysis so the real concept I just wanted to highlight is around the tree itself and some of the terminologies that we might be using as we go through the demo so this is a simple example of the data from the Titanic and it just uses a simple tree decision tree to showcase the categorical variable in this particular case if the individual survived or not and as you can see like a tree structure it comprises of leaves and branches so the leaves basically reflect the class label itself and the branches reflect the conditions so think of decision tree is really like a divide and conquer strategy if you will like the most prominent condition being at the top so that's one way to look at it or the other way to think of it as a you know ginormous glorified if then else if mega statements but I'm probably oversimplifying it that way so that's it for theory and background again feel free to stop the video and look around for other resources to know more about decision trees because the rest of the demos really or this video is really focused around the demo all right so so let's dive in next to the demo and in this particular case I'm going to be using the our pot library now R comes with several packages I think there's more than six or seven packages easily that that can be used to build decision trees like today we'll be seeing our part there's the tree library there's a map tree and several several other libraries each library tends to have a slightly different way of you know the internal algorithm used so chances are you will see slightly different results and some variations between individual libraries so again feel free to look around but today we are going to be taking a look at our part library so for us to get started we need a sample data set and again I'm gonna go I can resort to the iris data set so it's one of those simple out-of-the-box data sets that you get with our so it's just for different flaws actually three different flaws and again a couple of different attributes like length on and width of the petals and sepals so one of the first things we need to do is actually split this data so that we have a training data set and a testing data set and you'll notice here in this example they're they're all grouped together so I can't we need a more interesting way than just say for example take the first hundred rows for example so in order for us to get some sample to work with so let's just say for example we have sample hoe by the way I did not highlight so here we have from 150 observations so we want to get a sample from 150 numbers we'll take 50 numbers alright so if we go back here oops so that's a sample so you can see here that we'll be running this script a couple of times so hence I'm writing it in a separate script window so it gives us the opportunity to kind of generate random numbers of course if there are much better alternatives to what I've just done then let's actually split the data set into the iris data set into training and testing so race a train will take a dress and get that and all the columns and similarly we'll take a look at the and finally the testing we will get the reverse of that so that should give us both the training and the test data so then Irish test and we have the Train right collectively it's given us all those 150 numbers okay so now that we have the data let's actually use our part to actually do build some decision trees so we will need to include the library our part our part and simple command here so now we want to work with the training data set you'll notice actually let me actually change that just bear with me a second we need more training data so I'm just going to increase that so that it's it's a hundred and definitely want to increase the amount of data that you have in training so okay so where were we lost so let's actually use our our part function so we're going to create a model a decision tree model in remodel same and our part here we want to predict the classification of species here let me just copy that species and we want to utilize all the other variables give me a second you have species tilde and period which is really a shortcut otherwise I'd have to keep instead of you're using a period the other alternative is I have to copy all of this here and you know keep adding a plus against each of these loops that's here so whichever suits your fancy in in my case the data said basically I'm utilizing all the other remaining you know attributes there so I'm gonna use the shorthand and just put a period there alright so that's the first parameter we know that we're going to be utilizing the training data set here and finally we want to do classification so method is equal to class I'm some keyboard yep plus all right so let's just run that yep that seems to run fine and now that we've got the data we can you know visually inspect that data so let's say for example we want to see what's in here so DTM you'll notice it's a it's a very textual description and basically we had 100 observations there so it's it's giving you an idea in terms of how the tree structure has been built the decision tree and you can see here's the legend if you want to use that to follow what's what's being described here so basically given the condition and what that split is of that condition the error or the loss itself and finally it tells you which of these are the leaf nodes so the leaf nodes are where it actually shows us the class labels and the others are purely just conditional so here you can visually see that there is some error in how it's classified certain flaws but that's a textual display we can visualize it in much more graphical ways option number one is to use the the plot but it's not really the best in terms of visual appeal so if you just use plot alone it just draws the decision tree without adding any of the that's oh you want to add text here so here it's plotted against the text but still let's it didn't do a very good job so the out of the box plots not very useful when you're trying to visually analyze the data of course you can analyze the data directly from here but that's where one of the more powerful features of the Arpad library is it's plotting capability so we also have to utilize the the or include the are part plot so that's what included that and now we can actually run our part plant let me just copy that easier than typing and decision tree model so here it's as you can see it's a much more visual you know visually useful way for us to draw the tree and you can clearly see the conditions so yes is always to the left and no to the right so here you can see that is actually use the petal the leaf line the length and the petal width as the two primary conditions to segment the three types of flowers there's quite many other parameters that we can use here so I let you look into it on your own but I've spent some time looking at some of the attributes here and you can see there's a there's a range of parameters that you can pass in here to produce much richer much more visually appealing plots of the tree so I'll just give you an example so we're going to use a the type of for here and under extra we're going to use a different set of values here to come up with something a little more visually appealing so here as an example type this is for and the other one was extra extra is equal to 101 so these look really good in my opinion much better than what you get with all these parameters so just to explain here you can actually see the the actual percentages and also what's very helpful as it will actually show you if there were any errors like here it's a accurately predicted set soda but here you can actually see that it incorrectly predicted - so visually you can see right here in the tree so again if if you use other parameters it will it allows you to basically tweak that UI if you will and have a play with the plot functionality so as you can see if we run and a couple of times using random numbers anyway so it's actually giving you an idea of what that error conditions can be we will talk about that in a minute but last thing perhaps what we want to do is now that we've got a tree here we want to utilize that and do some predictions so let's save dict we want to use the the tree the decision tree model that we have just created we want to run it against the test data set so in the past of course we created the model using the training data so this time we're going to be using the test data we're going to consider all rows there and we want to use columns 5 which is the category and then finally the the type of classification again and we'll store that as a prediction say and finally now that we've got that prediction let's actually compare how that fed the predictions against the actual test value so we can use the table function so table we're going to take this data set here and let's actually seed so that's the actuals versus the predicted values ah just give iris test did I miss something here okay make sure that's correct to have the same length ah just realize I made a mistake didn't have to specify this column here let's run that again so that's a prediction all right so here we are so in this particular case you can actually see that what you have here is the actuals and the columns represent the predictions so in in some cases you can see that it's or in one case here there was an incorrect prediction so there was one error so one out of 50 is the error rate which isn't too bad but then that again is a reflection of the sample of the data that we have selected so if I run this again so let's let's see if it comes up with a different result so here again you can see because I've run the script again it's used a different sample so here you can actually see that this time around the error was 4 4 out of 50 so again mom you can see variations in the result so that's I think an important thing you want to consider when you are using decision trees or for that matter any machine learning technique particularly when you're looking at supervised learning techniques like this providing the right training data or you know a good representative training data is vital for the success of your algorithm so what we've seen so far today as you can see is just a really quick overview of the decision trees and how you can implement decision trees using our part as I mentioned there are several other libraries with an R that you can use to create these decision trees we've talked about some of the limitations so again you've seen in this example here today you know how important it is to have a good representative set for your and the data that you feed into your training clearly clearly influences the results that you get off your predictive models now position please is by itself is fairly simple right but there are other and symbol techniques like say random forests and gradient boosted machines which uses decision trees internally to create much more elegant and much more sophisticated machine learning algorithms but we'll cover that in a future video so that's a wrap for this one so we've seen in today's video a simple example of how you can build decision trees there are other advanced topics like how you can prune trees and various other optimizations which I hope to cover in a future video thanks everyone for watching",
            "videoid": "JFJIQ0_2ijg",
            "viewCount": "56107"
        },
        "LDRbO9a6XPU": {
            "caption_exist": "T",
            "channel_id": "UC_x5XG1OV2P6uZZ5FSM9Ttw",
            "channel_title": "Google Developers",
            "concepts": [
                [
                    "node",
                    23
                ],
                [
                    "tree",
                    21
                ],
                [
                    "root",
                    7
                ],
                [
                    "list",
                    4
                ],
                [
                    "leaf",
                    3
                ],
                [
                    "decision tree",
                    3
                ],
                [
                    "algorithm",
                    2
                ],
                [
                    "functions",
                    2
                ],
                [
                    "diameter",
                    1
                ],
                [
                    "pattern",
                    1
                ],
                [
                    "mean",
                    1
                ],
                [
                    "block",
                    1
                ],
                [
                    "walk",
                    1
                ],
                [
                    "recursion",
                    1
                ]
            ],
            "description": "Hey everyone! Glad to be back! Decision Tree classifiers are intuitive, interpretable, and one of my favorite supervised learning algorithms. In this episode, I\u2019ll walk you through writing a Decision Tree classifier from scratch, in pure Python. I\u2019ll introduce concepts including Decision Tree Learning, Gini Impurity, and Information Gain. Then, we\u2019ll code it all up. Understanding how to accomplish this was helpful to me when I studied Machine Learning for the first time, and I hope it will prove useful to you as well.\n\nYou can find the code from this video here: \nhttps://goo.gl/UdZoNr\nhttps://goo.gl/ZpWYzt\n\nBooks! \nHands-On Machine Learning with Scikit-Learn and TensorFlow https://goo.gl/kM0anQ\n\nFollow Josh on Twitter: https://twitter.com/random_forests\n\nCheck out more Machine Learning Recipes here: https://goo.gl/KewA03\n\nSubscribe to the Google Developers channel: http://goo.gl/mQyv5L",
            "dislikeCount": "21",
            "duration": "PT9M53S",
            "likeCount": "1868",
            "published_time": "2017-09-13T17:21:52.000Z",
            "tags": [
                "Google",
                "developers",
                "machine learning",
                "tensorflow",
                "tensor flow",
                "ML",
                "ML tips",
                "machine learning tips",
                "decision trees",
                "machine learning decision trees",
                "decision tree learning",
                "devtools",
                "dev tools",
                "developer",
                "developer tips",
                "developer tools",
                "machine learning developer",
                "product: machine learning",
                "fullname: Josh Gordon",
                "Location: NYC",
                "Team: Scalable Advocacy",
                "Type: DevByte",
                "GDS: Full Production",
                "Other: NoGreenScreen"
            ],
            "thumbnail": "https://i.ytimg.com/vi/LDRbO9a6XPU/hqdefault.jpg",
            "title": "Let\u2019s Write a Decision Tree Classifier from Scratch - Machine Learning Recipes #8",
            "transcript": "  oh hey everyone welcome back in this episode we'll write a decision tree classifier from scratch in pure Python here's an outline of what we'll cover I'll start by introducing the data set we'll work with next we'll preview the completed tree and then we'll build it on the way we'll cover concepts like decision tree learning Gini impurity and information gain and you can find the code for this episode in the description and it's available in two formats both as a jupiter notebook and as a regular python file okay let's get started for this episode I've written a toy dataset that includes both numeric and categorical attributes and here our goal will be to predict the type of fruit like an apple or a grape based on features like color and size at the end of the episode I encourage you to swap out this data set for one of your own and build a tree for a problem you care about let's look at the format I've redrawn it here for clarity each row is an example and the first two columns provide features or attributes that describe the data the last column gives the label or the class we want to predict and if you like you can modify this data set by adding additional features or more examples and our program will work in exactly the same way now this data set is pretty straightforward except for one thing I've written it so it's not perfectly separable and by that I mean there's no way to tell apart the second and fifth examples they have the same features but different labels and this is so we can see how our tree handles this case towards the end of the notebook you'll find testing data in the same format now I've written a few utility functions that make it easier to work with this data and below each function I've written a small demo to show how it works and I've repeated this pattern for every block of code in the notebook now to build the tree will use a decision tree learning algorithm called cart and as it happens there's a whole family of algorithms used to build trees from data at their core they give you a procedure to decide which questions to ask and when cart stands for classification and regression trees and here's a preview of how it works to begin we'll add a root node for the tree and all nodes receive a list of roses input and the root will receive the entire training set now each node will ask a true/false question about one of the features and in response to this question we split or partition the data into two subsets these subsets then become the input to two child nodes we add to the tree and the goal of the question is to unmix the labels as we proceed down or in other words to produce the purest possible distribution of the labels at each node for example the input of this node contains only a single type of label so we'd say it's perfectly unmixed there's no uncertainty about the type of label on the other hand the labels in this node are still mixed up so we'd ask another question to further narrow it down and the trick to building an effective tree is to understand which questions to ask and when and to do that we need to quantify how much a question helps to unmix the labels and we can quantify the amount of uncertainty at a single node using a metric called Gini impurity and we can quantify how much a question reduces that uncertainty using a concept called information gain we'll use these to select the best question to ask at each point and given that question will recursively build the tree on each of the new nodes we'll continue dividing the data until there are no further questions to ask at which point we'll add a leaf to implement this first we need to understand what type of questions can we ask about the data and second we need to understand how to decide which question to ask when now each node takes a list of roses input and to generate a list of questions we'll iterate over every value for every feature that appears in those rows each of these becomes a candidate for a threshold we can use to partition the data and there will often be many possibilities in code we represent a question by storing a column number and a column value or the threshold will use to partition the data for example here's how we write a question to test if the color is green and here's an example for a numeric attribute to test if the diameter is greater than or equal to 3 in response to a question we divide a first contains all the rows for which the question is true and the second contains everything else encode our partition function takes a question and a list of roses input for example here's how we partition the rows based on whether the color is red here true rose contains all the red examples and false Rose contains everything else the best question is the one that reduces our uncertainty the most and Ginny in purity lets us quantify how much uncertainty there is at a node information gain will let us that let's work on impurity first now this is a metric that ranges between zero and one where lower values indicate less uncertainty or mixing at a node it quantifies our chance of being incorrect if we randomly assign a label from a set to an example in that set here's an example to make that clearer imagine we have two bowls and one contains the examples and the other contains labels first will randomly draw an example from the first bowl then we'll randomly draw a label from the second and now we'll classify the example as having that label and Gini impurity gives us our chance of being incorrect in this example we have only apples in each Bowl there's no way to make a mistake so we say the impurity is zero on the other hand given a bowl with five different types of fruit in equal proportion we'd say it has an impurity of 0.8 that's because we have a 1 out of 5 chance of being right if we randomly assign a label to an example encode this method calculates the impurity of a data set and have written a couple examples below that demonstrate how it works you can see the impurity for the first set is zero because there's no mixing and here you can see the impurity is 0.8 now information gained will let us find the question that reduces our uncertainty the most and it's just a number that describes how much a question helps to unmix the labels at a node here's the idea we begin by calculating the uncertainty of our starting set then for each question we can ask we'll try partitioning the data and calculating the uncertainty of the child nodes that result we'll take a weighted average of their uncertainty because we care more about a large set with low and certainty than a small set with high then we'll subtract this from our starting uncertainty and that's our information gain as we go we'll keep track of the question that produces the most gain and that will be the best one to ask at this node let's see how this looks in code here we'll iterate over every value for the features well generate a question for that feature then partition the data on it notice we discard any questions that fail to produce a split then we'll calculate our information gain and inside this function you can see we take a weighted average and the impurity of each set we see how much this reduces the uncertainty from our starting set and we keep track of the best value I've written a couple demos below as well okay with these concepts in hand were ready to build the tree and to put this all together I think the most useful thing I can do is walk you through the algorithm as it builds a tree for our training data this uses recursion so seeing it in action can be helpful you can find the code for this inside the build tree function when we call build tree for the first time it receives the entire training set as input and is output it will return a reference to the root node of our tree I'll draw a placeholder for the root here in gray and here the Rose we're considering at this node and to start that's the entire training set now we find the best question to ask at this node and we do that by iterating over each of these values well split the data and calculate the information gained for each one and question that produces the most gain now in this case there's a useful question to ask so the gain will be greater than zero and we'll split the data using that question and now we'll use for Kirchen by calling build tree again to add a node for the true branch the roads were considering now are the first half of the split and again we'll find the best question to ask for this data once more we split and called the build tree function to add the child node now for this data there are no further questions to ask so the information game will be zero and this node becomes a leaf it will predict that an example is either an apple or lemon with 50% confidence because that's the ratio of the labels and the data now we'll continue by building the false branch and here this will also become a leaf will predict apple with 100% confidence now the previous call returns and this node becomes a decision node in code that just means it holds a reference to the question we asked and the two child nodes that results and we're nearly done now we return to the root node and build this becomes Alif and that predicts great with 100% confidence and finally the root node also becomes a decision node and our call to build tree returns a reference to it if you scroll down in the code you'll see that I've added functions to classify data and print the tree and these start with a reference to the root node so you can see how it works okay I hope that was helpful and you can check out the code for more details there's a lot more I have to say about decision trees but there's only so much we can fit into a short time here a couple of topics that are good to be aware of and you can check out the books in the description to learn more as the next step I'd recommend modifying the tree to work with your own data set and this can be a fun way to build a simple and interpretive classifier for use in your projects thanks for watching everyone and I'll see you next time",
            "videoid": "LDRbO9a6XPU",
            "viewCount": "97286"
        },
        "LgLMX5nYfN4": {
            "caption_exist": "T",
            "channel_id": "UCBVCi5JbYmfG3q5MEuoWdOw",
            "channel_title": "Udacity",
            "concepts": [
                [
                    "shape",
                    1
                ],
                [
                    "work",
                    1
                ],
                [
                    "animal",
                    1
                ],
                [
                    "type",
                    1
                ],
                [
                    "group",
                    1
                ],
                [
                    "algorithm",
                    1
                ],
                [
                    "range",
                    1
                ]
            ],
            "description": "Watch on Udacity: https://www.udacity.com/course/viewer#!/c-ud262/l-313488098/m-641939067\nCheck out the full Advanced Operating Systems course for free at: https://www.udacity.com/course/ud262 \nGeorgia Tech online Master's program: https://www.udacity.com/georgia-tech",
            "dislikeCount": "11",
            "duration": "PT4M13S",
            "likeCount": "23",
            "published_time": "2015-02-23T19:51:22.000Z",
            "tags": [
                "machine learning",
                "supervised learning",
                "computer science",
                "Georgia Tech",
                "Udacity"
            ],
            "thumbnail": "https://i.ytimg.com/vi/LgLMX5nYfN4/hqdefault.jpg",
            "title": "Decision Trees Continuous Attributes - Georgia Tech - Machine Learning",
            "transcript": " OK. So, we've done well. So during all  Finally, we discovered what the trees of the decision were. we  Know what to represent. We know how expressive it is. we've got  Algorithm enables us to build resolution trees in a manner  Effective. We have done almost everything we can do  With decision trees, but there are still some open questions  I want to think about it. Here are some of them  I want you to think about it and then discuss it.  So, all the examples we have used so far. And all  Things we thought about for good educational reasons. did not  We have only separate outputs but we also have separate inputs. So  One of the questions we may ask ourselves is what happens if we have,  Attributes Connected? So Michael, let me ask you  this question. Let's say that we have some features  Related. We were not just asking if something  An animal or whether it is a human being or whether it is  It rained outside or whether we really cared about age  Weight, distance or anything else  It may have a connected attribute. How  Will we make this work in a decision tree?  &gt;&gt; Well, I think the real way to do that  In something like age, there will be an equal factor  Number of potential ages.  &gt;&gt; Well, then this is a possibility. So let's put the age, then  We have a branch. For value 1.0, another for value 1.1, and another  For the value of 1.11, and another for the value of 1.111  &gt;&gt; Yes, I understand that. OK. Well, at the very least, then. Hahaha. What  If we include only the ages in the training group?  Assuming that there are at least a limited number of them. Oh,  We can do that. We can do that, except  What we will do next when we arrive  To something in the future was not present in the training session.  &gt;&gt; Oh, right. Can we check the test set?  &gt;&gt; No, we are not allowed to see the test set. This is cheating,  It is not a pretty good cheat that we do when we choose a good representation.  &gt;&gt; Well, that's enough. Well we can use ranges. What about domains?  Is not this the way we deal with more than just individual values?  &gt;&gt; Give me an example.  &gt;&gt; For example, in the twenties.  &gt;&gt; Well, then, huh. How will we look like this  With a decision tree? Let's move in the twenties. Age.  &gt;&gt; How we can do it.  &gt;&gt; You can type something like age, tag element, or bracket.  &gt;&gt; 20.  &gt;&gt; 20 and comma, or 21, or 29, or 30  To the end.  &gt;&gt; Yes this is too much. Why not write the age,  Let's see, greater than or equal to 20, less than 30.  And just draw an oval shape around it. OK? So, this range, these  All numbers are between 20 and 30, including 20, but not 30. Is not it?  &gt;&gt; Yes.  &gt;&gt; The good thing about this is that this is a question. You either will be in  Twenties of your age or not. So, the result of this is actually true or false.  &gt;&gt; So, I think the good news is that we are now  Know how we evaluate such features because we have  A formula from [UNKNOWN] three tells you what to do.  But there seem to be many other different features to check them out.  &gt;&gt; True, indeed if it is  Already a connected variable, there is basically an infinite number  Of which has been verified. But now we can do the cheating that you wanted  To play before. We can see the training package,  And try to choose questions about the data types found in  Training group. So, for example, if all values \u200b\u200bare  In the 1920s, then there is no point in asking the question.  It will only begin to divide it [UNKNOWN] instead on the values \u200b\u200bthat were, to move less than  25 or greater than 25, you can imagine all kinds of ways  In which you may do so. You may consider all the values \u200b\u200bthat appear in the  Training group, says Well, I'll do a binary research. So, I will  Just create an attribute for less  Of half of anything in the training group  Or greater than half of any existing range  In the training group. Does this seem logical?  &gt;&gt; Yes, that's smart thinking.  &gt;&gt; Well. Thank you. I just invented this  At the moment. Well, then you do those things and that's how  Your dealing with related features. This leads me to a question  Next, in fact I will do so as a short test because I am  I want an answer from viewers. ",
            "videoid": "LgLMX5nYfN4",
            "viewCount": "10487"
        },
        "NsUqRe-9tb4": {
            "caption_exist": "T",
            "channel_id": "UC5zx8Owijmv-bbhAK6Z9apg",
            "channel_title": "Data Science Tutorials - All in One",
            "concepts": [
                [
                    "tree",
                    13
                ],
                [
                    "decision tree",
                    6
                ],
                [
                    "node",
                    6
                ],
                [
                    "leaf",
                    2
                ],
                [
                    "two-dimensional",
                    2
                ],
                [
                    "algorithm",
                    2
                ],
                [
                    "predicate",
                    2
                ],
                [
                    "scale",
                    1
                ],
                [
                    "state",
                    1
                ],
                [
                    "mean",
                    1
                ]
            ],
            "description": ".\nCopyright Disclaimer Under Section 107 of the Copyright Act 1976, allowance is made for \"FAIR USE\" for purposes such as criticism, comment, news reporting, teaching, scholarship, and research. Fair use is a use permitted by copyright statute that might otherwise be infringing. Non-profit, educational or personal use tips the balance in favor of fair use.\n.",
            "dislikeCount": "0",
            "duration": "PT8M34S",
            "likeCount": "14",
            "published_time": "2016-04-14T02:33:34.000Z",
            "tags": [
                "Mining of Massive Datasets",
                "Data Mining",
                "Information Retrieval",
                "Coursera",
                "Computer Science",
                "Video Lecture",
                "Video Tutorial",
                "Video Course",
                "Course",
                "Data Science",
                "Data Mining Video Lecture",
                "Data Mining Video Course",
                "Stanford University",
                "University of Stanford",
                "Stanford",
                "Jure Leskovec",
                "Anand Rajaraman",
                "Jeff Ullman",
                "Mining Data",
                "Online Data Mining",
                "Online Data Mining Course",
                "Best Data Mining video course",
                "Coursera Data Mining Video Lecture",
                "Decision Trees",
                "Machine Learning"
            ],
            "thumbnail": "https://i.ytimg.com/vi/NsUqRe-9tb4/hqdefault.jpg",
            "title": "Decision Trees | Mining of Massive Datasets | Stanford University",
            "transcript": "  so continuing the exploration of large scale machine learning topics today we will focus on a different algorithm called the decision tree algorithm and the idea here is that instead of modeling the decision space between let's say positive and negative examples using a single single line we will we will learn much more complicated decision boundaries and this will allow us to in a kind of much more fine-grained way represent the differences between let's say positive and negative examples if we are talking about classification so just to remind you what we are talking about or thinking about is that given given a given attribute let's say a wealth of a person we want to predict the value of this attribute by the means of some other features or attributes available available to us right so in a sense what we would like to do is we would like to figure out how wealthy is the person given various other characteristics of this person the way we can think about this is that we can think of input attributes as the features that we know about the person so for example imagine that every person is described by a set of the features x1 2 X sub D and then each feature let's call it J has a domain all sub J right when I say a domain I mean what is the number of or what are all the different values that this feature or this attribute can take for example if the feature X sub J is a favorite color of a person this would be what we call a categorical attribute right it can take values red green blue and so on another thing for example would be if I can have numerical features in a sense that for example if I ask what's the weight of that person or how old is that person that is that is nicely a number and in a given variable in this case a feature would have a domain that is let's say non-negative real numbers if we are asking about a weight of a person and now of course the same thing happens to what is the value of y which is the value that we want to predict we will call is the dependent variable or this is the the thing the quantity we want to predict so now our idea is is the following as kind of it started last time we are given a set of data D we're given we are given a set of n we will call them training examples X sub R X sub y comma Y sub I where basically the idea is that X sub I is a D dimensional feature vector right so D dimensions from up here and then Y is simply the variable that we want to predict and now based what we want to do is we want to learn a mapping that map's the features of a given person to the wealth of a given person so that is our goal and now the question is how is this function that takes the features and transformed them into the value we want to predict how does this function look like right so the idea will be that basically decision trees is a is a tree structure the plan that given a set of variables it wants to test that set of variables and predict the output so to be a bit more concrete here is an example of a decision tree right a decision tree is a is a tree hierarchical structure where I have two types of nodes I have what we will call internal decision nodes and I have a prediction nodes here here at the node of the sec hexagons and the state can be you know as deep as we want as wide as we want and so on and the important thing here is that in this tree we have the as I mentioned decision nodes that basically examine the value of a given attribute and ask you know is this predicate satisfied yes or no and then we have the internal leaf nodes which we can call the seizure nodes which then say okay for this set of examples let's predict the value so the idea is that the as I mentioned internal nodes have the split values the leaf nodes make predictions and in particular what we look at today is only decision nodes that that are based on binary splits so basically every node has at most two children so kind of the there is a predicate or a condition and then whether that condition is satisfied or not right so kind of if condition is satisfied we go to the left and if it's not we go to the right we will be talking about numerical attributes and we will think about regression which means why we'll be let's call it a real number so this is the setting that we want to talk about so the hard part here is how do we build the tree right now the easy part is how do we make a prediction so making a prediction is very easy right so given that somebody that we already build the tree given a training example we want to predict what is the y-value for that training example so the idea is that we want to take this X sub X sub I and kind of drop it throughout throughout the tree and see which pitch prediction node it's it ends it ends in and then predict that value for example imagine that I have a particular example that comes in here I can first evaluate right is the first feature of this training example is it does it have value less than v1 if the answer is yes I move to the left I end up in the prediction out and my predicted value is 0.4 k2 if the condition is not satisfied and the answer is now to my to my condition I would move to the right and I would keep kind of moving down the tree until somewhere I would hit my prediction out and I would make a prediction for that case right so the idea is that basically every node examines one individual feature looks at it and then makes a decision is the condition satisfied or not and then kind of moves either to the left branch or to the lab to the right branch so to have an idea what decision trees are really doing and what kind of interesting decision surfaces they can find let's look at this simple two-dimensional example where I have a set of data points in this two-dimensional space and imagine we want to do classification one cat we want to separate pluses from minuses so the way the decision tree building procedure would would start is that given this kind of training data set we want to go and recursively kind of split this space into smaller and smaller pieces such that each individual region in this space is uniformly populated by either all pluses or or all minuses so for example what we could do is is to say first we have our first node in the decision tree and we want to decide what is the first split and imagine that we say the first split is that value we want so we say is the value x1 of a given of a given data point image in the data point here is the act the value x1 less or more than the value v1 right so I have v1 and now if the answer is yes I go to the left and otherwise I go I go to the right and now I could now go and find the second split so I would take the OL all the data that is that is down here this is everything that kind of goes to the left and I ask okay how can I now split pluses and minuses in this case and maybe I find the new decide to draw a line here so now I would have the value of v2 and I can draw another decision node and asked you know is x2 less than veto and then I say is is it is it or is it not if it is right then I notice I have all the pluses so if the value is less than V 2 then I here I say yes let's predict plus if the value is not less than V 2 I still have this messy part so for example I would want to maybe split again along this dimension and split along that dimension and similarly on the top I would want to split here and this way build the build 3 throughout throughout it in the end where I have the prediction out so this is kind of one idea how we can think about building a decision tree what is now interesting here is that we have this complicated decision boundary that splits pluses and minuses from each other and we see that we kind of have this area where there are minuses and then we have this other area here where there are pluses and we see that we could never kind of separate these two classes out with a single line but we can use decision trees to learn this more complicated decision boundary",
            "videoid": "NsUqRe-9tb4",
            "viewCount": "2265"
        },
        "O__7lAqni7A": {
            "caption_exist": "T",
            "channel_id": "UCNYv4HA3WjV3gZGLfBehRWQ",
            "channel_title": "Noureddin Sadawi",
            "concepts": [
                [
                    "tree",
                    13
                ],
                [
                    "target",
                    10
                ],
                [
                    "decision tree",
                    10
                ],
                [
                    "node",
                    8
                ],
                [
                    "leaf",
                    5
                ],
                [
                    "algorithm",
                    4
                ],
                [
                    "root",
                    3
                ],
                [
                    "block",
                    1
                ],
                [
                    "stable",
                    1
                ],
                [
                    "backtracking",
                    1
                ],
                [
                    "edge",
                    1
                ]
            ],
            "description": "My web page:\nwww.imperial.ac.uk/people/n.sadawi",
            "dislikeCount": "17",
            "duration": "PT12M35S",
            "likeCount": "181",
            "published_time": "2014-08-27T18:36:45.000Z",
            "tags": [
                "Decision Tree",
                "Entropy",
                "How Decision Trees Work"
            ],
            "thumbnail": "https://i.ytimg.com/vi/O__7lAqni7A/hqdefault.jpg",
            "title": "How Decision Trees Work 1/2 .. an Introduction + What is Entropy",
            "transcript": "  Obachan in this video on the next video I'll be expected to you the intuition behind the decision tree classifier how it works and I'll give you an example to make sure that things are clear we continue our frequency table based classifiers we have done the first three and now we're doing the decision tree classifier the decision tree classifier what it does it as the name suggests it builds classification or regression models so it can be used actually for classification and regression in the form of a tree structure so the name says it all what it does it breaks down a data set into smaller and smaller subsets while at the same time and associated decision tree is incrementally developed so break down data set into smaller sets with at the same time building or increasing a decision tree the final result is a tree with decision nodes and leaf nodes a decision node for example if you remember how did our weather data set it can be the outlook it can have two or more branches for Outlook for example we have three branches we can branch on sunny overcast and rainy for a leaf node for example to play or not to play a leaf node represents a classification or a decision so leaf node there's no branches we only have a class or classes there the topmost decision node in a tree which corresponds to the best projector is called the root node decision trees can handle both categorical and numerical data if you remember from before if we need to transform then we can use transformation techniques that we learn in our data exploration and analysis tutorial now for our data set for our weather data set if you remember we had four attributes outlook temperature humidity and windy all of them are of type categorical and we had our target or a class to play golf either yes or no at the century might look like this we can have a root node while outlook and then we branch for sunny overcast and rainy and therefore the sunny may be the best way to branch now is on windy for true or false we can have either yes or no and for overcast when overcast and then it's always a yes we don't have any nodes with over with Outlook overcast and then for rainy after that we can probably split on or branch on humidity notice that temperature now is not us at all we can branch on humidity for high or normal then we can have our leaf nodes now as having the glass valleys another yes or no now the way it works the core algorithm behind it is called the id3 algorithm invented by j.r Quinlan it employs a top-down greedy search through the space of possible branches with no backtracking so it tries all possible all possible branches and it chooses the first one the idea algorithm it uses entropy and information gain to construct a decision tree entropy from decision theory now the entropy in case you're not familiar with it is sort of a measure of uncertainty and the value of the entropy is computed for for example two or maybe three sort of classes or categories and the way it's done is by multiplying the probability of each category of each class by the log to page two of the value of that probability and summing over all the values of classes I come to the next slide so a decision tree is built upon is built you know top-down from a root node and it involves partitioning the data into subsets that contain instances with similar values so they need to be homogenous after we branch things need to be get need to get more homogeneous the id3 algorithm uses entropy to calculate the homogeneity of a sample so we use entropy to decide where something is homogenous or not if the sample is completely homogeneous the entropy is zero so if all the values are the same or the same class then you need the ID entropy will be zero if the sample is equally divided the entropy is one now as we mentioned before the equation of the entropy is like this is the summation for all classes minus P I log base 2 P I now P I is the probability of each of these classes from I equals 1 to C the number of classes P is the probability of that class times log to log base 2 of that probability the minus here is because log to base 2 of any probability probability is also between 0 and 1 and log base 2 of any value to 0 1 is negative that's why we have that negative that minus sign there so they cancel each other and becomes positive if you look at this diagram here we have probability here point 1.2.3.4 until 1 and here we have the values of the entropy point 1 point 1 till 1 now for point here for for this point at this point here point 5 can have the entropy equals minus P log to P minus Q log to Q let's say P and Q are two values now at 0.5 so things are equally divided and if we do that the entropy is minus 0.5 log to base 2 point 5 minus point 5 log base 2 point 5 and that's 1 so if things are equally divided in the entropy is 1 if everything is homogeneous then the entropy will be 0 either there or there P is zero now to construct a decision tree we need to compute two types of entropy we need to compute entropy of the target for splitting and then we compute the edge after splitting and we compute the difference between the two inch piece and see the highest information gain that will come to that when we have an example but just focus here that to build a decision tree we need to calculate two times of entropy using frequency tables as follows let's assume now we have our weather data the first type of entropy we want to compute is entropy of the targets from the frequency table now the equation for entropy of your target is inch P of s is our target now summation of minus P I log to base 2 P I and I is from 1 to C so these are the are color classes P of every class times log to base 2 of the probability of the same class and we sum all the classes if we remember our weather data from here our class now we have 14 instances and we have 5 nodes and 9 yes is now we compute the entropy now of our target of our class regardless of the orbit attributes and frequency table for the class will play golf which is our target now we have nine aces and five knows entropy of play golf is the entropy of five and nine no and yes entropy of because now the probability of 5 is 5 over 14 which is 0.36 probability of 9 is 9 over so probability of yes poverty of no is 5 over 14 probability of es is 9 over 14 which is 0.6 for and we plug that in our equation we sum over the values with two glasses layers and now we plug in their probabilities compute loop log to base two of the probability and the value will be 0.9 for that's an example for interview of the target the second one now is entropy using the frequency table of two attributes now if we split entropy if we split let's say for example we split you with split using Outlook now focused on this frequency table for the outlook now and notice the counts now we count them against we build it from stable against the class values and now we're only concerned about the rows if you remember from before from Bayesian classifier we were concerned about the columns probability this would have been three over nine the sum of the SSF of nodes but now because we want to use these categories of variable outlook to split the data then we really concerned about the rules about the these categories these values so sunny when the class is yes probability is three over five size 5 is a sum of the row and when the class is not probability is two over five likewise for overcast for over four zero for remember the zero frequency problem from class from Bayesian classifier the rainy is when a class is es 200 500 letters no is 3 over 5 and these now most time up to 14 as you can see here the number of instances now what we do is we compute the entropy for the target when we split by Outlook and the way we do that is we have the probability now for for the category over the class times entropy of that category yes so when the probability of that category times probability uptime entropy category overall values of the Castaway ie here our variable is outlook so we do that over the three values for sunny overcast and rainy probability of sunny times entropy of the values for sunny ie 3 &amp; 2 plus because we have summation here the probability of overcast times the entropy of 4 &amp; 0 plus the probability of rainy times the entropy of 2 &amp; 3 now if we block those guys plug those guys in probability of sunny is 5 over 40 probability of overcast is for over 40 probability of rainy is 5 over 14 and now the entropy both 3 &amp; 2 we just use the exactly the same concept as we mentioned here we use the same idea here - P the summation of - beloved ways to be each of them and remember now for example for sunny the probability of the years is risen of the over 3 is 3 over 5 and here is 2 over 5 when the class is no we don't use 3 over 9 and we only understand otherwise as we mentioned before we plug those values in there should be nice and easy should be straightforward and we end up with a value of the entropy as point 6 9 3 after that we need to compute the information gained the information gain is based on the decrease in entropy after the data set is split is split on an attribute so the information gained is entropy before splitting minus entropy after splitting this is the entropy of the target as we mentioned before and this is after we split using one of the attributes now constructing a decision tree is all about finding attribute that returns the highest information gained ie the most homogeneous branches what that means is we want the one that the lowest entropy so the difference here is as high as possible these things will make sense when we compute an example but just remember that we need to compute two values or two types of entropy entropy over the target of your for splitting them into B after difference I'm going to disappear in the next video I'll give you an example some things to make more sense thanks again for watching gonna see",
            "videoid": "O__7lAqni7A",
            "viewCount": "46061"
        },
        "Q4NVG1IHQOU": {
            "caption_exist": "T",
            "channel_id": "UCs7alOMRnxhzfKAJ4JjZ7Wg",
            "channel_title": "Victor Lavrenko",
            "concepts": [
                [
                    "tree",
                    14
                ],
                [
                    "node",
                    4
                ],
                [
                    "algorithm",
                    4
                ],
                [
                    "decision tree",
                    3
                ],
                [
                    "depth",
                    2
                ],
                [
                    "subset",
                    2
                ],
                [
                    "deterministic",
                    1
                ],
                [
                    "iteration",
                    1
                ]
            ],
            "description": "Full lecture: http://bit.ly/D-Tree \nA decision tree can always classify the training data perfectly (unless there are duplicate examples with different class labels). In the process of doing this, the tree might over-fit to the peculiarities of the training data, and will not do well on the future data (test set). We avoid overfitting by pruning the decision tree.",
            "dislikeCount": "14",
            "duration": "PT7M32S",
            "likeCount": "311",
            "published_time": "2014-01-19T20:30:29.000Z",
            "tags": [
                "decision",
                "tree",
                "decision tree",
                "ID3",
                "C4.5",
                "attribute",
                "value",
                "classification",
                "prediction",
                "machine learning",
                "machine",
                "learning",
                "applied",
                "overfitting",
                "pruning",
                "training",
                "testing",
                "data"
            ],
            "thumbnail": "https://i.ytimg.com/vi/Q4NVG1IHQOU/hqdefault.jpg",
            "title": "Decision Tree 5: overfitting and pruning",
            "transcript": "  okay so now you know the entire algorithm now let's talk about the specifics the details so one interesting thing about decision trees is it's a recursive algorithm and what it will do is it will keep splitting the data until it ends up with pure sets and what this means yeah yes yes okay so the question there was what if you have two exactly identical days right if you have two exactly identical days so they have the same weather and the same wind and the same humidity and all the parameters were the same but on one day John played and on another John on another day John didn't play there is no way to separate them all right so this is called noise in labels right it either means your representation is inadequate you should be looking at different set of attributes what it means maybe there is a mistake in labels or maybe this is just the way it is right maybe there's some uncertainty and that kind of uncertainty cannot be taken care of no classifier is able to deal with that it's it's it's impossible you know if you have classifier as a function if you feed in two identical inputs you always get an identical out but it's not it's not stochastic its deterministic so it's true for every for every classifier um but that's a degenerate example so assuming that that doesn't happen what the decision tree will do is it will keep splitting the data until every subset is perfect and what that means is the decision tree will always classify your training examples perfectly always right so remember in the last in the last lecture we talked about naive Bayes and the mistake that it made so we had a training example and the naive Bayes after training classified that in the exact opposite class from what it's labeled also everybody remember that so it happens with naive Bayes it happens with lots of other classifiers it will never happen with decision trees because decision trees will keep splitting until it gets it perfect so until the accuracy is 100% now in many cases this means splitting until you have singleton subsets so it will keep splitting until the leaves have one example each right and and that's not a very good thing it's not a very good thing because you are not guaranteed that the testing data so this will only work if anything you see in the future is something that you've already seen in the past right your tree will not be able to classify any data item that it hasn't seen before because it's basically it separated them all into Singleton's and it's looking at the labels for each single so that's not a very good idea and this is a symptom of a much more general problem that we'll talk about in the subsequent lecture it's called overfitting right so as the decision tree keeps splitting the data the tree gets bigger and bigger as it gets bigger it becomes more and more accurate on the training data but at some point it will become less accurate on the data that you haven't seen before and the data that you're not using to train it on the future data so here's a typical example this is accuracy how accurate your trees what percentage of things is classifying correctly this is the depth of the sorry that the number of nodes in the tree and what you see is this is the performance on the training data the more you split the better your accuracy becomes and this will just keep rising until it hits one what but by that point the size of the tree is going to be the same as the size of the data set that you have fed into it if you try to measure performance on testing data this is what's going to happen it's going to keep rising at first but then it's actually going to start dropping so this is called overfitting your algorithm is becoming too specific to the data that you used to train it and it cannot generalize very well to the examples that you haven't given it before so that's a problem how do you avoid overfitting well there's there's basically two ways they both try to do the same thing they try to not grow a tree that is too large not grow a tree with Singleton subsets at the leaves so two ways to do that one in addition to the splitting you do significance tests so every time you're you're splitting your you're trying to do you run a little significance test and say okay I was able to split the data now could that be due to chance or not basically so if you had a sort of two examples and you split it into two so now you have two Singleton's what is the chance that you're guaranteed that the subsets will be pure right because there is only one example so you wouldn't be surprised at all so by chance it would happen 100 percent of the time but if you had a subset of say a thousand right and you split it into two sets and you end up with a pure set when one is five hundred and another is five hundred that is there's a very low chance of that happening accidentally randomly so a significance test can give you a number not only how good the split is but how what what the chance would be to for it to happen just randomly and then you can put a threshold on that and just stop splitting once the numbers are above a certain threshold so that's one way um another way which is which is covered in the textbook is the is the pruning so here what you do is you grow the tree right to the full depth and then you start pruning it removing the branches that don't seem to do well on the future data so how do you do it you take the set of that you said you take your set of training examples you randomly split it into a training set and a validation set you grow the tree on the training portion and then you use the validation set to prune so you go over each node of the tree pretend that you remove the node and then see how well this pruned tree would do on your validation set and then in the end once you've computed this number you remove the node that gives you the greatest improvement so you basically you're trimming the nodes the overfitting nodes the nodes that are too specific to your training data and that seem to hurt performance on the validation set and and so this is one iteration of the algorithm you remove one node and then you keep repeating it until further improving coming is harmful so basically you build the whole tree you start here and you start chopping nodes until you get to this point and that is that is your final treatment okay so we've covered we've covered decision trees but that's actually before we talk about this stuff if you're trying to follow the book by the way these are these are the sections that it's doing",
            "videoid": "Q4NVG1IHQOU",
            "viewCount": "82273"
        },
        "VS4eMbpc43w": {
            "caption_exist": "T",
            "channel_id": "UCifWfwxTfOYYoczCWmIY8bA",
            "channel_title": "BharatiDWConsultancy",
            "concepts": [
                [
                    "algorithm",
                    2
                ],
                [
                    "link",
                    2
                ],
                [
                    "tree",
                    2
                ],
                [
                    "scale",
                    2
                ],
                [
                    "location",
                    1
                ],
                [
                    "flow",
                    1
                ],
                [
                    "target",
                    1
                ],
                [
                    "decision tree",
                    1
                ]
            ],
            "description": "Data Science & Machine Learning - C5.0 Decision Tree Intro - DIY- 25 -of-50\nDo it yourself Tutorial\nby\nBharati DW Consultancy \ncell: +1-562-646-6746 (Cell & Whatsapp)\nemail:  bharati.dwconsultancy@gmail.com \nwebsite: http://bharaticonsultancy.in/\n\nGoogle Drive- https://drive.google.com/open?id=0ByQlW_DfZdxHeVBtTXllR0ZNcEU \n\n\nC5.0 Decision Tree - Classification\nDecision trees are very powerful classifiers, which utilize a tree structure to model the relationships among the features and the potential outcomes.\n\nAn all-purpose classifier which has a highly automatic learning process; it can  handle numeric or nominal features. \n\nC5.0 uses entropy, a concept analogous to the information theory that quantifies the randomness, or disorder, within a set of class values.\n C50_model{anglebrace}- C5.0(train_Predictors, train_Target)\n C50_predict{anglebrace}- predict(C50_model, test_data)\n\nGet the data from Balance Scale Data Set. \n Attribute Information:\nClass Name: 3 (L, B, R) \nLeft-Weight: 5 (1, 2, 3, 4, 5) \nLeft-Distance: 5 (1, 2, 3, 4, 5) \nRight-Weight: 5 (1, 2, 3, 4, 5) \nRight-Distance: 5 (1, 2, 3, 4, 5)\n\nhttp://archive.ics.uci.edu/ml/datasets/Balance+Scale  \n\nCitation Policy:\nIf you publish material based on databases obtained from this repository, then, in your acknowledgements, please note the assistance you received by using this repository. This will help others to obtain the same data sets and replicate your experiments. We suggest the following pseudo-APA reference format for referring to this repository:\nLichman, M. (2013). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science.\nHere is a BiBTeX citation as well:\n@misc{Lichman:2013 , author = \"M. Lichman\",\nyear = \"2013\", title = \"{UCI} Machine Learning Repository\",\nurl = \"http://archive.ics.uci.edu/ml\",  institution = \"University of California, Irvine, School of Information and Computer Sciences\" }\n\n\nData Science & Machine Learning - Getting Started - DIY- 1 -of-50\nData Science & Machine Learning - R Data Structures - DIY- 2 -of-50\nData Science & Machine Learning - R Data Structures - Factors - DIY- 3 -of-50\nData Science & Machine Learning - R Data Structures - List & Matrices - DIY- 4 -of-50\nData Science & Machine Learning - R Data Structures - Data Frames - DIY- 5 -of-50\nData Science & Machine Learning - Frequently used R commands - DIY- 6 -of-50\nData Science & Machine Learning - Frequently used R commands contd - DIY- 7 -of-50\nData Science & Machine Learning - Installing RStudio- DIY- 8 -of-50\nData Science & Machine Learning - R Data Visualization Basics - DIY- 9 -of-50\nData Science & Machine Learning - Linear Regression Model - DIY- 10(a) -of-50\nData Science & Machine Learning - Linear Regression Model - DIY- 10(b) -of-50\nData Science & Machine Learning - Multiple Linear Regression Model - DIY- 11 -of-50\nData Science & Machine Learning - Evaluate Model Performance - DIY- 12 -of-50 \nData Science & Machine Learning - RMSE & R-Squared - DIY- 13 -of-50 \nData Science & Machine Learning - Numeric Predictions using Regression Trees - DIY- 14 -of-50 \nData Science & Machine Learning - Regression Decision Trees contd - DIY- 15 -of-50\nData Science & Machine Learning - Method Types in Regression Trees - DIY- 16 -of-50\nData Science & Machine Learning - Real Time Project 1 - DIY- 17 -of-50\nData Science & Machine Learning - KNN Classification - DIY- 21 -of-50\nData Science & Machine Learning - KNN Classification Hands on - DIY- 22 -of-50\nData Science & Machine Learning - KNN Classification HandsOn Contd - DIY- 23 -of-50\nData Science & Machine Learning - KNN Classification Exercise - DIY- 24 -of-50\nData Science & Machine Learning - C5.0 Decision Tree Intro - DIY- 25 -of-50\n\nMachine learning, data science, R programming, Deep Learning, Regression, Neural Network, R Data Structures, Data Frame, RMSE & R-Squared, Regression Trees, Decision Trees, Real-time scenario, KNN, C5.0 Decision Tree,",
            "dislikeCount": "0",
            "duration": "PT4M44S",
            "likeCount": "12",
            "published_time": "2017-05-19T04:32:57.000Z",
            "tags": [
                "Cognos Training",
                "Qlikview training",
                "Tableau Training",
                "Informatica Training",
                "Machine learning",
                "data science",
                "R programming",
                "Deep Learning",
                "Regression",
                "Neural Network",
                "R Data Structures",
                "Data Frame",
                "RMSE & R-Squared",
                "Regression Trees",
                "Decision Trees",
                "Real-time scenario",
                "KNN",
                "C5.0 Decision Tree"
            ],
            "thumbnail": "https://i.ytimg.com/vi/VS4eMbpc43w/hqdefault.jpg",
            "title": "Data Science & Machine Learning - C5.0 Decision Tree Intro - DIY- 25 -of-50",
            "transcript": "  hello and welcome to another video of this machine learning to wit your so series in this video we're gonna talk about decision tree and specifically on si point 5.0 classification so as we have discussed earlier in the other videos for regression decision trees are also very powerful classifiers these utilize because a tree structure it's more of a flow chart and it's really strong in terms of classifications as well it's an all-purpose classifier which is C 0.3 5.0 is an all-purpose classifier which has a highly automatic learning process it can handle both numeric or non numeric features it uses entropy which is and loggers to information theory that quantifies the randomness or disorder within a set of class values the syntax for C Point C 5.0 is this C model the keyword c 5.0 which is in blue train predictors the predictors which are going to be used for training and train target for training purposes and then this is predict again here predict is the keyword which is going to predict the test using the model train here so we're going to see the hands-on in a few minutes so to do this we are going to use the same balanced scale data set which we have used in the previous video on the knn algorithm so class names left weight distance right weight and right distance this is the data set location and this is the citation policy for that data set so I am making sure that I am following the policy now last time had the data set and I changed the class from a numeric to one to three which is a non numeric soil from non numeric be our else two numeric one two three but I'm going to keep this time class as d for balance R for right and L for left Alice I have saved this version as balance scale copy and this is also available in the Google Drive link which looks something like this and this link is available in the description section of the video and here you go so this is your R script and this is the CSV file for this video so coming back to our studio I'm going to use the same kind of format as we did for knn algorithm so the same set of 625 values with five variables and if you do the summary this is the classification left we'd left distance right weight and right distance next case we will do the clean and test data set exactly similar to what we did for K and n so this is just to make sure that we have good mix of the last few variables and then please note that the last time we did 600 now we are doing 550 just for better test you're putting more number of Records in the test data frame so you can see now it's 550 and 75 and then you have to require see you need you need c50 package if you don't have the package please install this package using installed packages c500 so I'm so require so in the next video we are going to do the classification on decision 3c 5.2 and we see how to improve the model as well thank you for watching",
            "videoid": "VS4eMbpc43w",
            "viewCount": "784"
        },
        "WOOTNBxbi8c": {
            "caption_exist": "T",
            "channel_id": "UCUSRKgnYeos2aX_JnV9qa3w",
            "channel_title": "Alexander Ihler",
            "concepts": [
                [
                    "tree",
                    11
                ],
                [
                    "threshold",
                    6
                ],
                [
                    "depth",
                    5
                ],
                [
                    "decision tree",
                    5
                ],
                [
                    "functions",
                    4
                ],
                [
                    "binary tree",
                    2
                ],
                [
                    "leaf",
                    2
                ],
                [
                    "node",
                    1
                ],
                [
                    "root",
                    1
                ],
                [
                    "target",
                    1
                ]
            ],
            "description": "Decision tree basics",
            "dislikeCount": "0",
            "duration": "PT5M33S",
            "likeCount": "69",
            "published_time": "2012-11-04T15:36:30.000Z",
            "tags": [
                "machine learning",
                "UC Irvine",
                "UCI-cs273a",
                "UCI-cs178"
            ],
            "thumbnail": "https://i.ytimg.com/vi/WOOTNBxbi8c/hqdefault.jpg",
            "title": "Decision Trees (1)",
            "transcript": "  decision trees are another flexible class of functions or learners that are commonly used in machine learning a decision tree represents a function consisting of a series of comparisons or if-then-else statements each branch may lead to another comparison and branch or to an output value we can draw this as a tree where the outputs are the leaf nodes I'll draw true values going to the right and false values going to the left for example this decision tree corresponds to a function that outputs class red if feature one compares to greater than 0.5 if it compares to less than 0.5 we move to another decision node where we compare feature 2 to another threshold say in this case point 5 again if that comparison is false we output Class blue if it's outputs true we continue to get another branch in comparison where we look at feature 1 again compared to a threshold point 1 and if that's true we output red if it's false we output blue the output for this function is given over on the right where the first comparison x1 greater than 0.5 leads us to determine that this entire region will predict red the 2nd comparison x2 greater than 0.5 leads us to make a decision in this region if x2 is less than 0.5 we decide blue leading to blue in that entire region and finally we compare x1 to 0.1 in the remaining region if x1 is less than 0.1 we predict blue plus blue in that region if it's red if it's greater we predict red if s red in this region here for features that have discrete values we can do one of several things since the notion of threshold is not very meaningful instead we typically have a few possible options first we could branch on all possible values of the feature so if we compare a discrete feature x1 that has four possible values a b c and d then perhaps we make a branch for each of those on the other hand we may prefer to have a binary value a binary tree in that case we might compare feature x1 and branch say to the left only if it equals a and branch to the right if it equals any other value or we might have a more general comparison where we say if x1 is in some set say a and D branch left and if it's not if it's in any other value branch right the advantages and disadvantages of this in the left hand case that since everything down the far left path will have x1 equal to a there's never any advantage to comparing x1 to any other value below that point because x1 will always equal a in that branch on the other hand in the right hand branch many of these branches correspond to multiple possible values of x1 and thus x1 might appear again and need to be split however in that case we'll have a binary tree and that may be easier for us to represent the complexity of the function of a decision tree depends on the depth of the tree for example a depth 1 decision tree which is often called a decision stump since it's cut off at the root is an extremely simple classifier here we could look at only one feature we compared to some threshold and if the threat comparison is false we output one value and if the comparison is true we output another value so in for example a two dimensional feature space x1 x2 this would lead to functions which partition the space up either left right or top bottom into two different classes extending our decision tree to two levels leads to a more complex set functions since we can now compare x12 threshold and then within each of those domains we can further compare x1 or x2 to another threshold and this will be partitioning our space up to into up to four different output regions each of which can have its own class assigned to it in general for a tree of depth D we can end up with up to 2 to the D regions each of which has their own prediction in it decision trees can be used for regression in almost exactly the same way here instead of outputting a discrete class value we output a real valued number at the leaf nodes since our prediction Y is over some real valued set again the functions look essentially the same so here we're drawing pictures with a single feature X single feature X to predict a real valued target Y and if we have a single depth we partition that X feature space up into two parts left and right each of which has its own real valued prediction associated with it for a depth to a tree we divide the feature space up into up to four regions based on thresholds so here we might have four different real valued predictions",
            "videoid": "WOOTNBxbi8c",
            "viewCount": "13947"
        },
        "Zze7SKuz9QQ": {
            "caption_exist": "T",
            "channel_id": "UCxC911vLGv05O3jXvCkW26A",
            "channel_title": "Laurel Powell",
            "concepts": [
                [
                    "tree",
                    13
                ],
                [
                    "implies",
                    7
                ],
                [
                    "decision tree",
                    2
                ],
                [
                    "algorithm",
                    1
                ]
            ],
            "description": "In this video, I create a decision tree using the Gini Index to determine the splitting attributes.\n\nI originally created this video (and the others in my series) to be used with a specific KDD class which is taught at my home university. I first encountered this algorithm in class there.  If you would like to look into this topic in more detail, or read a bit about some similar algorithms, I am including the link to one of the presentations that I used as a reference.  \n\ncoitweb.uncc.edu/~ras/KBS-Class/1-Decision-Trees.ppt\n\nThank you for watching!",
            "dislikeCount": "35",
            "duration": "PT14M36S",
            "likeCount": "448",
            "published_time": "2015-09-02T03:28:22.000Z",
            "tags": [
                "Decision Tree Learning",
                "Gini Coefficient",
                "algorithms",
                "decision feature",
                "data mining"
            ],
            "thumbnail": "https://i.ytimg.com/vi/Zze7SKuz9QQ/hqdefault.jpg",
            "title": "Decision Tree Building based on Impurity for KDD or Machine Learning",
            "transcript": "  hi I'm Laurel and in this video we will be constructing a decision tree from an information system using the Gini index to determine the splitting attributes let's work this problem together this system consists of tuples X 1 through X 8 and contains attributes m and Q and the decision feature are the system contains 8 records X 1 through X 8 attribute M can be M 1 or M 2 the domain of attribute n is N 1 n 2 or in 3 like attribute in attribute Q can be q1 q2 or q3 lastly the decision feature can be r1 or r2 in order to begin constructing our tree we need to determine which attributes to split table on there are many methods for determining these splitting attributes in this example we will be using the Gini index one definition for the Gini index is the expected error rate of the system to calculate this determine the probability of getting each distinctive value of the decision attribute for example value r1 appears in three out of eight tuples it's probability is three times out of eight next we calculate the probability of value r2 r2 appears more often in five out of eight tuples after calculating the probabilities for each different value of the decision feature multiply them to determine the Gini value for the system in this system the value is 15 over 64 or 0.23 for 375 I like to use decimals with more complex fractions now that we have calculated the Gini value for the system as a whole we calculate it for each individual attribute this is used to determine the splitting attributes to determine the Gini of M first we find the probability of a particular M value let's start with M 1 M 1 appears four times out of 8 tuples which can be reduced to provide a probability of 1/2 next multiply that times the probability of M 1 sharing a record with each value of our first M 1 and R 1 are never in the same couple so the probability of them being together is 0 out of 4 or just 0 now we look at the connection between M 1 and R 2 M 1 and R 2 are always together so the probability of M 1 and R 2 being connected is 4 out of 4 or 1 multiply 0 &amp; 1 then multiply that times 1/2 for the probability of M 1 this number is then added to M 2 the value for M 2 is calculated exactly the same way M 2 appears in 50% or 1/2 of the tuples next begin looking at how M 2 relates to R 1 and R 2 multiply 1/2 times the quotient of the probability of M 2 being connected with R 1 and R 2 R 1 is paired with M 2 in three out of four tuples and multiply that times the number of times M 2 is paired with R 2 which appears in 1 out of 4 topple the total genie value for attribute in is 3 over 32 or zero point zero nine three seven five after calculating the genie value of attribute M move on to attribute n as you can see in this system and one appears in two out of eight or one out of four tuples multiply that times the probability of n one being paired with R 1 and R 2 N 1 and R 1 never appear in the same couple as there are only two values for R and one is always paired with R - now repeat these steps with n 2 and n 3 and add the parts of the equation together and 2 it appears with our one one out of three and with our two two out of three times like n2 in three appears three out of eight times it is paired with our one two out of and with our to one out of three times after working it out the genie value for attribute n is 1 over 6 or zero point one six six six seven after completing the equation for n we move on to calculating the genie value for Q since the calculation used for Q is the same as the one use for M and n I will be speeding up a bit if you would like this is a good place to pause the video and work out the genie value of Q yourself after working it all out the genie value of Q is one over six the same as the genie value of N after working out the Gini values for each attribute the next step is to calculate the gain the gain is what we use to determine the splitting attribute to calculate the gain for M we use the Gini calculation for the entire system earlier we calculated that to be 0.23 next we need the genie value of M the value of M is 0.09 375 now subtract the value of M from the value of the system the gain of em is zero point one four zero six to five we do the same process again to calculate the gain of n take and the genie value for n find the difference between the value of the system and the value of n which is the gain for n which in this case is zero point zero six seven seven zero five we use the same method to calculate the gain for Q I'm going to speed up for this next part now that we have calculated the games for each attribute we can compare them we use the attribute with the largest gain as the splitting attribute in our decision tree now we can use em as are splitting attribute in constructing our M can be M 1 or M 2 so our tree has two branches let's take a look at our original table rows X 1 X 4 X 6 and X 8 contain M 1 notice that the decision feature in all four rows is r2 we can use this to construct our first rule our first rule is M one implies R two it has a support of four and a confidence of 100% so branch M one leads to r2 now we can start working with M two tuples X 2 X 3 X 5 and X 7 contain m2 let's separate those out into a separate table unlike M 1 M 2 can imply r1 or r2 to continue building our tree we need to look at the N and Q attributes we determine the second splitting attribute the same way as we determine the first one calculate the Gini value for the new table start by determining the probability of R 1 it appears in three after that we look at the probability of r2 it appears in one out of four tuples multiply the two to determine the genie value for this part of the system now we calculate the genie value of the two remaining attributes with the equation we used earlier like we did earlier start with the probability of n1 which is 1 over 4 and multiply that times the probability that r1 and r2 will appear in the same couple as n1 we repeat this for n2 lastly we add the values for n 1 and n 2 2 the value for n 3 after completing the calculation the Gini value of n for this part of the system is zero Gini can be interpreted as a expected error rate because in this sub table n has a Gini value of zero this means that for each value of n there is only one possible value of R we calculate the Gini value of Q in exactly the same manner as we use for n I'm going to speed through this part just like within earlier the genie value of Q is zero the next step is to calculate the game for N and Q we repeat the same steps as we used earlier to calculate the game use a genie value of 0.1875 and zero for the value of n the gain of n is 0.18 75 repeat these steps to calculate the gain for Q because the gains of N and Q are equal they would work equally well as the splitting attribute for this example I'm going to create the trees and rules for both n and Q let's start with splitting on n and one only appears in one tupple x5 this shows that n1 implies R 2 now we can create the rule remember that because the subtable we have been working from is part of the m2 branch of our tree all rules generated must include m2 on the left hand side M 2 and N 1 imply r2 has a support of 1 and a confidence of 100% we can move on to creating rules with n 2 tupple X 3 shows that N 2 implies R 1 we write the rule as M 2 and n to imply R 1 this rule has a support of 1 and a lastly we look at the rules we can make with n 3 in both tuples n 3 implies R 1 we write this last rule as M 2 and n 3 imply R 1 it has a support of two and a confidence of 100% now we can add this information to the tree split n has three branches and one which implies R 2 n 2 which implies R 1 and n 3 which implies R 1 this finishes out our tree and here are the rules generated using M is our first living attribute and n is our second we use exactly the same procedure to generate the tree and rules when we split on cue like we did earlier I'm going to speed up when doing the steps for Q you may want to pause the video here and generate the tree and rules yourself here is the completed tree with M as the first splitting attribute and Q as the second splitting attribute and these are the rules generated with splits on M and on Q so to conclude here's one last look at the tree for N and a look at the tree for Q I hope you enjoyed my video thank you for watching and please check out my channel for other algorithm videos",
            "videoid": "Zze7SKuz9QQ",
            "viewCount": "43246"
        },
        "eKD5gxPPeY0": {
            "caption_exist": "T",
            "channel_id": "UCs7alOMRnxhzfKAJ4JjZ7Wg",
            "channel_title": "Victor Lavrenko",
            "concepts": [
                [
                    "subset",
                    10
                ],
                [
                    "algorithm",
                    6
                ],
                [
                    "node",
                    2
                ],
                [
                    "decision tree",
                    2
                ],
                [
                    "tree",
                    2
                ],
                [
                    "list",
                    1
                ],
                [
                    "predicate",
                    1
                ],
                [
                    "mean",
                    1
                ]
            ],
            "description": "Full lecture: http://bit.ly/D-Tree \nA Decision Tree recursively splits training data into subsets based on the value of a single attribute. Each split corresponds to a node in the. Splitting stops when every subset is pure (all elements belong to a single class) -- this can always be achieved, unless there are duplicate training examples with different classes.",
            "dislikeCount": "43",
            "duration": "PT9M26S",
            "likeCount": "1835",
            "published_time": "2014-01-19T17:51:41.000Z",
            "tags": [
                "decision",
                "tree",
                "decision tree",
                "ID3",
                "C4.5",
                "attribute",
                "value",
                "classification",
                "prediction",
                "machine learning",
                "machine",
                "learning",
                "applied",
                "recursive",
                "split",
                "pure",
                "subset"
            ],
            "thumbnail": "https://i.ytimg.com/vi/eKD5gxPPeY0/hqdefault.jpg",
            "title": "Decision Tree 1: how it works",
            "transcript": "  all right so let's start we're going to be talking about our second classifier today last class we talked about naive Bayes right and that's the classifier that basically tries to estimate probabilities for each class and then picks picks the class with the maximum probability so today we're going to talk about a different classified our decision trees decision trees work in a very different way from naive Bayes and the best way to show how decision trees work is to actually start with an example so suppose I have the following data set so my task is to predict if a certain guy named John is going to play tennis on a given day right so to help me with that I've looked I've observed John over a number of days and recorded various things that I think might influence John's decision to play tennis right so I looked at what kind of weather it is is it sunny or is it raining right humidity is it high as normal is it windy right and whether German ended up playing on that day so that is my training set those are the examples that I'm going to build a classifier from and the question is okay a new set of data comes in all right so on day 15 it's raining the humidity is high it's not very windy so is John going to play or not and just by looking at the data it's kind of hard to it's it's kind of hard to decide right because it's you know some days when it's raining John is playing other days John is not playing sometimes he plays with strong winds sometimes he doesn't play with strong winds and with weak winds life so so what do you do happen how do you predict it and the basic idea behind decision trees is to try to at some level understand why John plays right so this is the only classifier that that will cover that tries to predict John playing in this way and by understanding I mean what the what decision trees are going to do is they're going to take a look at each one of these attributes the weather the humidity the wind and try to make the best inference possible about assuming that it's raining is John going to play with what chance and what other factors may influence John playing if it's raining right so in general what the algorithm is going to look like is it's going to look at the different attributes that you have in your data use those attributes to split the data into subsets so for example if I'm looking at if I'm looking at the outlook right it could be raining or it could be sunny or it could be overcast so those are three different subsets and you will have some examples in each one of those subsets and if and if some subset is pure then that's great so if John always for example plays when it's rainy then we stop otherwise we keep going we look at a different attribute and try to split the data further so it's a divide-and-conquer algorithm of sorts I guess you could not really but you could say it that way and then when we have a new data set what we're going to do is we're going to when we have a testing example for which we want to make a prediction we're going to look at which subset that example falls into and then use the dominant class in that subset so if if if this falls into a subset where John always plays then we're going to say yes he will play on that day and if not we'll say no so that's at a high level so let's look at how it operates so suppose suppose we take outlook as our first as our first attribute right so here's a outlook attribute there are three types of outlook right it could be sunny all right and if it is sunny then these are the training examples that corresponds to a sunny day right so do you want to eat so you can just look on the previous slide and see right sunny sunny sunny sunny right so we just look at all there we just look at all examples when it was sunny and you and you see that okay for some of them John played in fact two times John played and for three of them John didn't play okay now it could also be overcast and these are the examples for overcast 2n or it could be raining raining and these are the training examples the training days when it was raining so so we look at them and start counting in this in this subset when it was overcast all days when it was overcast out of my training data John ended up playing alright so this is what you would call a pure subset it's a subset where you have only positive examples or only negative examples but not a mix so you have four yeses and 0 knows when it was sunny it was two years and three knows when it was raining it was three yeses and two knows so the way the algorithm is going to proceed is it's going to say this is a pure subset we don't need to split it any further so if it's overcast we just know that John plays tennis but if it's sunny there's still some uncertainty right so what we need to do is we need to figure out a way to split this set further so that we can get pure subsets right so let's take this subset when it was sunny and let's assume that we're picking humidity as our next attribute to split on it looks good right so you split on humidity and you end up with two sets you mid ities either high or normal whenever it was high John didn't play and when it was normal John did play okay so now we have two pure subsets and the algorithm would stop at this point because there is no further there's no need to divide the data further ah so then you do the same thing on the rain and let's say you just take the wind for the whim of it right so you take the winds you break on the wind and for the wind the wind was fine a week we're strong and when the wind is weak John plays when the wind is strong John doesn't play ok that's it the algorithm is done because at this point what you've done is you've taken your training set and you have sorted it into pure subsets based on some attribute values right so now there is there's no need to try to split it any further good so the resulting decision tree from this would be something like that so what you do is you just replace the list of examples with the decision that your algorithm would make so everything here was negative everything here was positive question how do we select which attribute goes next at this point an angel comes down and tells us but on the on the next slide we'll talk about that so so so basically when you once you've replaced the examples with decisions this is what is called a decision tree right it's basically a set of rules or you can think of it as a formula it's really like a logical predicate logical formula that tells you in what cases John will play and in what cases jungle not right so if it's overcast John will play right if it's sunny and humid John will not play if it's raining and but there's not much wind then John will play and in all other cases John jungle motley ok so you get our new data example all right so day 15 it's raining high humidity and it's not very windy so how do you how do you compute the prediction for this day really simple you look at the outlook right and it was raining so you're go down to this branch then you look at the wind because that's the next attribute on this branch to look at and the wind in this case was weak so yes John will play so that's how decision trees work that's how they compute these predictions and important thing about the decision trees is you strictly yeses and noes what you also keep as you keep the counts that you had in each node right how many positives and how many negatives you had an income that in each node and these positives and negatives allow you to not just output a prediction but to also put a confidence on that prediction so for example before I did at any comparisons I had nine positives and five negatives so your confidence wouldn't be wouldn't be very high so here I have four positives and zero negatives so your confidence would be high relative to say something like this or something like that so you have more examples the more examples you have in your subset the more confident you would be about the decision that your name",
            "videoid": "eKD5gxPPeY0",
            "viewCount": "388404"
        },
        "nfWU1EwkwwU": {
            "caption_exist": "T",
            "channel_id": "UCBVCi5JbYmfG3q5MEuoWdOw",
            "channel_title": "Udacity",
            "concepts": [],
            "description": "Watch on Udacity: https://www.udacity.com/course/viewer#!/c-ud262/l-313488098/m-314025767\nCheck out the full Advanced Operating Systems course for free at: https://www.udacity.com/course/ud262 \nGeorgia Tech online Master's program: https://www.udacity.com/georgia-tech",
            "dislikeCount": "18",
            "duration": "PT3M15S",
            "likeCount": "27",
            "published_time": "2015-02-23T20:00:29.000Z",
            "tags": [
                "machine learning",
                "supervised learning",
                "computer science",
                "Georgia Tech",
                "Udacity"
            ],
            "thumbnail": "https://i.ytimg.com/vi/nfWU1EwkwwU/hqdefault.jpg",
            "title": "Decision Trees Learning - Georgia Tech - Machine Learning",
            "transcript": " &gt;&gt; Well, inspired by the game 20 questions, let's try to write  Exactly what I did during these  The game to get the answer and find out that what I was thinking of is a person Michael Jackson  . So what was the first thing I did?  &gt;&gt; I tried to imagine a group of  Possible answers that may be in your mind.  I tried to think of a question  Helps divide it into almost equal parts.  &gt;&gt; Well, so what's the way to write it  In the language we use with  Categorization and supervised learning so far?  &gt;&gt; Oh, I see. So if you already know there are 200 things  One of the things you might ask me about. And what  Rate their own attributes. I will think about the question  Which can divide this group into two halves,  is not it? Instead of just imagining a set of  Things, if I have a list, which I have in the case of training group.  The first thing I did was choose the best feature  You can think about them. By the way, I mentioned something  Very important here. You have already identified what is meant by the best. You mentioned  The best is the same thing that divides things into almost two halves.  So let's look at it in a moment. Well, then the first thing  I did is choose the best feature. Then  You asked a question around and according to the answer  You chose another feature, right? Does this seem fair?  &gt;&gt; Yes.  &gt;&gt; Well.  So the way we think about decision trees is  Follow the path of answer, to be a vision and repeat what we did. [LAUGH].  We again choose the best feature, and then ask a question,  Follow the path of answer and so forth, and continue to do so for how long?  &gt;&gt; until the possibilities are limited  In one era only, as was the case with what we did.  &gt;&gt; Well, so you get the answer. OK.  This represents an algorithm. So you choose the best attribute,  You have already defined what is meant by the best feature. You want to  Choose a feature that can somehow get rid of at least half  Things you worry about and then the other half  in the game. Then ask a specific question. Track and track  Answer and then again select an attribute  Etc. and so on until you get the answer  You want. This represents an algorithm and these  The algorithm is that we can use to build a decision tree already.  The only difference is between what you did and what you did  In the case of teaching the decision tree is,  As long as you do not already know what the answer is  Indeed, because you do not know what the thing is  Which you might think of, you'll already have to follow all the possible paths  Each time think about all the following best and potential features  So you can answer any question in full. Does this seem logical?  &gt;&gt; I understand that. So, well, instead of  Practice the game interactively only,  I imagine all the possible ways that the answer can be taken  And then I build this whole scheme of paths, this whole tree.  &gt;&gt; Well, then, let's see if we can do this with some pictures  I really want to decide whether you really believe in your definition of the best.  Concept?  &gt;&gt; Of course.  &gt;&gt; Well, let's do it. ",
            "videoid": "nfWU1EwkwwU",
            "viewCount": "17257"
        },
        "p17C9q2M00Q": {
            "caption_exist": "T",
            "channel_id": "UCcAtD_VYwcYwVbTdvArsm7w",
            "channel_title": "mathematicalmonk",
            "concepts": [
                [
                    "tree",
                    10
                ],
                [
                    "leaf",
                    7
                ],
                [
                    "binary tree",
                    3
                ],
                [
                    "decision tree",
                    2
                ],
                [
                    "classification",
                    2
                ],
                [
                    "node",
                    1
                ],
                [
                    "randomization",
                    1
                ],
                [
                    "form",
                    1
                ]
            ],
            "description": "Basic intro to decision trees for classification using the CART approach.\r\n\r\n\r\nA playlist of these Machine Learning videos is available here:\r\nhttp://www.youtube.com/my_playlists?p=D0F06AA0D2E8FFBA",
            "dislikeCount": "14",
            "duration": "PT10M16S",
            "likeCount": "346",
            "published_time": "2011-06-11T00:49:28.000Z",
            "tags": [
                "machine learning",
                "statistics",
                "math"
            ],
            "thumbnail": "https://i.ytimg.com/vi/p17C9q2M00Q/hqdefault.jpg",
            "title": "(ML 2.1) Classification trees (CART)",
            "transcript": "  in this video we'll start to look at decision trees decision trees and we'll focus on the cart approach to decision trees as introduced by Bremen at Al and it turns out so decision trees are an extraordinarily conceptually simple approach to classification and regression but it turns out that despite their simplicity decision trees can be extremely powerful especially when coupled with some randomization techniques that we'll talk about a little later on they can give the essentially the best performance of any current algorithms very surprising so what's the Bissell remember the basic set up as always in supervised learning is we have some data and we're given a new point X or a bunch of new points and we have to associate a Y with the new X or X's so the main idea behind decision trees at least in this formulation the card formulation is you form a binary tree and you minimize the error error minimize the error in each leaf of the tree so that's the main idea I'll explain what that means for an example so let's say we've got so let's say these these X's are in r2 something nice and simple that we can draw so we've got our nice r2 here and let's draw some some points so let's let's say maybe maybe we've got some ones here some red ones and maybe we've got some ones some ones down here or something like this and maybe someones here or something something like that and then we've also gotten of course another class maybe we this one's got some some zeros some points which are labeled zero something like this maybe something like that now we have our data set are these are all points and each of them has a label and in a decision tree you choose a sequence of binary splits of the data so first so let's say so if you want to split this data in some good way maybe a first choice would be let's split on the the first coordinate here so let's split on this coordinate that would be a good choice because we can separate all these ones over here from everything else and then maybe for the next split so now now we have two regions so this this divides our two into two disjoint regions and now we've consider each of these regions this one's this one looks excellent everything is the same but over here we need to do some more splitting perhaps so let's choose this one say maybe we choose split now we get we separate this into two regions again and maybe we're going to split something something like this and then maybe we'll split here something we'll split that one also so this gives us some nice nice separate regions and this defines a tree a binary so what's the binary tree associated with this that sequence of splits so first so if we call this this access here let's call it the xi1 axis and this we'll call this X I to axis so it's just the first coordinate in each of these X's and it's just the second coordinate so the first split I drew let's say maybe this is I don't know maybe this is it 1 or something maybe that happens to be the point 1 maybe this happens to be 2 maybe this is I don't know maybe this one's like 1.8 or something maybe this is one so the first split says is x i1 this coordinate is it greater than 1 and if it is if it's yes so if yes we'll go that direction and if no we go this direction so no all these points are nose so they all go down this path in the tree and now and we'll let's go ahead and take care of this so this this now since we haven't split this anymore this becomes a leaf in the tree and let's let's draw the leaves according to the number of points of each class so how many 0 so we get 0 zeroes so let's put 0 4 0 0 s and we get 4 ones now on this side on the right side when all the points were that are greater we then split along this log via the two axis we made this split so the next one is X I 2 greater than 1 and if yes so yes we're going to go that way no we're going to go this way so if it's yes right so if it's greater we're up here and we have to split one more time so it's xi1 greater than 1.8 that was that one we get yes and no so the yes right so it is greater than that leaves these so this would be 1 0 and 4 ones right so greater and greater and on this side we get another leaf that would be this one so we get four zeros and 0 ones now we add the split this guy again so we're going to split this along X I 1 greater than 2 and we get 2 more leaves so we get whatever this is so less let's say greater than was this one so that's three zeroes zero ones and this one of course is just you know whatever two zeros three ones so this is a decision tree and oh well well I have to say how are we going to classify the points so this this defines a tree and what we will do so we have our binary tree and now we want to minimize the error in each leaf and in particular I should say the training error that says training so it's the error on our training data and on our training data to minimize the error at each leaf so that was no that was yes okay so to minimize the error at each leaf we would just take in this case for since this is classification we would just take a majority vote so any point any new point we get that falls down this path and end up at this leaf we would classify as a 1 so let's say so if we got a point here so let me draw a point let me pick a more interesting one let's see so say we get say we get a new point here something like that so this so this new point so this is so they say this is our X so this is X so we come down here we say okay it's greater than 1 so the first split tells us to go down this path so we go down here now is X I 2 greater than 1 and no it's not right so we go down here now we're at this look at this node and we say is X I 1 greater than 2 and yes it is so we're down here and we classify this according to the majority boat which says make it a zero so that would be classified as a zero a new test point and if we got a point here we would do the majority vote again so maybe this point here we would say it's a 1 and so on so you get the idea so the class the classes that we predict so the function that we predict so we we constructed to this and this defines a function from X's or points X to Y's that's constant on each of these",
            "videoid": "p17C9q2M00Q",
            "viewCount": "91978"
        },
        "tNa99PG8hR8": {
            "caption_exist": "T",
            "channel_id": "UC_x5XG1OV2P6uZZ5FSM9Ttw",
            "channel_title": "Google Developers",
            "concepts": [
                [
                    "tree",
                    17
                ],
                [
                    "type",
                    4
                ],
                [
                    "work",
                    3
                ],
                [
                    "target",
                    3
                ],
                [
                    "decision tree",
                    3
                ],
                [
                    "name",
                    2
                ],
                [
                    "accuracy",
                    1
                ],
                [
                    "walk",
                    1
                ],
                [
                    "species",
                    1
                ]
            ],
            "description": "Last episode, we treated our Decision Tree as a blackbox. In this episode, we'll build one on a real dataset, add code to visualize it, and practice reading it - so you can see how it works under the hood. And hey -- I may have gone a little fast through some parts. Just let me know, I'll slow down. Also: we'll do a Q&A episode down the road, so if anything is unclear, just ask!\n\nFollow https://twitter.com/random_forests for updates on new episodes!\n\nSubscribe to the Google Developers: http://goo.gl/mQyv5L - \nSubscribe to the brand new Firebase Channel: https://goo.gl/9giPHG\nAnd here's our playlist: https://goo.gl/KewA03",
            "dislikeCount": "78",
            "duration": "PT6M42S",
            "likeCount": "6238",
            "published_time": "2016-04-13T23:14:06.000Z",
            "tags": [
                "machine learning",
                "machine learning recipes",
                "classifiers",
                "decision tree",
                "Iris dataset",
                "decision tree analysis",
                "scikit learn",
                "sklearn",
                "test data",
                "train a classifier",
                "visualizing a decision tree",
                "classifier machine learning",
                "test data management",
                "Google",
                "developers",
                "product: web",
                "Fullname: Josh Gordon",
                "Location: NYC",
                "Other: NoGreenScreen",
                "GDS: Full Production",
                "Type: Other",
                "Team: Scalable Advocacy",
                "machine learning tutorial"
            ],
            "thumbnail": "https://i.ytimg.com/vi/tNa99PG8hR8/hqdefault.jpg",
            "title": "Visualizing a Decision Tree - Machine Learning Recipes #2",
            "transcript": " [Music]  In the last episode we used the decision tree as our own.  Today we will add a code to visualize  So we can see how it works when applied.  There are many types of workbooks  You may have heard before - things like nerve networks  Or vector support devices.  So why do we use the decision tree at the beginning?  Well, it has a very unique property -  They are easy to read and understand  In fact, it is one of the few models that can be explained,  Where you can fully understand why the workbook is made  Decision.  This is amazingly useful in practice.  Let's start by offering you  A real data set will work with it today.  They are called lily.  Tulips are the problem of traditional machine learning.  By them, you want to determine what the flower is  Depending on different metrics,  Such as the length and width of the petal.  The data set includes three different types of flowers.  All kinds of lily - cytos, embossed,  The Verginica.  Scroll down and you can see  Example 50 shows each type, Total Permission 150 example.  He noted that there were four features  Used to describe each example.  There is the length and width of the cypal and the pebble.  Just like the problem of our apples and oranges,  The first four feature columns and the last column appear  The labels appear, which is the type of flower in each row.  Our goal is to use this data set to train the workbook.  Then we can use this workbook to predict type  The flower that we have if we were given a new flower  We have never seen it before.  It is a good skill to know how to work with  The existing data set, so let's get the tulips to scikit-learn  And see what it looks like in your code.  Properly, friendly people  Provide a set of sample data sets  Includes lily, plus aid  To make it easy to recover.  We can retrieve the lily inside the code so.  The dataset contains two tables  From Wikipedia in addition to some metadata.  Metadata tells you the attribute names  And the names of different species of flowers.  The features and examples are the same  Are contained in variable data.  For example, if you type the first entry,  You can see the measurements of this flower.  This is an indication of attribute names, permission, and first value  Refers to the length of the cypal, the second to its width,  And so on.  The target variable contains the labels.  Also, these refer to the names of the targets.  Let's print the first one.  Zero is called Setosa.  If you look at the Wikipedia table,  You will notice that we only printed the first row.  Now both the data variables and the target have 150 entries.  If you wish, it can be replicated  So that the entire set of data is printed like this.  Now we know how to work with the data set  So we are ready to train the work.  But before we do that, we first need to divide the data.  I am going to remove many examples  I set it aside for later.  We will call the examples you set aside test data  We will keep them separate from training data,  Later we will use test examples  To test the accuracy of the workbook  With data you have not seen before.  Testing is actually a really important part  For the quality of automated education in practice  We will cover this in more detail in one of the next episodes.  For this exercise only, I will remove one example  Of each flower type.  While this is happening, has been requested  The data set is therefore the first cytosic at zero index,  The first varnish at 50, and so on.  The wording seems a bit complicated, but all I do  Is to remove three input variables from the target data.  Then two new sets of variables will be created - one  For training and one for testing.  Training will contain the majority of data,  The test will contain the examples you removed.  Now, as before, we can create a decision tree  Training on our training data.  Before we imagine it, let's use the tree classifier  To classify test data.  We know that we have a flower of every kind,  And that we can print the names we desire.  Now let's see what the tree expects.  We will give it the characteristics of the test data.  We will re-name.  You can see that the expected names match the test data.  That means getting them all right.  Now, keep in mind that this was a very simple test,  We will go through more details as we walk our way.  Now let's imagine the tree so we can see  How the workbook works.  To do so, I will copy and paste  Some code from educational courses,  And because this code is visualized  The concepts of machine learning,  I will not cover the details here.  Note that I collect the code from these two examples  To create an easy-to-read PDF version.  I can run the document and open the PDF,  We can see the tree.  To use it to classify data, start reading from above.  Each question box asks a choice between yes and no  About a feature.  For example, ask this box about whether  The petal width is less than 0.8 centimeters.  If this is true for the example you are categorizing, turn left.  Otherwise, turn right.  Now let's use this tree to classify an example  Of test data.  Here are the attributes and name of the first flower test.  Remember, you can find feature names  Given the metadata.  We know that this flower is Setosa, so let's see  What the tree expects.  I will adjust the size of the windows to make them easier to see.  The first question asked by the tree  It is about whether the petal width is less than 0.8 centimeters.  This is the fourth attribute.  The answer is correct, so we go to the left.  To this extent, we are already at the cards box.  There are no other questions to ask,  So the tree gives us the expectation, the cytos,  This is true.  Note that the name is zero, which indicates the type of flower.  Now let's try a second example of the test.  This flower is varicose.  Let's see what the tree expects.  Again we read from the top, this time the display petal  Greater than 0.8 centimeters.  Answer the wrong tree question,  So we'll turn right  The second question the tree asks is whether it is  The petal width is less than 1.75.  Trying to narrow it.  This is true, so we go left.  Now ask for whether the petal length is less than 4.95.  This is true, so we turn left again.  Finally the tree asks if it is  The petal width is less than 1.65.  That's right, if we turn left.  Now we have the expectation - it's the variant,  This is true again.  You can try another one yourself as a workout.  Remember, the way we use the tree  Is the same way that it works in code.  That explains your perception and quick reading  For the tree of decisions.  There is so much to learn here  Especially how to automatically build examples.  We'll get to it in one next episode.  But for now, let's finish at a necessary point.  Every question asked by trees must be about one  Of features.  That means that the more features they have, the better the tree  Which they adopt best.  The next episode will start to look  To what makes a good feature.  Thank you very much for watching, and see you next time. ",
            "videoid": "tNa99PG8hR8",
            "viewCount": "545658"
        },
        "w4MnOA14pYs": {
            "caption_exist": "T",
            "channel_id": "UC_n0_PEKnH_gjJvro0ihBNg",
            "channel_title": "Anders Munk-Nielsen",
            "concepts": [
                [
                    "tree",
                    9
                ],
                [
                    "mean",
                    4
                ],
                [
                    "depth",
                    3
                ],
                [
                    "algorithm",
                    3
                ],
                [
                    "node",
                    2
                ],
                [
                    "leaf",
                    2
                ],
                [
                    "one-dimensional",
                    1
                ],
                [
                    "iteration",
                    1
                ],
                [
                    "subset",
                    1
                ],
                [
                    "greedy algorithm",
                    1
                ],
                [
                    "graph",
                    1
                ]
            ],
            "description": "I discuss Regression Trees. This is a non-parametric estimation method, where the predicted values are constant over \"regions\" of x_i. These regions are chosen in a clever way known as \"recursive binary splitting\", which is a greedy algorithm for finding the optimal way of partitioning the space of possible values of x_i.",
            "dislikeCount": "2",
            "duration": "PT11M23S",
            "likeCount": "61",
            "published_time": "2016-11-21T16:55:45.000Z",
            "tags": [
                "Econometrics",
                "Regression Tree",
                "Recursive binary splitting"
            ],
            "thumbnail": "https://i.ytimg.com/vi/w4MnOA14pYs/hqdefault.jpg",
            "title": "Lecture 21: Regression Trees",
            "transcript": "  all right so what is a regression tree firstly there are two types of trees either regression trees which is when the outcome is continuous or classification trees which is when the outcome is discrete so when it falls in a finite number of groups I'm going to talk about regression trees now and and note that it's not that we have OLS going on in each of these regression tree is just a name and it refers to the fact that Y now comes continuous the model for a regression tree is that the outcome Y is some function of X H naught and an error term in this error term is uncorrelated with the exes and iid so our interest of course in is in estimating this non function here without imposing any restrictions on it whatsoever completely nonparametric thing the first idea you might get that could inspire how regression trees work is if you think of just creating dummies for the deciles of X I and then within each of these deciles you just your prediction is just the average value of x in that bin and that gives you this graph and that's actually a first good idea for it gives you the intuition for how a regression tree works so we've petitioned the x-axis based on that into deciles so there are 10 exactly 10% of the observations within each of these bins where the function jumps and then within each of them our prediction of the true function is just the average of the observations in that bin so that's a first guess and regression trees then instead say suppose we didn't put these X values at the deciles but suppose that we want to put them in different ways that's what it's all about and that's going to get a little bit more complicated to understand what's going on and we're into binary splitting but you can think of it as all it is is we're trying to put the instead of having the X the values where the function jumps be on the deciles of the of the distribution of X we want to see if we can do better and and this becomes really important if we have malta mentions than one I'll get back to that so if in the you know dimensional case using the des \u00eeles might actually work just perfectly but when we have many many many different regressors when K is large then that's a very bad idea I'll get back to one so in general you can think of the problem is choosing regions RM so that you can minimize the within region so summing over all of the individuals in region M dividing by the number of observations in region in the Y minus the average wise the squared error around the mean and in this case the regions are M is that's the first region here then the second region is here and so forth see that's the general but infeasible problem if you don't put any restrictions on how the regions are then they can be arbitrarily complicated you can say that one region is this X and this X an interval here and an interval here and there's an infinitely vagine' infinitely many ways of doing that in fact if you have 50 observations and you want to create 5 groups then there are two million possible ways of doing that of putting those 50 observations into groups so there so so this problem is is extremely hot even with a low number of of regions and we are actually also going to want to choose the number of regions in at Eveleigh so so that's that problem is we're going to have to put some restrictions on how we choose the regions and we don't want them to just be the deciles so to avoid searching over all possible groups we search on only hypercubes which can be found using what's called recursive binary splits and let's see what that is this is an example of a recursive binary split where you have xi1 xi2 and then there's an X I three that isn't used and excite for the first thing we do is we look at X iPhone and then we split into two samples either xi4 was greater than 0.7 or smaller than 0.7 or greater than 0.7 then in these sub sangil's here we split in this subset when we split on whether x i1 is smaller than 0.3 or larger than 0.3 and this results in region 1 region 2 and then similarly over here where we split on x i2 and point 6 that results in region 3 and region 4 and within each of these regions they then uniquely all observations belong to one region but only one so it petitions the set of values that X can have and then within each of these regions we just have the average Y of the observations that fall in that region and that's our prediction so this is a recursive binary splitting we can do this graphic geometrically as well here you have X 1 on this axis this is a picture from hasty tips Iranian treatment from the textbook and X 2 here so we start by saying is x1 greater or smaller than t1 and if it's smaller than t1 we say is X 2 greater or smaller than t2 and then that ends us up in these final regions here if X 1 is between t1 and t3 then we hit there's no split on X 2 all observations in that category fall in region 3 and similarly here this has split the outcome outcome space of x1 and x2 into five distinct regions it's a partition of the outcome space how do we do how's this done actually then well eating each iteration of the regression tree algorithm for all of the variables XK we found find the best cutoff point s such that if we split the sample into then the mean square error of the sample below s plus the mean squared error of the sample above s needs to be minimized so we minimize with respect to s in other words we look at once we're in this sub sample here for example we vary T 2 and we find the exact point that's here where it minimizes the mean squared errors within these two regions once we've done this for all of the variables then we pick the variable K that gave us the best improvement and means where error so at each step we consider that we could actually be splitting X 1 here and we find out that if we want to split X 1 then we should do it here if we want to split extreme then we should do it here and then we pick the best of the two and then we stop when any further steps will give us 2 little game in means whatever so we don't keep going forever we have some tolerance this is what's called a greedy algorithm because and we do it in this stepwise iterative way so we were not aren t to find the best possible way of splitting the sample but we're guaranteed of finding a way of splitting the sample that will probably be pretty good and then their ways of forcing the algorithm to keep going to have a reach a minimum depth in the tree the depth and mystery is 1/2 so you force it to keep going and then that way if there are strong interaction effects you're most likely going to find them here's the one-dimensional regression tree example at default parameters in MATLAB so you can see it's set a split here and here and here and now it's setting a lot of splits so it looks perhaps a little bit too jittery that's the default parameters so one of the parameters that you have to choose is this minimum depth and the minimum number also the minimum number of observations that you want in any node so in any of the regions you can tell it I don't want you to keep going if you get to fewer than five observations in one of the Leafs it's called the end nodes and if you do that it helps this example here split the tree for that example here looks like this and probably in some of these Leafs the end nodes they're very few observations so as I said we can improve the fit we can always get a better fit if we just keep going until there's one observation in all of the needs then we'll have a hundred percent fit in sample so we need to cross validate and then pick the number of slip the maximum number of splits or the minimum number of observations in a leaf or in node these are the hyper parameters of the regression tree in the same way that the penalty parameter lambda was a Hamburg parameter for lasso and Ridge and the way that you set them in MATLAB is you call this fit r3 which returns RF which is an object and one of the methods you can call on this object is called predict and you can evaluate it at some Z values not necessarily the same that you estimated it on and then you can just give it these options maximum splits or min leaf size and that sets these type of parameters and then how do you choose to have a parameters you use k-fold cross-validation a default is k equals to 10 and and actually if you keep running the cross-validation and you choose a different set of crossover that you can get a different criterion value so so it actually changes because you pick different folds so the criterion value can move around a little bit so there's M so you have to think about that this is how we call get that the cross valve and then the K fold loss for that so here you can compare them and here's two where I've set these maxfield equal five-minute equal to five the yellow line you can see goes through all the data points a little bit better and it has much simpler regression trees it's more parsimonious model it's less jittery",
            "videoid": "w4MnOA14pYs",
            "viewCount": "9740"
        }
    }
}