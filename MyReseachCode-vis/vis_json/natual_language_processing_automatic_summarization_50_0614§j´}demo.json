{
    "concept_relationship": {
        "links": [
            {
                "prerequisite": null,
                "similarity": 1.0,
                "source": 0,
                "target": 0
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 0,
                "target": 1
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 0,
                "target": 2
            },
            {
                "prerequisite": 2.477345471035656,
                "similarity": 0.6035533905932737,
                "source": 3,
                "target": 0
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 0,
                "target": 4
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 0,
                "target": 5
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 0,
                "target": 6
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 0,
                "target": 7
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 0,
                "target": 8
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 0,
                "target": 9
            },
            {
                "prerequisite": null,
                "similarity": 0.1889822365046136,
                "source": 0,
                "target": 10
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 0,
                "target": 11
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 0,
                "target": 12
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 0,
                "target": 13
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 0,
                "target": 14
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 0,
                "target": 15
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 0,
                "target": 16
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 0,
                "target": 17
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 0,
                "target": 18
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 0,
                "target": 19
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 0,
                "target": 20
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 0,
                "target": 21
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 0,
                "target": 22
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 0,
                "target": 23
            },
            {
                "prerequisite": null,
                "similarity": 0.22360679774997896,
                "source": 0,
                "target": 24
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 0,
                "target": 25
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 0,
                "target": 26
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 0,
                "target": 27
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 0,
                "target": 28
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 1,
                "target": 0
            },
            {
                "prerequisite": null,
                "similarity": 1.0,
                "source": 1,
                "target": 1
            },
            {
                "prerequisite": null,
                "similarity": 0.09128709291752769,
                "source": 1,
                "target": 2
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 1,
                "target": 3
            },
            {
                "prerequisite": null,
                "similarity": 0.45412414523193156,
                "source": 1,
                "target": 4
            },
            {
                "prerequisite": null,
                "similarity": 0.18257418583505539,
                "source": 1,
                "target": 5
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 1,
                "target": 6
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 1,
                "target": 7
            },
            {
                "prerequisite": null,
                "similarity": 0.5,
                "source": 1,
                "target": 8
            },
            {
                "prerequisite": null,
                "similarity": 0.39433756729740643,
                "source": 1,
                "target": 9
            },
            {
                "prerequisite": 0.17263112833393443,
                "similarity": 0.5586066999241839,
                "source": 10,
                "target": 1
            },
            {
                "prerequisite": null,
                "similarity": 0.5,
                "source": 1,
                "target": 11
            },
            {
                "prerequisite": 2.357465032981792,
                "similarity": 0.5586066999241839,
                "source": 12,
                "target": 1
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 1,
                "target": 13
            },
            {
                "prerequisite": null,
                "similarity": 0.14433756729740646,
                "source": 1,
                "target": 14
            },
            {
                "prerequisite": null,
                "similarity": 0.2357022603955159,
                "source": 1,
                "target": 15
            },
            {
                "prerequisite": null,
                "similarity": 0.14433756729740646,
                "source": 1,
                "target": 16
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 1,
                "target": 17
            },
            {
                "prerequisite": null,
                "similarity": 0.20412414523193154,
                "source": 1,
                "target": 18
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 1,
                "target": 19
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 1,
                "target": 20
            },
            {
                "prerequisite": null,
                "similarity": 0.20412414523193154,
                "source": 1,
                "target": 21
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 1,
                "target": 22
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 1,
                "target": 23
            },
            {
                "prerequisite": null,
                "similarity": 0.18257418583505539,
                "source": 1,
                "target": 24
            },
            {
                "prerequisite": null,
                "similarity": 0.3520620726159658,
                "source": 1,
                "target": 25
            },
            {
                "prerequisite": null,
                "similarity": 0.14433756729740646,
                "source": 1,
                "target": 26
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 1,
                "target": 27
            },
            {
                "prerequisite": null,
                "similarity": 0.39433756729740643,
                "source": 1,
                "target": 28
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 2,
                "target": 0
            },
            {
                "prerequisite": null,
                "similarity": 0.09128709291752769,
                "source": 2,
                "target": 1
            },
            {
                "prerequisite": null,
                "similarity": 1.0,
                "source": 2,
                "target": 2
            },
            {
                "prerequisite": 1.2547263300413514,
                "similarity": 0.658113883008419,
                "source": 2,
                "target": 3
            },
            {
                "prerequisite": null,
                "similarity": 0.5,
                "source": 2,
                "target": 4
            },
            {
                "prerequisite": 1.6509560659704965,
                "similarity": 0.6499999999999999,
                "source": 2,
                "target": 5
            },
            {
                "prerequisite": null,
                "similarity": 0.22360679774997896,
                "source": 2,
                "target": 6
            },
            {
                "prerequisite": null,
                "similarity": 0.25819888974716115,
                "source": 2,
                "target": 7
            },
            {
                "prerequisite": 1.5215536984136746,
                "similarity": 0.5238612787525831,
                "source": 8,
                "target": 2
            },
            {
                "prerequisite": null,
                "similarity": 0.15811388300841894,
                "source": 2,
                "target": 9
            },
            {
                "prerequisite": null,
                "similarity": 0.253546276418555,
                "source": 2,
                "target": 10
            },
            {
                "prerequisite": null,
                "similarity": 0.22360679774997896,
                "source": 2,
                "target": 11
            },
            {
                "prerequisite": null,
                "similarity": 0.253546276418555,
                "source": 2,
                "target": 12
            },
            {
                "prerequisite": null,
                "similarity": 0.25819888974716115,
                "source": 2,
                "target": 13
            },
            {
                "prerequisite": null,
                "similarity": 0.15811388300841894,
                "source": 2,
                "target": 14
            },
            {
                "prerequisite": 3.6855415510979173,
                "similarity": 0.5081988897471612,
                "source": 2,
                "target": 15
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 2,
                "target": 16
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 2,
                "target": 17
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 2,
                "target": 18
            },
            {
                "prerequisite": null,
                "similarity": 0.47360679774997894,
                "source": 2,
                "target": 19
            },
            {
                "prerequisite": null,
                "similarity": 0.15811388300841894,
                "source": 2,
                "target": 20
            },
            {
                "prerequisite": null,
                "similarity": 0.11180339887498948,
                "source": 2,
                "target": 21
            },
            {
                "prerequisite": null,
                "similarity": 0.22360679774997896,
                "source": 2,
                "target": 22
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 2,
                "target": 23
            },
            {
                "prerequisite": null,
                "similarity": 0.44999999999999996,
                "source": 2,
                "target": 24
            },
            {
                "prerequisite": 1.5579190175773034,
                "similarity": 0.5854101966249685,
                "source": 2,
                "target": 25
            },
            {
                "prerequisite": null,
                "similarity": 0.15811388300841894,
                "source": 2,
                "target": 26
            },
            {
                "prerequisite": null,
                "similarity": 0.22360679774997896,
                "source": 2,
                "target": 27
            },
            {
                "prerequisite": null,
                "similarity": 0.15811388300841894,
                "source": 2,
                "target": 28
            },
            {
                "prerequisite": null,
                "similarity": 0.6035533905932737,
                "source": 3,
                "target": 0
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 3,
                "target": 1
            },
            {
                "prerequisite": null,
                "similarity": 0.658113883008419,
                "source": 3,
                "target": 2
            },
            {
                "prerequisite": null,
                "similarity": 0.9999999999999999,
                "source": 3,
                "target": 3
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 3,
                "target": 4
            },
            {
                "prerequisite": null,
                "similarity": 0.40811388300841894,
                "source": 3,
                "target": 5
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 3,
                "target": 6
            },
            {
                "prerequisite": null,
                "similarity": 0.2041241452319315,
                "source": 3,
                "target": 7
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 3,
                "target": 8
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 3,
                "target": 9
            },
            {
                "prerequisite": null,
                "similarity": 0.38363062095621214,
                "source": 3,
                "target": 10
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 3,
                "target": 11
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 3,
                "target": 12
            },
            {
                "prerequisite": null,
                "similarity": 0.2041241452319315,
                "source": 3,
                "target": 13
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 3,
                "target": 14
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 3,
                "target": 15
            },
            {
                "prerequisite": null,
                "similarity": 0.5,
                "source": 3,
                "target": 16
            },
            {
                "prerequisite": null,
                "similarity": 0.5,
                "source": 3,
                "target": 17
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 3,
                "target": 18
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 3,
                "target": 19
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 3,
                "target": 20
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 3,
                "target": 21
            },
            {
                "prerequisite": 2.635452412360004,
                "similarity": 0.6035533905932737,
                "source": 3,
                "target": 22
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 3,
                "target": 23
            },
            {
                "prerequisite": null,
                "similarity": 0.40811388300841894,
                "source": 3,
                "target": 24
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 3,
                "target": 25
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 3,
                "target": 26
            },
            {
                "prerequisite": 1.61859546993453,
                "similarity": 0.6035533905932737,
                "source": 3,
                "target": 27
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 3,
                "target": 28
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 4,
                "target": 0
            },
            {
                "prerequisite": null,
                "similarity": 0.45412414523193156,
                "source": 4,
                "target": 1
            },
            {
                "prerequisite": null,
                "similarity": 0.5,
                "source": 4,
                "target": 2
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 4,
                "target": 3
            },
            {
                "prerequisite": null,
                "similarity": 1.0,
                "source": 4,
                "target": 4
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 4,
                "target": 5
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 4,
                "target": 6
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 4,
                "target": 7
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 4,
                "target": 8
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 4,
                "target": 9
            },
            {
                "prerequisite": 3.1602290066486396,
                "similarity": 0.6889822365046137,
                "source": 10,
                "target": 4
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 4,
                "target": 11
            },
            {
                "prerequisite": null,
                "similarity": 0.4389822365046136,
                "source": 4,
                "target": 12
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 4,
                "target": 13
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 4,
                "target": 14
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 4,
                "target": 15
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 4,
                "target": 16
            },
            {
                "prerequisite": null,
                "similarity": 0.5,
                "source": 4,
                "target": 17
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 4,
                "target": 18
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 4,
                "target": 19
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 4,
                "target": 20
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 4,
                "target": 21
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 4,
                "target": 22
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 4,
                "target": 23
            },
            {
                "prerequisite": 3.5861242498061774,
                "similarity": 0.7236067977499789,
                "source": 24,
                "target": 4
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 4,
                "target": 25
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 4,
                "target": 26
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 4,
                "target": 27
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 4,
                "target": 28
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 5,
                "target": 0
            },
            {
                "prerequisite": null,
                "similarity": 0.18257418583505539,
                "source": 5,
                "target": 1
            },
            {
                "prerequisite": null,
                "similarity": 0.6499999999999999,
                "source": 5,
                "target": 2
            },
            {
                "prerequisite": null,
                "similarity": 0.40811388300841894,
                "source": 5,
                "target": 3
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 5,
                "target": 4
            },
            {
                "prerequisite": null,
                "similarity": 1.0,
                "source": 5,
                "target": 5
            },
            {
                "prerequisite": null,
                "similarity": 0.22360679774997896,
                "source": 5,
                "target": 6
            },
            {
                "prerequisite": 0.057815558528613475,
                "similarity": 0.5081988897471612,
                "source": 7,
                "target": 5
            },
            {
                "prerequisite": null,
                "similarity": 0.43257418583505536,
                "source": 5,
                "target": 8
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 5,
                "target": 9
            },
            {
                "prerequisite": null,
                "similarity": 0.4190308509457033,
                "source": 5,
                "target": 10
            },
            {
                "prerequisite": null,
                "similarity": 0.22360679774997896,
                "source": 5,
                "target": 11
            },
            {
                "prerequisite": 1.400670539042416,
                "similarity": 0.503546276418555,
                "source": 12,
                "target": 5
            },
            {
                "prerequisite": null,
                "similarity": 0.25819888974716115,
                "source": 5,
                "target": 13
            },
            {
                "prerequisite": null,
                "similarity": 0.40811388300841894,
                "source": 5,
                "target": 14
            },
            {
                "prerequisite": 1.0174848362249487,
                "similarity": 0.5081988897471612,
                "source": 5,
                "target": 15
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 5,
                "target": 16
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 5,
                "target": 17
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 5,
                "target": 18
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 5,
                "target": 19
            },
            {
                "prerequisite": null,
                "similarity": 0.15811388300841894,
                "source": 5,
                "target": 20
            },
            {
                "prerequisite": null,
                "similarity": 0.11180339887498948,
                "source": 5,
                "target": 21
            },
            {
                "prerequisite": null,
                "similarity": 0.47360679774997894,
                "source": 5,
                "target": 22
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 5,
                "target": 23
            },
            {
                "prerequisite": null,
                "similarity": 0.44999999999999996,
                "source": 5,
                "target": 24
            },
            {
                "prerequisite": 0.5385496481745117,
                "similarity": 0.5854101966249685,
                "source": 25,
                "target": 5
            },
            {
                "prerequisite": null,
                "similarity": 0.40811388300841894,
                "source": 5,
                "target": 26
            },
            {
                "prerequisite": null,
                "similarity": 0.47360679774997894,
                "source": 5,
                "target": 27
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 5,
                "target": 28
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 6,
                "target": 0
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 6,
                "target": 1
            },
            {
                "prerequisite": null,
                "similarity": 0.22360679774997896,
                "source": 6,
                "target": 2
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 6,
                "target": 3
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 6,
                "target": 4
            },
            {
                "prerequisite": null,
                "similarity": 0.22360679774997896,
                "source": 6,
                "target": 5
            },
            {
                "prerequisite": null,
                "similarity": 1.0,
                "source": 6,
                "target": 6
            },
            {
                "prerequisite": null,
                "similarity": 0.2886751345948129,
                "source": 6,
                "target": 7
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 6,
                "target": 8
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 6,
                "target": 9
            },
            {
                "prerequisite": null,
                "similarity": 0.1889822365046136,
                "source": 6,
                "target": 10
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 6,
                "target": 11
            },
            {
                "prerequisite": null,
                "similarity": 0.1889822365046136,
                "source": 6,
                "target": 12
            },
            {
                "prerequisite": null,
                "similarity": 0.2886751345948129,
                "source": 6,
                "target": 13
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 6,
                "target": 14
            },
            {
                "prerequisite": null,
                "similarity": 0.2886751345948129,
                "source": 6,
                "target": 15
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 6,
                "target": 16
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 6,
                "target": 17
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 6,
                "target": 18
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 6,
                "target": 19
            },
            {
                "prerequisite": null,
                "similarity": 0.35355339059327373,
                "source": 6,
                "target": 20
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 6,
                "target": 21
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 6,
                "target": 22
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 6,
                "target": 23
            },
            {
                "prerequisite": null,
                "similarity": 0.22360679774997896,
                "source": 6,
                "target": 24
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 6,
                "target": 25
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 6,
                "target": 26
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 6,
                "target": 27
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 6,
                "target": 28
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 7,
                "target": 0
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 7,
                "target": 1
            },
            {
                "prerequisite": null,
                "similarity": 0.25819888974716115,
                "source": 7,
                "target": 2
            },
            {
                "prerequisite": null,
                "similarity": 0.2041241452319315,
                "source": 7,
                "target": 3
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 7,
                "target": 4
            },
            {
                "prerequisite": null,
                "similarity": 0.5081988897471612,
                "source": 7,
                "target": 5
            },
            {
                "prerequisite": null,
                "similarity": 0.2886751345948129,
                "source": 7,
                "target": 6
            },
            {
                "prerequisite": null,
                "similarity": 1.0,
                "source": 7,
                "target": 7
            },
            {
                "prerequisite": null,
                "similarity": 0.36785113019775795,
                "source": 7,
                "target": 8
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 7,
                "target": 9
            },
            {
                "prerequisite": null,
                "similarity": 0.3591089451179962,
                "source": 7,
                "target": 10
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 7,
                "target": 11
            },
            {
                "prerequisite": null,
                "similarity": 0.46821789023599236,
                "source": 7,
                "target": 12
            },
            {
                "prerequisite": 1.4031673078659563,
                "similarity": 0.7500000000000001,
                "source": 7,
                "target": 13
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 7,
                "target": 14
            },
            {
                "prerequisite": null,
                "similarity": 0.1666666666666667,
                "source": 7,
                "target": 15
            },
            {
                "prerequisite": null,
                "similarity": 0.4541241452319315,
                "source": 7,
                "target": 16
            },
            {
                "prerequisite": 2.893367452753228,
                "similarity": 0.5386751345948129,
                "source": 7,
                "target": 17
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 7,
                "target": 18
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 7,
                "target": 19
            },
            {
                "prerequisite": null,
                "similarity": 0.408248290463863,
                "source": 7,
                "target": 20
            },
            {
                "prerequisite": null,
                "similarity": 0.14433756729740646,
                "source": 7,
                "target": 21
            },
            {
                "prerequisite": 3.057061282666519,
                "similarity": 0.5386751345948129,
                "source": 7,
                "target": 22
            },
            {
                "prerequisite": null,
                "similarity": 0.2886751345948129,
                "source": 7,
                "target": 23
            },
            {
                "prerequisite": null,
                "similarity": 0.3790994448735806,
                "source": 7,
                "target": 24
            },
            {
                "prerequisite": 0.7618360028083209,
                "similarity": 0.5386751345948129,
                "source": 7,
                "target": 25
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 7,
                "target": 26
            },
            {
                "prerequisite": 2.700522632620565,
                "similarity": 0.5386751345948129,
                "source": 7,
                "target": 27
            },
            {
                "prerequisite": null,
                "similarity": 0.5,
                "source": 7,
                "target": 28
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 8,
                "target": 0
            },
            {
                "prerequisite": null,
                "similarity": 0.5,
                "source": 8,
                "target": 1
            },
            {
                "prerequisite": null,
                "similarity": 0.5238612787525831,
                "source": 8,
                "target": 2
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 8,
                "target": 3
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 8,
                "target": 4
            },
            {
                "prerequisite": null,
                "similarity": 0.43257418583505536,
                "source": 8,
                "target": 5
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 8,
                "target": 6
            },
            {
                "prerequisite": null,
                "similarity": 0.36785113019775795,
                "source": 8,
                "target": 7
            },
            {
                "prerequisite": null,
                "similarity": 1.0,
                "source": 8,
                "target": 8
            },
            {
                "prerequisite": 4.063677710272427,
                "similarity": 0.5386751345948129,
                "source": 8,
                "target": 9
            },
            {
                "prerequisite": 0.2620471865681676,
                "similarity": 0.5586066999241839,
                "source": 10,
                "target": 8
            },
            {
                "prerequisite": null,
                "similarity": 0.45412414523193156,
                "source": 8,
                "target": 11
            },
            {
                "prerequisite": 0.4673102256640831,
                "similarity": 0.6357583749052298,
                "source": 8,
                "target": 12
            },
            {
                "prerequisite": null,
                "similarity": 0.11785113019775795,
                "source": 8,
                "target": 13
            },
            {
                "prerequisite": null,
                "similarity": 0.2886751345948129,
                "source": 8,
                "target": 14
            },
            {
                "prerequisite": null,
                "similarity": 0.4857022603955159,
                "source": 8,
                "target": 15
            },
            {
                "prerequisite": 2.4944928893265153,
                "similarity": 0.5386751345948129,
                "source": 8,
                "target": 16
            },
            {
                "prerequisite": null,
                "similarity": 0.20412414523193154,
                "source": 8,
                "target": 17
            },
            {
                "prerequisite": null,
                "similarity": 0.45412414523193156,
                "source": 8,
                "target": 18
            },
            {
                "prerequisite": null,
                "similarity": 0.45412414523193156,
                "source": 8,
                "target": 19
            },
            {
                "prerequisite": null,
                "similarity": 0.14433756729740646,
                "source": 8,
                "target": 20
            },
            {
                "prerequisite": 2.046106979842825,
                "similarity": 0.6582482904638631,
                "source": 8,
                "target": 21
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 8,
                "target": 22
            },
            {
                "prerequisite": null,
                "similarity": 0.20412414523193154,
                "source": 8,
                "target": 23
            },
            {
                "prerequisite": null,
                "similarity": 0.43257418583505536,
                "source": 8,
                "target": 24
            },
            {
                "prerequisite": 3.303077518419024,
                "similarity": 0.5561862178478973,
                "source": 8,
                "target": 25
            },
            {
                "prerequisite": 4.436671337698636,
                "similarity": 0.5386751345948129,
                "source": 8,
                "target": 26
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 8,
                "target": 27
            },
            {
                "prerequisite": null,
                "similarity": 0.2886751345948129,
                "source": 8,
                "target": 28
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 9,
                "target": 0
            },
            {
                "prerequisite": null,
                "similarity": 0.39433756729740643,
                "source": 9,
                "target": 1
            },
            {
                "prerequisite": null,
                "similarity": 0.15811388300841894,
                "source": 9,
                "target": 2
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 9,
                "target": 3
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 9,
                "target": 4
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 9,
                "target": 5
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 9,
                "target": 6
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 9,
                "target": 7
            },
            {
                "prerequisite": null,
                "similarity": 0.5386751345948129,
                "source": 9,
                "target": 8
            },
            {
                "prerequisite": null,
                "similarity": 0.9999999999999999,
                "source": 9,
                "target": 9
            },
            {
                "prerequisite": 3.8662494987363365,
                "similarity": 0.5172612419124243,
                "source": 10,
                "target": 9
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 9,
                "target": 11
            },
            {
                "prerequisite": null,
                "similarity": 0.13363062095621217,
                "source": 9,
                "target": 12
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 9,
                "target": 13
            },
            {
                "prerequisite": null,
                "similarity": 0.24999999999999994,
                "source": 9,
                "target": 14
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 9,
                "target": 15
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 9,
                "target": 16
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 9,
                "target": 17
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 9,
                "target": 18
            },
            {
                "prerequisite": null,
                "similarity": 0.35355339059327373,
                "source": 9,
                "target": 19
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 9,
                "target": 20
            },
            {
                "prerequisite": null,
                "similarity": 0.42677669529663687,
                "source": 9,
                "target": 21
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 9,
                "target": 22
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 9,
                "target": 23
            },
            {
                "prerequisite": null,
                "similarity": 0.15811388300841894,
                "source": 9,
                "target": 24
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 9,
                "target": 25
            },
            {
                "prerequisite": null,
                "similarity": 0.24999999999999994,
                "source": 9,
                "target": 26
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 9,
                "target": 27
            },
            {
                "prerequisite": null,
                "similarity": 0.24999999999999994,
                "source": 9,
                "target": 28
            },
            {
                "prerequisite": null,
                "similarity": 0.1889822365046136,
                "source": 10,
                "target": 0
            },
            {
                "prerequisite": null,
                "similarity": 0.5586066999241839,
                "source": 10,
                "target": 1
            },
            {
                "prerequisite": null,
                "similarity": 0.253546276418555,
                "source": 10,
                "target": 2
            },
            {
                "prerequisite": null,
                "similarity": 0.38363062095621214,
                "source": 10,
                "target": 3
            },
            {
                "prerequisite": null,
                "similarity": 0.6889822365046137,
                "source": 10,
                "target": 4
            },
            {
                "prerequisite": null,
                "similarity": 0.4190308509457033,
                "source": 10,
                "target": 5
            },
            {
                "prerequisite": null,
                "similarity": 0.1889822365046136,
                "source": 10,
                "target": 6
            },
            {
                "prerequisite": null,
                "similarity": 0.3591089451179962,
                "source": 10,
                "target": 7
            },
            {
                "prerequisite": null,
                "similarity": 0.5586066999241839,
                "source": 10,
                "target": 8
            },
            {
                "prerequisite": null,
                "similarity": 0.5172612419124243,
                "source": 10,
                "target": 9
            },
            {
                "prerequisite": null,
                "similarity": 0.9999999999999998,
                "source": 10,
                "target": 10
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 10,
                "target": 11
            },
            {
                "prerequisite": 0.18846443192934426,
                "similarity": 0.607142857142857,
                "source": 12,
                "target": 10
            },
            {
                "prerequisite": null,
                "similarity": 0.3591089451179962,
                "source": 10,
                "target": 13
            },
            {
                "prerequisite": 3.2236871154993514,
                "similarity": 0.6336306209562121,
                "source": 10,
                "target": 14
            },
            {
                "prerequisite": null,
                "similarity": 0.3273268353539886,
                "source": 10,
                "target": 15
            },
            {
                "prerequisite": null,
                "similarity": 0.38363062095621214,
                "source": 10,
                "target": 16
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 10,
                "target": 17
            },
            {
                "prerequisite": null,
                "similarity": 0.1889822365046136,
                "source": 10,
                "target": 18
            },
            {
                "prerequisite": null,
                "similarity": 0.4389822365046136,
                "source": 10,
                "target": 19
            },
            {
                "prerequisite": null,
                "similarity": 0.13363062095621217,
                "source": 10,
                "target": 20
            },
            {
                "prerequisite": null,
                "similarity": 0.4389822365046136,
                "source": 10,
                "target": 21
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 10,
                "target": 22
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 10,
                "target": 23
            },
            {
                "prerequisite": 0.5256278529334935,
                "similarity": 0.5880617018914066,
                "source": 10,
                "target": 24
            },
            {
                "prerequisite": null,
                "similarity": 0.4389822365046136,
                "source": 10,
                "target": 25
            },
            {
                "prerequisite": null,
                "similarity": 0.38363062095621214,
                "source": 10,
                "target": 26
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 10,
                "target": 27
            },
            {
                "prerequisite": 3.1489445096323463,
                "similarity": 0.5172612419124243,
                "source": 10,
                "target": 28
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 11,
                "target": 0
            },
            {
                "prerequisite": null,
                "similarity": 0.5,
                "source": 11,
                "target": 1
            },
            {
                "prerequisite": null,
                "similarity": 0.22360679774997896,
                "source": 11,
                "target": 2
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 11,
                "target": 3
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 11,
                "target": 4
            },
            {
                "prerequisite": null,
                "similarity": 0.22360679774997896,
                "source": 11,
                "target": 5
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 11,
                "target": 6
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 11,
                "target": 7
            },
            {
                "prerequisite": null,
                "similarity": 0.45412414523193156,
                "source": 11,
                "target": 8
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 11,
                "target": 9
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 11,
                "target": 10
            },
            {
                "prerequisite": null,
                "similarity": 1.0,
                "source": 11,
                "target": 11
            },
            {
                "prerequisite": null,
                "similarity": 0.4389822365046136,
                "source": 11,
                "target": 12
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 11,
                "target": 13
            },
            {
                "prerequisite": null,
                "similarity": 0.35355339059327373,
                "source": 11,
                "target": 14
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 11,
                "target": 15
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 11,
                "target": 16
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 11,
                "target": 17
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 11,
                "target": 18
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 11,
                "target": 19
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 11,
                "target": 20
            },
            {
                "prerequisite": null,
                "similarity": 0.5,
                "source": 11,
                "target": 21
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 11,
                "target": 22
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 11,
                "target": 23
            },
            {
                "prerequisite": null,
                "similarity": 0.22360679774997896,
                "source": 11,
                "target": 24
            },
            {
                "prerequisite": null,
                "similarity": 0.5,
                "source": 11,
                "target": 25
            },
            {
                "prerequisite": null,
                "similarity": 0.35355339059327373,
                "source": 11,
                "target": 26
            },
            {
                "prerequisite": null,
                "similarity": 0.5,
                "source": 11,
                "target": 27
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 11,
                "target": 28
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 12,
                "target": 0
            },
            {
                "prerequisite": null,
                "similarity": 0.5586066999241839,
                "source": 12,
                "target": 1
            },
            {
                "prerequisite": null,
                "similarity": 0.253546276418555,
                "source": 12,
                "target": 2
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 12,
                "target": 3
            },
            {
                "prerequisite": null,
                "similarity": 0.4389822365046136,
                "source": 12,
                "target": 4
            },
            {
                "prerequisite": null,
                "similarity": 0.503546276418555,
                "source": 12,
                "target": 5
            },
            {
                "prerequisite": null,
                "similarity": 0.1889822365046136,
                "source": 12,
                "target": 6
            },
            {
                "prerequisite": null,
                "similarity": 0.46821789023599236,
                "source": 12,
                "target": 7
            },
            {
                "prerequisite": null,
                "similarity": 0.6357583749052298,
                "source": 12,
                "target": 8
            },
            {
                "prerequisite": null,
                "similarity": 0.13363062095621217,
                "source": 12,
                "target": 9
            },
            {
                "prerequisite": null,
                "similarity": 0.607142857142857,
                "source": 12,
                "target": 10
            },
            {
                "prerequisite": null,
                "similarity": 0.4389822365046136,
                "source": 12,
                "target": 11
            },
            {
                "prerequisite": null,
                "similarity": 0.9999999999999998,
                "source": 12,
                "target": 12
            },
            {
                "prerequisite": null,
                "similarity": 0.2182178902359924,
                "source": 12,
                "target": 13
            },
            {
                "prerequisite": 3.0185695641141317,
                "similarity": 0.5172612419124243,
                "source": 12,
                "target": 14
            },
            {
                "prerequisite": 4.131882941690831,
                "similarity": 0.5773268353539887,
                "source": 12,
                "target": 15
            },
            {
                "prerequisite": null,
                "similarity": 0.26726124191242434,
                "source": 12,
                "target": 16
            },
            {
                "prerequisite": null,
                "similarity": 0.4389822365046136,
                "source": 12,
                "target": 17
            },
            {
                "prerequisite": null,
                "similarity": 0.4389822365046136,
                "source": 12,
                "target": 18
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 12,
                "target": 19
            },
            {
                "prerequisite": null,
                "similarity": 0.26726124191242434,
                "source": 12,
                "target": 20
            },
            {
                "prerequisite": null,
                "similarity": 0.3779644730092272,
                "source": 12,
                "target": 21
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 12,
                "target": 22
            },
            {
                "prerequisite": null,
                "similarity": 0.1889822365046136,
                "source": 12,
                "target": 23
            },
            {
                "prerequisite": 2.071214946065679,
                "similarity": 0.8380617018914066,
                "source": 12,
                "target": 24
            },
            {
                "prerequisite": 0.9614192271621756,
                "similarity": 0.6279644730092272,
                "source": 12,
                "target": 25
            },
            {
                "prerequisite": 3.238809307843817,
                "similarity": 0.5172612419124243,
                "source": 12,
                "target": 26
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 12,
                "target": 27
            },
            {
                "prerequisite": null,
                "similarity": 0.38363062095621214,
                "source": 12,
                "target": 28
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 13,
                "target": 0
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 13,
                "target": 1
            },
            {
                "prerequisite": null,
                "similarity": 0.25819888974716115,
                "source": 13,
                "target": 2
            },
            {
                "prerequisite": null,
                "similarity": 0.2041241452319315,
                "source": 13,
                "target": 3
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 13,
                "target": 4
            },
            {
                "prerequisite": null,
                "similarity": 0.25819888974716115,
                "source": 13,
                "target": 5
            },
            {
                "prerequisite": null,
                "similarity": 0.2886751345948129,
                "source": 13,
                "target": 6
            },
            {
                "prerequisite": null,
                "similarity": 0.7500000000000001,
                "source": 13,
                "target": 7
            },
            {
                "prerequisite": null,
                "similarity": 0.11785113019775795,
                "source": 13,
                "target": 8
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 13,
                "target": 9
            },
            {
                "prerequisite": null,
                "similarity": 0.3591089451179962,
                "source": 13,
                "target": 10
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 13,
                "target": 11
            },
            {
                "prerequisite": null,
                "similarity": 0.2182178902359924,
                "source": 13,
                "target": 12
            },
            {
                "prerequisite": null,
                "similarity": 1.0,
                "source": 13,
                "target": 13
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 13,
                "target": 14
            },
            {
                "prerequisite": null,
                "similarity": 0.1666666666666667,
                "source": 13,
                "target": 15
            },
            {
                "prerequisite": null,
                "similarity": 0.2041241452319315,
                "source": 13,
                "target": 16
            },
            {
                "prerequisite": 1.0076896602345666,
                "similarity": 0.5386751345948129,
                "source": 13,
                "target": 17
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 13,
                "target": 18
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 13,
                "target": 19
            },
            {
                "prerequisite": null,
                "similarity": 0.408248290463863,
                "source": 13,
                "target": 20
            },
            {
                "prerequisite": null,
                "similarity": 0.14433756729740646,
                "source": 13,
                "target": 21
            },
            {
                "prerequisite": null,
                "similarity": 0.2886751345948129,
                "source": 13,
                "target": 22
            },
            {
                "prerequisite": null,
                "similarity": 0.2886751345948129,
                "source": 13,
                "target": 23
            },
            {
                "prerequisite": null,
                "similarity": 0.12909944487358058,
                "source": 13,
                "target": 24
            },
            {
                "prerequisite": 0.38043302056223405,
                "similarity": 0.5386751345948129,
                "source": 13,
                "target": 25
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 13,
                "target": 26
            },
            {
                "prerequisite": null,
                "similarity": 0.2886751345948129,
                "source": 13,
                "target": 27
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 13,
                "target": 28
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 14,
                "target": 0
            },
            {
                "prerequisite": null,
                "similarity": 0.14433756729740646,
                "source": 14,
                "target": 1
            },
            {
                "prerequisite": null,
                "similarity": 0.15811388300841894,
                "source": 14,
                "target": 2
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 14,
                "target": 3
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 14,
                "target": 4
            },
            {
                "prerequisite": null,
                "similarity": 0.40811388300841894,
                "source": 14,
                "target": 5
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 14,
                "target": 6
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 14,
                "target": 7
            },
            {
                "prerequisite": null,
                "similarity": 0.2886751345948129,
                "source": 14,
                "target": 8
            },
            {
                "prerequisite": null,
                "similarity": 0.24999999999999994,
                "source": 14,
                "target": 9
            },
            {
                "prerequisite": null,
                "similarity": 0.6336306209562121,
                "source": 14,
                "target": 10
            },
            {
                "prerequisite": null,
                "similarity": 0.35355339059327373,
                "source": 14,
                "target": 11
            },
            {
                "prerequisite": null,
                "similarity": 0.5172612419124243,
                "source": 14,
                "target": 12
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 14,
                "target": 13
            },
            {
                "prerequisite": null,
                "similarity": 0.9999999999999999,
                "source": 14,
                "target": 14
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 14,
                "target": 15
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 14,
                "target": 16
            },
            {
                "prerequisite": null,
                "similarity": 0.5,
                "source": 14,
                "target": 17
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 14,
                "target": 18
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 14,
                "target": 19
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 14,
                "target": 20
            },
            {
                "prerequisite": null,
                "similarity": 0.35355339059327373,
                "source": 14,
                "target": 21
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 14,
                "target": 22
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 14,
                "target": 23
            },
            {
                "prerequisite": 2.6374336409561114,
                "similarity": 0.5662277660168379,
                "source": 24,
                "target": 14
            },
            {
                "prerequisite": null,
                "similarity": 0.17677669529663687,
                "source": 14,
                "target": 25
            },
            {
                "prerequisite": null,
                "similarity": 0.4999999999999999,
                "source": 14,
                "target": 26
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 14,
                "target": 27
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 14,
                "target": 28
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 15,
                "target": 0
            },
            {
                "prerequisite": null,
                "similarity": 0.2357022603955159,
                "source": 15,
                "target": 1
            },
            {
                "prerequisite": null,
                "similarity": 0.5081988897471612,
                "source": 15,
                "target": 2
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 15,
                "target": 3
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 15,
                "target": 4
            },
            {
                "prerequisite": null,
                "similarity": 0.5081988897471612,
                "source": 15,
                "target": 5
            },
            {
                "prerequisite": null,
                "similarity": 0.2886751345948129,
                "source": 15,
                "target": 6
            },
            {
                "prerequisite": null,
                "similarity": 0.1666666666666667,
                "source": 15,
                "target": 7
            },
            {
                "prerequisite": null,
                "similarity": 0.4857022603955159,
                "source": 15,
                "target": 8
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 15,
                "target": 9
            },
            {
                "prerequisite": null,
                "similarity": 0.3273268353539886,
                "source": 15,
                "target": 10
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 15,
                "target": 11
            },
            {
                "prerequisite": null,
                "similarity": 0.5773268353539887,
                "source": 15,
                "target": 12
            },
            {
                "prerequisite": null,
                "similarity": 0.1666666666666667,
                "source": 15,
                "target": 13
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 15,
                "target": 14
            },
            {
                "prerequisite": null,
                "similarity": 1.0,
                "source": 15,
                "target": 15
            },
            {
                "prerequisite": null,
                "similarity": 0.2041241452319315,
                "source": 15,
                "target": 16
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 15,
                "target": 17
            },
            {
                "prerequisite": null,
                "similarity": 0.2886751345948129,
                "source": 15,
                "target": 18
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 15,
                "target": 19
            },
            {
                "prerequisite": null,
                "similarity": 0.2041241452319315,
                "source": 15,
                "target": 20
            },
            {
                "prerequisite": null,
                "similarity": 0.14433756729740646,
                "source": 15,
                "target": 21
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 15,
                "target": 22
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 15,
                "target": 23
            },
            {
                "prerequisite": null,
                "similarity": 0.12909944487358058,
                "source": 15,
                "target": 24
            },
            {
                "prerequisite": 2.581622929721473,
                "similarity": 0.5386751345948129,
                "source": 25,
                "target": 15
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 15,
                "target": 26
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 15,
                "target": 27
            },
            {
                "prerequisite": null,
                "similarity": 0.4541241452319315,
                "source": 15,
                "target": 28
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 16,
                "target": 0
            },
            {
                "prerequisite": null,
                "similarity": 0.14433756729740646,
                "source": 16,
                "target": 1
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 16,
                "target": 2
            },
            {
                "prerequisite": null,
                "similarity": 0.5,
                "source": 16,
                "target": 3
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 16,
                "target": 4
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 16,
                "target": 5
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 16,
                "target": 6
            },
            {
                "prerequisite": null,
                "similarity": 0.4541241452319315,
                "source": 16,
                "target": 7
            },
            {
                "prerequisite": null,
                "similarity": 0.5386751345948129,
                "source": 16,
                "target": 8
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 16,
                "target": 9
            },
            {
                "prerequisite": null,
                "similarity": 0.38363062095621214,
                "source": 16,
                "target": 10
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 16,
                "target": 11
            },
            {
                "prerequisite": null,
                "similarity": 0.26726124191242434,
                "source": 16,
                "target": 12
            },
            {
                "prerequisite": null,
                "similarity": 0.2041241452319315,
                "source": 16,
                "target": 13
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 16,
                "target": 14
            },
            {
                "prerequisite": null,
                "similarity": 0.2041241452319315,
                "source": 16,
                "target": 15
            },
            {
                "prerequisite": null,
                "similarity": 0.9999999999999999,
                "source": 16,
                "target": 16
            },
            {
                "prerequisite": null,
                "similarity": 0.35355339059327373,
                "source": 16,
                "target": 17
            },
            {
                "prerequisite": null,
                "similarity": 0.35355339059327373,
                "source": 16,
                "target": 18
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 16,
                "target": 19
            },
            {
                "prerequisite": null,
                "similarity": 0.24999999999999994,
                "source": 16,
                "target": 20
            },
            {
                "prerequisite": null,
                "similarity": 0.35355339059327373,
                "source": 16,
                "target": 21
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 16,
                "target": 22
            },
            {
                "prerequisite": null,
                "similarity": 0.35355339059327373,
                "source": 16,
                "target": 23
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 16,
                "target": 24
            },
            {
                "prerequisite": null,
                "similarity": 0.17677669529663687,
                "source": 16,
                "target": 25
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 16,
                "target": 26
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 16,
                "target": 27
            },
            {
                "prerequisite": null,
                "similarity": 0.49999999999999994,
                "source": 16,
                "target": 28
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 17,
                "target": 0
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 17,
                "target": 1
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 17,
                "target": 2
            },
            {
                "prerequisite": null,
                "similarity": 0.5,
                "source": 17,
                "target": 3
            },
            {
                "prerequisite": null,
                "similarity": 0.5,
                "source": 17,
                "target": 4
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 17,
                "target": 5
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 17,
                "target": 6
            },
            {
                "prerequisite": null,
                "similarity": 0.5386751345948129,
                "source": 17,
                "target": 7
            },
            {
                "prerequisite": null,
                "similarity": 0.20412414523193154,
                "source": 17,
                "target": 8
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 17,
                "target": 9
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 17,
                "target": 10
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 17,
                "target": 11
            },
            {
                "prerequisite": null,
                "similarity": 0.4389822365046136,
                "source": 17,
                "target": 12
            },
            {
                "prerequisite": null,
                "similarity": 0.5386751345948129,
                "source": 17,
                "target": 13
            },
            {
                "prerequisite": null,
                "similarity": 0.5,
                "source": 17,
                "target": 14
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 17,
                "target": 15
            },
            {
                "prerequisite": null,
                "similarity": 0.35355339059327373,
                "source": 17,
                "target": 16
            },
            {
                "prerequisite": null,
                "similarity": 1.0,
                "source": 17,
                "target": 17
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 17,
                "target": 18
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 17,
                "target": 19
            },
            {
                "prerequisite": null,
                "similarity": 0.35355339059327373,
                "source": 17,
                "target": 20
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 17,
                "target": 21
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 17,
                "target": 22
            },
            {
                "prerequisite": null,
                "similarity": 0.5,
                "source": 17,
                "target": 23
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 17,
                "target": 24
            },
            {
                "prerequisite": 2.1435591496603585,
                "similarity": 0.75,
                "source": 25,
                "target": 17
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 17,
                "target": 26
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 17,
                "target": 27
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 17,
                "target": 28
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 18,
                "target": 0
            },
            {
                "prerequisite": null,
                "similarity": 0.20412414523193154,
                "source": 18,
                "target": 1
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 18,
                "target": 2
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 18,
                "target": 3
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 18,
                "target": 4
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 18,
                "target": 5
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 18,
                "target": 6
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 18,
                "target": 7
            },
            {
                "prerequisite": null,
                "similarity": 0.45412414523193156,
                "source": 18,
                "target": 8
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 18,
                "target": 9
            },
            {
                "prerequisite": null,
                "similarity": 0.1889822365046136,
                "source": 18,
                "target": 10
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 18,
                "target": 11
            },
            {
                "prerequisite": null,
                "similarity": 0.4389822365046136,
                "source": 18,
                "target": 12
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 18,
                "target": 13
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 18,
                "target": 14
            },
            {
                "prerequisite": null,
                "similarity": 0.2886751345948129,
                "source": 18,
                "target": 15
            },
            {
                "prerequisite": null,
                "similarity": 0.35355339059327373,
                "source": 18,
                "target": 16
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 18,
                "target": 17
            },
            {
                "prerequisite": null,
                "similarity": 1.0,
                "source": 18,
                "target": 18
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 18,
                "target": 19
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 18,
                "target": 20
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 18,
                "target": 21
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 18,
                "target": 22
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 18,
                "target": 23
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 18,
                "target": 24
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 18,
                "target": 25
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 18,
                "target": 26
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 18,
                "target": 27
            },
            {
                "prerequisite": 2.3483274620724277,
                "similarity": 0.6035533905932737,
                "source": 28,
                "target": 18
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 19,
                "target": 0
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 19,
                "target": 1
            },
            {
                "prerequisite": null,
                "similarity": 0.47360679774997894,
                "source": 19,
                "target": 2
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 19,
                "target": 3
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 19,
                "target": 4
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 19,
                "target": 5
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 19,
                "target": 6
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 19,
                "target": 7
            },
            {
                "prerequisite": null,
                "similarity": 0.45412414523193156,
                "source": 19,
                "target": 8
            },
            {
                "prerequisite": null,
                "similarity": 0.35355339059327373,
                "source": 19,
                "target": 9
            },
            {
                "prerequisite": null,
                "similarity": 0.4389822365046136,
                "source": 19,
                "target": 10
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 19,
                "target": 11
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 19,
                "target": 12
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 19,
                "target": 13
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 19,
                "target": 14
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 19,
                "target": 15
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 19,
                "target": 16
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 19,
                "target": 17
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 19,
                "target": 18
            },
            {
                "prerequisite": null,
                "similarity": 1.0,
                "source": 19,
                "target": 19
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 19,
                "target": 20
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 19,
                "target": 21
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 19,
                "target": 22
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 19,
                "target": 23
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 19,
                "target": 24
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 19,
                "target": 25
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 19,
                "target": 26
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 19,
                "target": 27
            },
            {
                "prerequisite": null,
                "similarity": 0.35355339059327373,
                "source": 19,
                "target": 28
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 20,
                "target": 0
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 20,
                "target": 1
            },
            {
                "prerequisite": null,
                "similarity": 0.15811388300841894,
                "source": 20,
                "target": 2
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 20,
                "target": 3
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 20,
                "target": 4
            },
            {
                "prerequisite": null,
                "similarity": 0.15811388300841894,
                "source": 20,
                "target": 5
            },
            {
                "prerequisite": null,
                "similarity": 0.35355339059327373,
                "source": 20,
                "target": 6
            },
            {
                "prerequisite": null,
                "similarity": 0.408248290463863,
                "source": 20,
                "target": 7
            },
            {
                "prerequisite": null,
                "similarity": 0.14433756729740646,
                "source": 20,
                "target": 8
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 20,
                "target": 9
            },
            {
                "prerequisite": null,
                "similarity": 0.13363062095621217,
                "source": 20,
                "target": 10
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 20,
                "target": 11
            },
            {
                "prerequisite": null,
                "similarity": 0.26726124191242434,
                "source": 20,
                "target": 12
            },
            {
                "prerequisite": null,
                "similarity": 0.408248290463863,
                "source": 20,
                "target": 13
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 20,
                "target": 14
            },
            {
                "prerequisite": null,
                "similarity": 0.2041241452319315,
                "source": 20,
                "target": 15
            },
            {
                "prerequisite": null,
                "similarity": 0.24999999999999994,
                "source": 20,
                "target": 16
            },
            {
                "prerequisite": null,
                "similarity": 0.35355339059327373,
                "source": 20,
                "target": 17
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 20,
                "target": 18
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 20,
                "target": 19
            },
            {
                "prerequisite": null,
                "similarity": 0.9999999999999999,
                "source": 20,
                "target": 20
            },
            {
                "prerequisite": null,
                "similarity": 0.17677669529663687,
                "source": 20,
                "target": 21
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 20,
                "target": 22
            },
            {
                "prerequisite": null,
                "similarity": 0.35355339059327373,
                "source": 20,
                "target": 23
            },
            {
                "prerequisite": null,
                "similarity": 0.15811388300841894,
                "source": 20,
                "target": 24
            },
            {
                "prerequisite": null,
                "similarity": 0.35355339059327373,
                "source": 20,
                "target": 25
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 20,
                "target": 26
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 20,
                "target": 27
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 20,
                "target": 28
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 21,
                "target": 0
            },
            {
                "prerequisite": null,
                "similarity": 0.20412414523193154,
                "source": 21,
                "target": 1
            },
            {
                "prerequisite": null,
                "similarity": 0.11180339887498948,
                "source": 21,
                "target": 2
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 21,
                "target": 3
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 21,
                "target": 4
            },
            {
                "prerequisite": null,
                "similarity": 0.11180339887498948,
                "source": 21,
                "target": 5
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 21,
                "target": 6
            },
            {
                "prerequisite": null,
                "similarity": 0.14433756729740646,
                "source": 21,
                "target": 7
            },
            {
                "prerequisite": null,
                "similarity": 0.6582482904638631,
                "source": 21,
                "target": 8
            },
            {
                "prerequisite": null,
                "similarity": 0.42677669529663687,
                "source": 21,
                "target": 9
            },
            {
                "prerequisite": null,
                "similarity": 0.4389822365046136,
                "source": 21,
                "target": 10
            },
            {
                "prerequisite": null,
                "similarity": 0.5,
                "source": 21,
                "target": 11
            },
            {
                "prerequisite": null,
                "similarity": 0.3779644730092272,
                "source": 21,
                "target": 12
            },
            {
                "prerequisite": null,
                "similarity": 0.14433756729740646,
                "source": 21,
                "target": 13
            },
            {
                "prerequisite": null,
                "similarity": 0.35355339059327373,
                "source": 21,
                "target": 14
            },
            {
                "prerequisite": null,
                "similarity": 0.14433756729740646,
                "source": 21,
                "target": 15
            },
            {
                "prerequisite": null,
                "similarity": 0.35355339059327373,
                "source": 21,
                "target": 16
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 21,
                "target": 17
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 21,
                "target": 18
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 21,
                "target": 19
            },
            {
                "prerequisite": null,
                "similarity": 0.17677669529663687,
                "source": 21,
                "target": 20
            },
            {
                "prerequisite": null,
                "similarity": 1.0,
                "source": 21,
                "target": 21
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 21,
                "target": 22
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 21,
                "target": 23
            },
            {
                "prerequisite": null,
                "similarity": 0.22360679774997896,
                "source": 21,
                "target": 24
            },
            {
                "prerequisite": null,
                "similarity": 0.5,
                "source": 21,
                "target": 25
            },
            {
                "prerequisite": null,
                "similarity": 0.35355339059327373,
                "source": 21,
                "target": 26
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 21,
                "target": 27
            },
            {
                "prerequisite": null,
                "similarity": 0.17677669529663687,
                "source": 21,
                "target": 28
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 22,
                "target": 0
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 22,
                "target": 1
            },
            {
                "prerequisite": null,
                "similarity": 0.22360679774997896,
                "source": 22,
                "target": 2
            },
            {
                "prerequisite": null,
                "similarity": 0.6035533905932737,
                "source": 22,
                "target": 3
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 22,
                "target": 4
            },
            {
                "prerequisite": null,
                "similarity": 0.47360679774997894,
                "source": 22,
                "target": 5
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 22,
                "target": 6
            },
            {
                "prerequisite": null,
                "similarity": 0.5386751345948129,
                "source": 22,
                "target": 7
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 22,
                "target": 8
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 22,
                "target": 9
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 22,
                "target": 10
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 22,
                "target": 11
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 22,
                "target": 12
            },
            {
                "prerequisite": null,
                "similarity": 0.2886751345948129,
                "source": 22,
                "target": 13
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 22,
                "target": 14
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 22,
                "target": 15
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 22,
                "target": 16
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 22,
                "target": 17
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 22,
                "target": 18
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 22,
                "target": 19
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 22,
                "target": 20
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 22,
                "target": 21
            },
            {
                "prerequisite": null,
                "similarity": 1.0,
                "source": 22,
                "target": 22
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 22,
                "target": 23
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 22,
                "target": 24
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 22,
                "target": 25
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 22,
                "target": 26
            },
            {
                "prerequisite": null,
                "similarity": 0.5,
                "source": 22,
                "target": 27
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 22,
                "target": 28
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 23,
                "target": 0
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 23,
                "target": 1
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 23,
                "target": 2
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 23,
                "target": 3
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 23,
                "target": 4
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 23,
                "target": 5
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 23,
                "target": 6
            },
            {
                "prerequisite": null,
                "similarity": 0.2886751345948129,
                "source": 23,
                "target": 7
            },
            {
                "prerequisite": null,
                "similarity": 0.20412414523193154,
                "source": 23,
                "target": 8
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 23,
                "target": 9
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 23,
                "target": 10
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 23,
                "target": 11
            },
            {
                "prerequisite": null,
                "similarity": 0.1889822365046136,
                "source": 23,
                "target": 12
            },
            {
                "prerequisite": null,
                "similarity": 0.2886751345948129,
                "source": 23,
                "target": 13
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 23,
                "target": 14
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 23,
                "target": 15
            },
            {
                "prerequisite": null,
                "similarity": 0.35355339059327373,
                "source": 23,
                "target": 16
            },
            {
                "prerequisite": null,
                "similarity": 0.5,
                "source": 23,
                "target": 17
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 23,
                "target": 18
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 23,
                "target": 19
            },
            {
                "prerequisite": null,
                "similarity": 0.35355339059327373,
                "source": 23,
                "target": 20
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 23,
                "target": 21
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 23,
                "target": 22
            },
            {
                "prerequisite": null,
                "similarity": 1.0,
                "source": 23,
                "target": 23
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 23,
                "target": 24
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 23,
                "target": 25
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 23,
                "target": 26
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 23,
                "target": 27
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 23,
                "target": 28
            },
            {
                "prerequisite": null,
                "similarity": 0.22360679774997896,
                "source": 24,
                "target": 0
            },
            {
                "prerequisite": null,
                "similarity": 0.18257418583505539,
                "source": 24,
                "target": 1
            },
            {
                "prerequisite": null,
                "similarity": 0.44999999999999996,
                "source": 24,
                "target": 2
            },
            {
                "prerequisite": null,
                "similarity": 0.40811388300841894,
                "source": 24,
                "target": 3
            },
            {
                "prerequisite": null,
                "similarity": 0.7236067977499789,
                "source": 24,
                "target": 4
            },
            {
                "prerequisite": null,
                "similarity": 0.44999999999999996,
                "source": 24,
                "target": 5
            },
            {
                "prerequisite": null,
                "similarity": 0.22360679774997896,
                "source": 24,
                "target": 6
            },
            {
                "prerequisite": null,
                "similarity": 0.3790994448735806,
                "source": 24,
                "target": 7
            },
            {
                "prerequisite": null,
                "similarity": 0.43257418583505536,
                "source": 24,
                "target": 8
            },
            {
                "prerequisite": null,
                "similarity": 0.15811388300841894,
                "source": 24,
                "target": 9
            },
            {
                "prerequisite": null,
                "similarity": 0.5880617018914066,
                "source": 24,
                "target": 10
            },
            {
                "prerequisite": null,
                "similarity": 0.22360679774997896,
                "source": 24,
                "target": 11
            },
            {
                "prerequisite": null,
                "similarity": 0.8380617018914066,
                "source": 24,
                "target": 12
            },
            {
                "prerequisite": null,
                "similarity": 0.12909944487358058,
                "source": 24,
                "target": 13
            },
            {
                "prerequisite": null,
                "similarity": 0.5662277660168379,
                "source": 24,
                "target": 14
            },
            {
                "prerequisite": null,
                "similarity": 0.12909944487358058,
                "source": 24,
                "target": 15
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 24,
                "target": 16
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 24,
                "target": 17
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 24,
                "target": 18
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 24,
                "target": 19
            },
            {
                "prerequisite": null,
                "similarity": 0.15811388300841894,
                "source": 24,
                "target": 20
            },
            {
                "prerequisite": null,
                "similarity": 0.22360679774997896,
                "source": 24,
                "target": 21
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 24,
                "target": 22
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 24,
                "target": 23
            },
            {
                "prerequisite": null,
                "similarity": 1.0,
                "source": 24,
                "target": 24
            },
            {
                "prerequisite": null,
                "similarity": 0.22360679774997896,
                "source": 24,
                "target": 25
            },
            {
                "prerequisite": 3.4737447154970336,
                "similarity": 0.5662277660168379,
                "source": 24,
                "target": 26
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 24,
                "target": 27
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 24,
                "target": 28
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 25,
                "target": 0
            },
            {
                "prerequisite": null,
                "similarity": 0.3520620726159658,
                "source": 25,
                "target": 1
            },
            {
                "prerequisite": null,
                "similarity": 0.5854101966249685,
                "source": 25,
                "target": 2
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 25,
                "target": 3
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 25,
                "target": 4
            },
            {
                "prerequisite": null,
                "similarity": 0.5854101966249685,
                "source": 25,
                "target": 5
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 25,
                "target": 6
            },
            {
                "prerequisite": null,
                "similarity": 0.5386751345948129,
                "source": 25,
                "target": 7
            },
            {
                "prerequisite": null,
                "similarity": 0.5561862178478973,
                "source": 25,
                "target": 8
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 25,
                "target": 9
            },
            {
                "prerequisite": null,
                "similarity": 0.4389822365046136,
                "source": 25,
                "target": 10
            },
            {
                "prerequisite": null,
                "similarity": 0.5,
                "source": 25,
                "target": 11
            },
            {
                "prerequisite": null,
                "similarity": 0.6279644730092272,
                "source": 25,
                "target": 12
            },
            {
                "prerequisite": null,
                "similarity": 0.5386751345948129,
                "source": 25,
                "target": 13
            },
            {
                "prerequisite": null,
                "similarity": 0.17677669529663687,
                "source": 25,
                "target": 14
            },
            {
                "prerequisite": null,
                "similarity": 0.5386751345948129,
                "source": 25,
                "target": 15
            },
            {
                "prerequisite": null,
                "similarity": 0.17677669529663687,
                "source": 25,
                "target": 16
            },
            {
                "prerequisite": null,
                "similarity": 0.75,
                "source": 25,
                "target": 17
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 25,
                "target": 18
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 25,
                "target": 19
            },
            {
                "prerequisite": null,
                "similarity": 0.35355339059327373,
                "source": 25,
                "target": 20
            },
            {
                "prerequisite": null,
                "similarity": 0.5,
                "source": 25,
                "target": 21
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 25,
                "target": 22
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 25,
                "target": 23
            },
            {
                "prerequisite": null,
                "similarity": 0.22360679774997896,
                "source": 25,
                "target": 24
            },
            {
                "prerequisite": null,
                "similarity": 1.0,
                "source": 25,
                "target": 25
            },
            {
                "prerequisite": null,
                "similarity": 0.17677669529663687,
                "source": 25,
                "target": 26
            },
            {
                "prerequisite": null,
                "similarity": 0.5,
                "source": 25,
                "target": 27
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 25,
                "target": 28
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 26,
                "target": 0
            },
            {
                "prerequisite": null,
                "similarity": 0.14433756729740646,
                "source": 26,
                "target": 1
            },
            {
                "prerequisite": null,
                "similarity": 0.15811388300841894,
                "source": 26,
                "target": 2
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 26,
                "target": 3
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 26,
                "target": 4
            },
            {
                "prerequisite": null,
                "similarity": 0.40811388300841894,
                "source": 26,
                "target": 5
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 26,
                "target": 6
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 26,
                "target": 7
            },
            {
                "prerequisite": null,
                "similarity": 0.5386751345948129,
                "source": 26,
                "target": 8
            },
            {
                "prerequisite": null,
                "similarity": 0.24999999999999994,
                "source": 26,
                "target": 9
            },
            {
                "prerequisite": null,
                "similarity": 0.38363062095621214,
                "source": 26,
                "target": 10
            },
            {
                "prerequisite": null,
                "similarity": 0.35355339059327373,
                "source": 26,
                "target": 11
            },
            {
                "prerequisite": null,
                "similarity": 0.5172612419124243,
                "source": 26,
                "target": 12
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 26,
                "target": 13
            },
            {
                "prerequisite": null,
                "similarity": 0.4999999999999999,
                "source": 26,
                "target": 14
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 26,
                "target": 15
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 26,
                "target": 16
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 26,
                "target": 17
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 26,
                "target": 18
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 26,
                "target": 19
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 26,
                "target": 20
            },
            {
                "prerequisite": null,
                "similarity": 0.35355339059327373,
                "source": 26,
                "target": 21
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 26,
                "target": 22
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 26,
                "target": 23
            },
            {
                "prerequisite": null,
                "similarity": 0.5662277660168379,
                "source": 26,
                "target": 24
            },
            {
                "prerequisite": null,
                "similarity": 0.17677669529663687,
                "source": 26,
                "target": 25
            },
            {
                "prerequisite": null,
                "similarity": 0.9999999999999999,
                "source": 26,
                "target": 26
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 26,
                "target": 27
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 26,
                "target": 28
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 27,
                "target": 0
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 27,
                "target": 1
            },
            {
                "prerequisite": null,
                "similarity": 0.22360679774997896,
                "source": 27,
                "target": 2
            },
            {
                "prerequisite": null,
                "similarity": 0.6035533905932737,
                "source": 27,
                "target": 3
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 27,
                "target": 4
            },
            {
                "prerequisite": null,
                "similarity": 0.47360679774997894,
                "source": 27,
                "target": 5
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 27,
                "target": 6
            },
            {
                "prerequisite": null,
                "similarity": 0.5386751345948129,
                "source": 27,
                "target": 7
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 27,
                "target": 8
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 27,
                "target": 9
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 27,
                "target": 10
            },
            {
                "prerequisite": null,
                "similarity": 0.5,
                "source": 27,
                "target": 11
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 27,
                "target": 12
            },
            {
                "prerequisite": null,
                "similarity": 0.2886751345948129,
                "source": 27,
                "target": 13
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 27,
                "target": 14
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 27,
                "target": 15
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 27,
                "target": 16
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 27,
                "target": 17
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 27,
                "target": 18
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 27,
                "target": 19
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 27,
                "target": 20
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 27,
                "target": 21
            },
            {
                "prerequisite": null,
                "similarity": 0.5,
                "source": 27,
                "target": 22
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 27,
                "target": 23
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 27,
                "target": 24
            },
            {
                "prerequisite": null,
                "similarity": 0.5,
                "source": 27,
                "target": 25
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 27,
                "target": 26
            },
            {
                "prerequisite": null,
                "similarity": 1.0,
                "source": 27,
                "target": 27
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 27,
                "target": 28
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 28,
                "target": 0
            },
            {
                "prerequisite": null,
                "similarity": 0.39433756729740643,
                "source": 28,
                "target": 1
            },
            {
                "prerequisite": null,
                "similarity": 0.15811388300841894,
                "source": 28,
                "target": 2
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 28,
                "target": 3
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 28,
                "target": 4
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 28,
                "target": 5
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 28,
                "target": 6
            },
            {
                "prerequisite": null,
                "similarity": 0.5,
                "source": 28,
                "target": 7
            },
            {
                "prerequisite": null,
                "similarity": 0.2886751345948129,
                "source": 28,
                "target": 8
            },
            {
                "prerequisite": null,
                "similarity": 0.24999999999999994,
                "source": 28,
                "target": 9
            },
            {
                "prerequisite": null,
                "similarity": 0.5172612419124243,
                "source": 28,
                "target": 10
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 28,
                "target": 11
            },
            {
                "prerequisite": null,
                "similarity": 0.38363062095621214,
                "source": 28,
                "target": 12
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 28,
                "target": 13
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 28,
                "target": 14
            },
            {
                "prerequisite": null,
                "similarity": 0.4541241452319315,
                "source": 28,
                "target": 15
            },
            {
                "prerequisite": null,
                "similarity": 0.49999999999999994,
                "source": 28,
                "target": 16
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 28,
                "target": 17
            },
            {
                "prerequisite": null,
                "similarity": 0.6035533905932737,
                "source": 28,
                "target": 18
            },
            {
                "prerequisite": null,
                "similarity": 0.35355339059327373,
                "source": 28,
                "target": 19
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 28,
                "target": 20
            },
            {
                "prerequisite": null,
                "similarity": 0.17677669529663687,
                "source": 28,
                "target": 21
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 28,
                "target": 22
            },
            {
                "prerequisite": null,
                "similarity": 0.0,
                "source": 28,
                "target": 23
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 28,
                "target": 24
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 28,
                "target": 25
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 28,
                "target": 26
            },
            {
                "prerequisite": null,
                "similarity": 0.25,
                "source": 28,
                "target": 27
            },
            {
                "prerequisite": null,
                "similarity": 0.9999999999999999,
                "source": 28,
                "target": 28
            }
        ],
        "nodes": [
            {
                "count": 3,
                "group": 1,
                "index": 0,
                "name": "objective function",
                "videos_id": [
                    "8CwbhSbbB1U"
                ]
            },
            {
                "count": 13,
                "group": 1,
                "index": 1,
                "name": "natural language processing",
                "videos_id": [
                    "NDPyjZkblJc",
                    "02Qvjff8Q_I",
                    "ogrJaOIuBx4",
                    "zi16nl82AMA",
                    "AKwfVAKaigI",
                    "xj5ZvWgPqKM"
                ]
            },
            {
                "count": 50,
                "group": 1,
                "index": 2,
                "name": "matrix",
                "videos_id": [
                    "ogrJaOIuBx4",
                    "AgvfJddkzvE",
                    "BJ0MnawUpaU",
                    "1PXGcUA3m18",
                    "-q4jlLnuRR8"
                ]
            },
            {
                "count": 6,
                "group": 1,
                "index": 3,
                "name": "state",
                "videos_id": [
                    "AgvfJddkzvE",
                    "8CwbhSbbB1U"
                ]
            },
            {
                "count": 7,
                "group": 1,
                "index": 4,
                "name": "natural",
                "videos_id": [
                    "xj5ZvWgPqKM"
                ]
            },
            {
                "count": 10,
                "group": 1,
                "index": 5,
                "name": "text summarization",
                "videos_id": [
                    "02Qvjff8Q_I",
                    "ogrJaOIuBx4",
                    "AgvfJddkzvE",
                    "1PXGcUA3m18",
                    "-q4jlLnuRR8"
                ]
            },
            {
                "count": 7,
                "group": 1,
                "index": 6,
                "name": "extraction based summarization",
                "videos_id": [
                    "-q4jlLnuRR8"
                ]
            },
            {
                "count": 27,
                "group": 1,
                "index": 7,
                "name": "graph",
                "videos_id": [
                    "AgvfJddkzvE",
                    "-q4jlLnuRR8",
                    "GX4RTIIuxy8"
                ]
            },
            {
                "count": 28,
                "group": 1,
                "index": 8,
                "name": "list",
                "videos_id": [
                    "NDPyjZkblJc",
                    "ogrJaOIuBx4",
                    "BJ0MnawUpaU",
                    "zi16nl82AMA",
                    "1PXGcUA3m18",
                    "GX4RTIIuxy8"
                ]
            },
            {
                "count": 13,
                "group": 1,
                "index": 9,
                "name": "stop words",
                "videos_id": [
                    "NDPyjZkblJc",
                    "BJ0MnawUpaU"
                ]
            },
            {
                "count": 25,
                "group": 1,
                "index": 10,
                "name": "language",
                "videos_id": [
                    "NDPyjZkblJc",
                    "ogrJaOIuBx4",
                    "BJ0MnawUpaU",
                    "zi16nl82AMA",
                    "xj5ZvWgPqKM",
                    "8CwbhSbbB1U",
                    "-q4jlLnuRR8"
                ]
            },
            {
                "count": 4,
                "group": 1,
                "index": 11,
                "name": "dictionary",
                "videos_id": [
                    "1PXGcUA3m18"
                ]
            },
            {
                "count": 36,
                "group": 1,
                "index": 12,
                "name": "algorithm",
                "videos_id": [
                    "NDPyjZkblJc",
                    "ogrJaOIuBx4",
                    "zi16nl82AMA",
                    "xj5ZvWgPqKM",
                    "1PXGcUA3m18",
                    "-q4jlLnuRR8",
                    "GX4RTIIuxy8"
                ]
            },
            {
                "count": 15,
                "group": 1,
                "index": 13,
                "name": "node",
                "videos_id": [
                    "AgvfJddkzvE",
                    "-q4jlLnuRR8",
                    "GX4RTIIuxy8"
                ]
            },
            {
                "count": 5,
                "group": 1,
                "index": 14,
                "name": "functions",
                "videos_id": [
                    "NDPyjZkblJc",
                    "1PXGcUA3m18"
                ]
            },
            {
                "count": 6,
                "group": 1,
                "index": 15,
                "name": "training data",
                "videos_id": [
                    "ogrJaOIuBx4",
                    "zi16nl82AMA",
                    "-q4jlLnuRR8"
                ]
            },
            {
                "count": 9,
                "group": 1,
                "index": 16,
                "name": "degree",
                "videos_id": [
                    "zi16nl82AMA",
                    "GX4RTIIuxy8"
                ]
            },
            {
                "count": 6,
                "group": 1,
                "index": 17,
                "name": "system",
                "videos_id": [
                    "GX4RTIIuxy8"
                ]
            },
            {
                "count": 3,
                "group": 1,
                "index": 18,
                "name": "shuffle",
                "videos_id": [
                    "zi16nl82AMA"
                ]
            },
            {
                "count": 3,
                "group": 1,
                "index": 19,
                "name": "sparse matrix",
                "videos_id": [
                    "BJ0MnawUpaU"
                ]
            },
            {
                "count": 8,
                "group": 1,
                "index": 20,
                "name": "pagerank algorithm",
                "videos_id": [
                    "-q4jlLnuRR8",
                    "GX4RTIIuxy8"
                ]
            },
            {
                "count": 11,
                "group": 1,
                "index": 21,
                "name": "common words",
                "videos_id": [
                    "NDPyjZkblJc",
                    "zi16nl82AMA",
                    "1PXGcUA3m18",
                    "GX4RTIIuxy8"
                ]
            },
            {
                "count": 3,
                "group": 1,
                "index": 22,
                "name": "walk",
                "videos_id": [
                    "AgvfJddkzvE"
                ]
            },
            {
                "count": 5,
                "group": 1,
                "index": 23,
                "name": "page rank algorithm",
                "videos_id": [
                    "GX4RTIIuxy8"
                ]
            },
            {
                "count": 10,
                "group": 1,
                "index": 24,
                "name": "key",
                "videos_id": [
                    "NDPyjZkblJc",
                    "xj5ZvWgPqKM",
                    "8CwbhSbbB1U",
                    "1PXGcUA3m18",
                    "-q4jlLnuRR8"
                ]
            },
            {
                "count": 10,
                "group": 1,
                "index": 25,
                "name": "link",
                "videos_id": [
                    "ogrJaOIuBx4",
                    "1PXGcUA3m18",
                    "-q4jlLnuRR8",
                    "GX4RTIIuxy8"
                ]
            },
            {
                "count": 4,
                "group": 1,
                "index": 26,
                "name": "string",
                "videos_id": [
                    "NDPyjZkblJc",
                    "1PXGcUA3m18"
                ]
            },
            {
                "count": 2,
                "group": 1,
                "index": 27,
                "name": "linear",
                "videos_id": [
                    "AgvfJddkzvE"
                ]
            },
            {
                "count": 4,
                "group": 1,
                "index": 28,
                "name": "mean",
                "videos_id": [
                    "BJ0MnawUpaU",
                    "zi16nl82AMA"
                ]
            }
        ]
    },
    "search_info": {
        "NumOfVideos": 14,
        "key": "natural_language_processing_automatic_summarization_50",
        "similarity_threshold": 0.5,
        "time_delta": 11.933333333333334,
        "voclist_SelectMethod": 2
    },
    "videos_info": {
        "-q4jlLnuRR8": {
            "caption_exist": "T",
            "channel_id": "UCKvhw2CPR-0S4XZ1bNlihnw",
            "channel_title": "FunctionalTV",
            "concepts": [
                [
                    "algorithm",
                    8
                ],
                [
                    "extraction based summarization",
                    7
                ],
                [
                    "text summarization",
                    3
                ],
                [
                    "key",
                    3
                ],
                [
                    "graph",
                    3
                ],
                [
                    "training data",
                    2
                ],
                [
                    "matrix",
                    2
                ],
                [
                    "reduction",
                    1
                ],
                [
                    "language",
                    1
                ],
                [
                    "node",
                    1
                ],
                [
                    "link",
                    1
                ],
                [
                    "heuristic",
                    1
                ],
                [
                    "pagerank algorithm",
                    1
                ]
            ],
            "description": "Cognition is in virtually everything that humans do, such as language understanding, perception, judgment, learning, spatial processing and social behavior ; and given that IBM Watson represents the first step at envisioning truly cognitive systems - it becomes crucial to constantly harness its abilities with processing natural language, evaluating hypotheses and learning dynamically across domains. The project we will go over in this talk is aimed at augmenting these very behaviors of IBM Watson as PhrazIt is focused at enriching raw data and essentially transforming information into insights. PhrazIt\u2019s technologically differentiated core that is powered by an augmented extraction-based text summarization algorithm utilizes a novel contextualized indexing framework thus making it a tremendous value-add when deploying cognitive services powered by Watson. Join us as we go over the current and emerging state of the art in the space of text summarization. Reflect on what is changing the world in this era of cognition. Dive deep into the pipeline and the core algorithmic paradigms that power a content extraction engine. And leave with an understanding of what it takes to build a product that provides data science-as-a-service. ----------------------------------------------------------------------------------------------------------------------------------------\n\nScal\u00e6 By the Bay 2016 conference\n\nhttp://scala.bythebay.io \n\n-- is held on November 11-13, 2016 at Twitter, San Francisco, to share the best practices in building data pipelines with three tracks:\n\n* Functional and Type-safe Programming\n* Reactive Microservices and Streaming Architectures\n* Data Pipelines for Machine Learning and AI",
            "dislikeCount": "0",
            "duration": "PT18M39S",
            "likeCount": "17",
            "published_time": "2016-07-15T21:07:24.000Z",
            "tags": [],
            "thumbnail": "https://i.ytimg.com/vi/-q4jlLnuRR8/hqdefault.jpg",
            "title": "data.bythebay.io: Niyati Parameswaran, PhrazIt : Tool for automatic text summarization",
            "transcript": "  so hi everybody I am Nia T and this is the second consecutive talk just a show of hands how many of you were there for the previous talk all right okay so now I'm gonna be talking about a phrase it which is a tool for automatic text summarization it's an extraction based summarization a little bit about me I am a data scientist at IBM Watson where I work on conceptualizing core machine learning and NLP algorithmic paradigms and also building cognitive solutions specific for partner use cases I did my bachelor's in computer science from BITS Pilani in India followed up with a master's from the University of Texas at Austin I've been at Watson for a little over 10 months now I am consistently battling in choppy I'm on the path where I'd like to become a Bayesian ninja and belong to the group of people who laugh out loud really loud and that's a picture of me doing exactly that ok so for this particular talk the focus is phrase it now text summarization happens in two ways there are two kinds of algorithms that exist and two kinds of literature that gets published around summarization there's extraction based summarization and there is abstraction based summarization so extraction based summarization is basically when given a document or documents of text effectively what is done is that you are extracting sentences from that passage or document of text verbatim so there's no condensing that's happening in any format it's just picking out sentences to form a shorter summary abstraction based summarization however works differently we're in apart from condensing sentences in the sense that picking out the most relevant sentences you're also changing the manner in which a document is represented so that's the more true summarization framework but phrase it is an extraction based summarization so it follows two primary steps the first is where we establish a thematic or a descriptive score with every sentence in a passage of text and then we reorder all the sentences in that passage or document of text on the basis of thematic relevance you'd see across literature in extraction based summarization that sentences get ranked on the basis of three status static features which you use as indexing weights and these static features are term frequency term position and term length so while the link phrase it the idea behind it was why just work with three static features why not account for context and a contextual understanding so that we are able to associate a theme with whatever document we are investigating in order to render a summary so the second step that phrase it did is where we ranked sentences on the basis of a thematic score which was associated to them and in this regard we were able to allow for a contextual understanding so phrase it therefore introduces a unique idea of a context based indexing to resolve the problem instead of working with a context independent term indexing it's important to understand that every document that we are working with more often than not contains content specific terms and background terms so instead of just thinking of every document as devoid of the context in which it appears it's kind of important to account for that context and that's exactly what our we did through phrase it because the existing models in the extraction based summarization space they're not really able to distinguish and differentiate between terms by a sole reliance on just term weights because the term bases here term length so we use even context when establishing the importance of a sentence in a passage or document so this complete dependence on term significance is reduced heavily and now your document indexing weight is not completely independent of the context in appears talking about the algorithmic framework that gets leveraged we are powering of our text rank which is an unsupervised algorithm the reason again for using an unsupervised algorithm is because with a supervised text somewhat supervised algorithmic paradigms for summarization you need to provide a large amount of training data and this in effect translates to having a whole bunch of documents with a bunch of non key phrases and while our supervised techniques are capable of producing what we call as interpretable rules in order to identify what characterizes a key phrase the trade-off was that you require a significant amount of training data and so we decided to move away from that into the space of unsupervised algorithmic paradigms for extraction based summarization instead additionally within unsupervised keyphrase extraction framework like phrase it it's also way more portable because it hasn't been trained on a specific domain so basically there isn't any kind of customization in the extraction process instead it's capable of learning features that are present in the text and they're able to determine whether certain key phrases are central to the text or not so this is done with text rank it works similar to Google's PageRank algorithm which I'm sure you guys are familiar with where the idea is to select the most important webpages but again like I said with text rank it is a graph based ordering algorithm and it Associates importance of a sentence in a passage or document of text by using the static features of term position term frequency and term length so we don't do that we work with also context and I'll just explain how yeah so what's happening with phrase it if we need to put it in perspective it's is that it's just a general or graph based ranking algorithm and this is how it it's going to work so basically every sentence in a passage or document of text gets represented as a node and the edges across these various sentences are what is going to this comes from the contextualized phrasal vectors which is what I went over in the previous talk but I'm just going to do a quick overview for those of you who weren't part of the previous talk effectively with the contextualized phrase of vectors I am able to get a phrasal representation for every sentence on the basis of the core concepts and the core topics which are referenced in that particular sentence it is it powers off both structural similarity as well as semantic similarity so for the structural similarity piece we are basically looking at how do I explain this in really short because that was all of the previous talk ok so we have a word to whack and we have word nets and sets and what I'm doing is that with word to work and word nets and sets in word to work I have LSA as well as Lda additionally so what work has been augmented now to be able to render phrase of variations Frasor representations of sentences as opposed to just a vector representation at a singular word level and this is possible because of LSA and Lda where LSA reference is the most important concepts in that sentence and lva are references the most important topics in that sentence so basically by leveraging this piece I'm able to get a phrasal representation for every sentence and this is what gets used in order to establish how important a sentence is to a passage or document of text because that is what is giving me the theme that associates a particular sentence given a document or passage post this kind of evaluation effectively once this kind of graph is constructed we use a stochastic or a Markov matrix to identify the ordering of these sentences on the basis of thematic relevance which sentence has a higher thematic relevance automatically gets bumped up in the reordering and that gets pushed up in the summary that gets rendered so this if we need to go into more detail basically for the Markov matrix I'm going to obtain that kind of ordering on the basis of the eigenvector that corresponds to the eigenvalue of one phrase it again has been validated against the state-of-the-art on semi well as well as the duke 2003 corpus post 2003 Duke as well as all of NIST they've been using a metric called Rouge on which I haven't done analysis just yet so we were sticking to our metrics of accuracy and F just in order to retain metric consistency across all of the state-of-the-art that we have evaluated on so here are the results around that phrase it essentially like I said it's a text rank with lexicalized Association because it's allowing for an understanding of context and here the accuracy it beats the state-of-the-art with 70 point zero two on accuracy and the F is fifty nine point nine the initial the baseline was a knife based algorithm that had an accuracy of forty point three and an f of twenty seven point eight I know that this talk is scheduled for 50 minutes but I don't know why that was it seems like the previous talk in this talk should have been interchanged on time but I'm gonna quickly show you guys a demo and I think we're gonna wrap up much earlier here so this is just a piece of dummy text it's an email conversation that it happened and what I do here is let me just show you with alchemy language because this is something that has been integrated with phrase it I get the most important concepts which are referenced in this particular piece of text so the piece of text for those of you if you can't read is basically I hope you got my email on the menu we look forward to seeing you at the breakfast session on Wednesday for the more given our past discussions in your focus area I also wanted to check with you if you'd like to shed you a meeting would want to shed you'll a meeting in the second half of tomorrow let me know if either option work for you so basically by leveraging alchemy I am also additionally able to get a sense of the keywords and the associated relevance across these keywords and that is something that gets used in phrase it additionally when building that contextual indexing space here is a demo of phrase it I'm plugging in the exact same text and this demo is just to basically show you how our phrase it is able to pick up sentences on the basis of what might be most significant to a particular passage of text and what is really the call to action item so if each one of you just reads this particular passage of text there are certain action items which are referenced there and so if you basically wanted to pick out just one sentence which is most crucial to this entire email it would probably be I also wanted to check with you if you would like to share you the meeting with Gregory and us where shed Yuling a meeting is the most important action item so as you would see phrase it there's a pretty neat job and this is I like to use this particular dummy piece of text because it picks out the correct action item as the single line summary now in the two line summary it picks out the second action item which is we look forward to Wednesday it also does something neat here which is it reorders the second call to action item in position one and pushes the first call to action item in position two the reasoning behind this is that phrase it also has a cohesive summary sense so it knows that sentences that start with furthermore or not the way that idle idly summaries would get generated and so it reorders various sentences in the summary in order to get a summary which is more cohesive to read you can work with so say you have a document of text and you want a 20% reduction in that document phrase it allows you to do that it's just not part of this particular demo you can also get a deduction on the basis of the number of lines that you would want in a summary say you have hundred lines in your document and you want to reduce that to like maybe the top 12 phrase it will also enable you to do that there is another piece to phrase it which I have added recently which is currently phrase it at least the demo here is a single document summarization which means for one particular document or a bunch of passages of text it's going to render the offense of that passage or document of text I've extended this to a multi document summarization framework where basically you can have more than one documents and a phrase it will still pick out the most important sentences referenced across all of those documents this is done where the text rank algorithm is replaced by Alec's rank algorithm and the utility of this is apart from just picking out those sentences which are important across various documents there is a post-processing heuristic that I use called cross information subsumption and using cross information subsumption if there are certain sentences which are basically paraphrases of each other or are saying the same kind of thing we remove those and we a love for we bias for lexical diversity so that every sentence which is picked out of different documents too gets represented in that summary so that's an extension of what is being worked on now and yeah that's pretty much what I wanted to show you guys through the automatic text summarization piece I see we've done really well on time if you guys have any questions please feel free to ask at this point yes um randomness just fighting randomness I have some kind of OCD and everything just needs to be structured kind of Mira is my job where the data scientists have to find structure and data so I guess it's it's just there in life the summaries oh that's just the sizing I guess yeah yes so were to whack I'm just using that word thwack peace which is the Wiktionary corpus so there are no labels as such which is associated with any kind of data the idea is to get a phrasal representation of every sentence in a vector format basically contextualize phrasal vectors as opposed to word vector representations yeah those vectors precisely precisely yep that's what it is any other questions yes any more yes no no phrase it was conceptualized for the use case wherein we have a lot of partners who work with IBM Watson who have a lot of noisy crappy data sometimes a lot of the data is just outliers the idea is to pick out what is the essence in that data so that we'd at least have neat enough data when we are training our systems so that's what that's been done to but that is an interesting thought and should be implemented for for what yeah they're like Google's open source work back although they call it is not really part of they're not really something that they've given out but it's still it's still easily available so that's something that you can look up",
            "videoid": "-q4jlLnuRR8",
            "viewCount": "1748"
        },
        "02Qvjff8Q_I": {
            "caption_exist": "T",
            "channel_id": "UC5zx8Owijmv-bbhAK6Z9apg",
            "channel_title": "Data Science Tutorials - All in One",
            "concepts": [
                [
                    "text summarization",
                    3
                ],
                [
                    "natural language processing",
                    2
                ]
            ],
            "description": ".\nCopyright Disclaimer Under Section 107 of the Copyright Act 1976, allowance is made for \"FAIR USE\" for purposes such as criticism, comment, news reporting, teaching, scholarship, and research. Fair use is a use permitted by copyright statute that might otherwise be infringing. Non-profit, educational or personal use tips the balance in favor of fair use.\n.",
            "dislikeCount": "1",
            "duration": "PT11M38S",
            "likeCount": "1",
            "published_time": "2016-04-06T01:52:36.000Z",
            "tags": [
                "Natural Language Processing",
                "Language Processing",
                "University of Michigan",
                "Michigan",
                "NLP",
                "Coursera",
                "Dragomir R. Radev",
                "Computational Linguistics",
                "Linguistics",
                "Information Retrieval",
                "Computer Science",
                "Video Lecture",
                "Video Tutorial",
                "NLP Video Course",
                "Video Course",
                "Natural Language Processing Video Course",
                "Course",
                "Data Science",
                "Natural Language Processing Video Lecture",
                "Summarization"
            ],
            "thumbnail": "https://i.ytimg.com/vi/02Qvjff8Q_I/hqdefault.jpg",
            "title": "Summarization | Natural Language Processing | Michigan",
            "transcript": "  welcome back to natural language processing today we're going to start a new section on text summarization which is one of the most interesting parts in natural language processing so here's a very simple introduction to text summarization we want to be able to get as input set of documents for example about health so here we have a bunch of sentences about eating vegetables and foods and the reasons why they're healthy I mean they often users don't have the time to read a lot of details and we want to be able to produce a summary of them for two reasons one so that they can read somebody instead of the original documents if they don't have enough time or even more realistically to show them the summary so that they get an idea what the document is all about and then once they read the summary they can decide whether they want to go and read the full set of documents so in this example the summary that we would like to get out of there is no today there's no summarization system currently that can do this we want to get a short summary like this eating vegetables is healthy so this is a very extreme example of text summarization which as I said is not practically feasible at this point but there are many instances of existing summarization systems at work and produce meaningful summaries that are actually useful for users so let's look at some examples what kind of summaries we want to be able to produce one is new summarization we want to get for example cluster of documents that are related to each other and would use a short summary like the one that appears in the middle of the page of everything that happened in this set of related documents we can also put his book summaries so here's an example we can have something like Cliff's Notes which probably everybody knows about this notes are short descriptions of what happens in some large book plus some additional comments about the characters and the plot and the techniques used in the book here's a funny example there's a website called book a minute where people have spent time to summarize famous books into really really short version so here's one can you guess which book it is some boys crash on an island and then valve says repeatedly we need a fire they make a fire it goes out that this happens a few times then jack says forget the fire let's cue each other the other boys say yeah and they do kill each other at the end so this is again a tongue-in-cheek summary of a famous book can you guess which one it is yeah so this book is obviously Lord of the Flies by Golding and it was out a condensed to use that term by David Parker and Samuel Stoddard on the rink works.com so more examples of some of his movie summaries so you have some like Titanic and short summaries of it for example beginning in genuine footage of the departure of the Titanic on its fateful voyage this epic movie tells the events of that tragic night from the perspective of fictional survivor roles as an old lady of a hundred she recounts the story of duty love and disaster to salvage crew searching for lost gem and obviously there many different summaries that you can put this first given movie obviously if you go to a website like I am DB you can get hundreds of somebody's written by different people and as you can see the sum is often can look very different from one another and yet have the same goal namely to capture the gist of the movie or book or other piece of work another type of summaries are search engine snippets the difference between those and the ones we looked at previous is that those are query based so search engines return little passages from the retrieved documents that are most similar to the query so here's an example I did a search for our Cloud Atlas which is a science fiction book and as you can see the top hits that are returned here by Google are accompanied by short passages or snippets and the words Cloud Atlas and the name of the author are highlighted there are many different genres of summaries we have also headlines so headline can be construed as a very short summary of a documents of a new story for example an outline can be produced for many different genres of dock which it could be an outline of a book an outline of a meeting an outline of an encyclopedia entry or paper and so on it can also be minutes of meetings biographies of people for example in obituaries or just plain biographies they can be abridgements so beach months or books are typically shorter versions for different audiences perhaps for younger children or for people who don't have that much time they can be soundbites it should just some small snippets of audio from an interview or from some event they can be movie summaries chronologies and so on and I want to give credit for this taxonomy to in digit money and may mark memory from the 99 paper so what types of summaries can there be well we already saw some of them but how do we distinguish between the different types of sample is well one of the factors is what's the input is it a single document or multiple documents is it grammatical text or not was the output is it grammatical sentences or perhaps just a few cue words is it speech or text what's the purpose is it intended to replace the original document or is it just indicative in which case it just tells you what the original document is about but doesn't give you all the details about it they can also be something called critical summaries where you are summarizing for example a book or movie but you're also giving some subjective information about how you feel about the form can be either extractive or obstructive so extracts are usually a representative paragraphs or sentences or phrases from a document whereas abstracts can be reformulated in different using different words and I in general to quote from a pace paper from 1990 are concise summary of the central subject matter of a document the other dimensions to take into account whether the documents are is based on a single multiple document input the contact is very important is it query specific for example how is this document related to let's say trade talks versus generic I mean what's a genetically a good summary of this document regardless of the context okay so a typical summarization system can have up to three stages typically there are the following the first stage is Content identification so given your input documents you have to determine what information you want to preserve and pass on to the next stage so that could be some specific sentences or named entities or facts the next thing is how to organize this information you want to combine information from multiple documents do you want to preserve entire sentences do you want to the orchidarium and finally have realization so in the realization you have to deal with some additional issues for example if you take two documents and you one sentence from each of them they may not read nicely next to each other so it's possible that the realization stage would include the generation of some connectives for example for example or therefore or in contrast so that the sentences that came from different sources can be tied together more coherently or realization can include the generation of some referring expression such as this person or he or she so here's an example of an extractive Samuraizer it takes as input a news story as you can see the news story consists of about ten sentences and then we want the summary to include the most important facts that are underlined and shown in red here you can see that a purely extractive summarizer would just underline those passages and sentences and present that as the summary so in this case realization is practically non-existent because we are just preserving information from the original documents as it appears in exactly the same order one important thing to mention here is that extracting summarization can come in two different kinds the first kind is where you have only full sentences like without the last sentence in this example here or you can have portions of sentences that still counts as an extractive summarizer even if you're not extracting the entire sentences so it turns out that summarization is something that humans have been doing for years for example there are professional abstractors that read scientific articles and create create manual abstracts of those that go into different bibliometric databases and here's a nice quote from many years ago from four years ago by Ashworth about what professional abstractors do quote-unquote to take an original article understand it and pack it neatly into a nutshell without loss of substance or clarity presents a challenge which many have felt worth taking up for the joys of achievement alone these are the characteristics of an art form so this passage very clearly indicates that summarization is a very difficult task that humans may or may not be good at but that it involves some sufficient some very significant amount of craftsmanship ok let's now focus on some specific types of summaries for example extractive summarization so as I mentioned before extracting some organization is about selecting eunuchs from the original text and presenting them in the order in which they appear there the units are usually sentences in one of the most common scenarios there is no simplification of the sentences allows you're not allowed to skip any portions of them or replacing words with others though here writing is allowed so it turns out that from some genres of documents there is a very important baseline that is very hard to beat for extracted summarization and that is the so-called need-based baseline so in lead based summaries you have a certain amount of text that you're allowed to produce as part of the summary for example let's say the equivalent of two sentences or five sentences so the baseline is to extract as many sentences from the beginning of the document as you are allowed to produce in the output so 2 or 5 in this example it turns out many evaluation metrics and for many genres of text the first few sentences which are in fact the most informative sentences well luckily for researchers in this field this is not the case in most of the other genres so it's possible to come up with techniques that I am and better so we're going to now continue with some additional considerations about summarization in the next segment",
            "videoid": "02Qvjff8Q_I",
            "viewCount": "506"
        },
        "0DLa5sApXhc": {
            "caption_exist": "T",
            "channel_id": "UCjRZr5HQKHVKP3SZdX8y8Qw",
            "channel_title": "Engineers.SG",
            "concepts": [],
            "description": "Speaker: Anusha\n\nSample Code: https://github.com/anooshac/machine-learning-projects/tree/master/text-summarizer\n\nEvent Page: https://www.meetup.com/TensorFlow-and-Deep-Learning-Singapore/events/239252636/\n\nProduced by Engineers.SG\n\nHelp us caption & translate this video!\n\nhttp://amara.org/v/7PAC/",
            "dislikeCount": "8",
            "duration": "PT16M26S",
            "likeCount": "75",
            "published_time": "2017-05-25T15:34:43.000Z",
            "tags": [],
            "thumbnail": "https://i.ytimg.com/vi/0DLa5sApXhc/hqdefault.jpg",
            "title": "Text Summarization - TensorFlow and Deep Learning Singapore",
            "transcript": " SO, I am Anusha! and I am actually a machine learning enthusiast , I am constantly learning myself everyday. And today I am here to share about one of my hobby projects which was text summarization. Subtitles by the Amara.org community",
            "videoid": "0DLa5sApXhc",
            "viewCount": "8436"
        },
        "1PXGcUA3m18": {
            "caption_exist": "T",
            "channel_id": "UC5_6ZD6s8klmMu9TXEB_1IA",
            "channel_title": "CodeEmporium",
            "concepts": [
                [
                    "matrix",
                    6
                ],
                [
                    "dictionary",
                    4
                ],
                [
                    "algorithm",
                    3
                ],
                [
                    "common words",
                    3
                ],
                [
                    "key",
                    3
                ],
                [
                    "link",
                    2
                ],
                [
                    "string",
                    2
                ],
                [
                    "list",
                    2
                ],
                [
                    "square matrix",
                    1
                ],
                [
                    "text summarization",
                    1
                ],
                [
                    "operations",
                    1
                ],
                [
                    "functions",
                    1
                ]
            ],
            "description": "Get the Code here : https://github.com/ajhalthor/text-summarizer\nFollow me on Twitter : https://twitter.com/ajhalthor\nTake a look at the original by Shlomi Babluki : http://thetokenizer.com/2013/04/28/build-your-own-summary-tool/\n\nTRANSCRIPT OVERVIEW\n\nALGORITHM\n\n1. Take the full CONTENT and split it into PARAGRAPHS. \n\n2. Split each PARAGRAPH into SENTENCES. \n\n3. Compare every sentence with every other. This is done by Counting the number of common words and then Normalize this by dividing by average number of words per sentence.\n\n4. These intermediate scores/values are stored in an INTERSECTION matrix\n\n5. Create the key-value dictionary\n - Key : Sentence\n - Value : Sum of intersection values with this sentence\n\n6. From every paragraph, extract the sentences  with the highest score.\n\n7. Sort the selected sentences in order of appearance in the original text to preserve content and meaning.\n\nAnd like that, you have generated a summary of the original text.\n\n\nCLASSES IN JAVA PROJECT\n\n1. Sentence : The entire text is divided into a number of paragraphs and each paragraph is divided into a number of sentences.\n2. Paragraph : Every paragraph has a number associated with it and an Array List of sentences. \n3.  Sentence Comparitor : Compare Sentence objects based on Score\n4.  SentenceComparatorForSummary  : Compare Sentence objects based on position in text.\n5. SummayTool : akes care of all the operations from extracting sentences to generating the summary.\n\n\nHOW IS MY SUMMARIZER BETTER THAN THE ORIGINAL ?\n\n\nMy text summarizer selects number of sentences from a paragraph depending on the length. This is an improvement over the original text summarizer implementation that only selects 1 sentence per paragraph regardless of length. So, If the author decides to crunch everything into 1 paragraph, then only one sentence  would  be chosen. In the current implementation, we set it to accept several sentences for larger paragraphs.  It delivers cogent summaries for general essays, reviews and publications. \n\n\nRUN THIS PROGRAM\n\n$ javac -d bin improved_summary.java\n$ java -classpath bin improved_summary",
            "dislikeCount": "3",
            "duration": "PT11M21S",
            "likeCount": "55",
            "published_time": "2017-03-17T11:41:03.000Z",
            "tags": [],
            "thumbnail": "https://i.ytimg.com/vi/1PXGcUA3m18/hqdefault.jpg",
            "title": "Build a Text Summarizer in Java",
            "transcript": "  hey guys today we're going to develop a text summarizer I'll start by discussing the algorithm and then explain the implementation and show the summarizer in action so let's get started this algorithm that we'll discuss is my modification of the text memorizer developed by Shlomi bob looky I hope I pronounced that right I'll give the link to his version down in the description below if you want to check it out too however my version does offer more flexibility and improvement which we'll discuss later so let's jump into the algorithm please so first we're going to take the entire context and split it down into paragraphs next we split each paragraphs into sentences then we compare every sentence with every other this is done by computing the number of common words and then normalizing this by dividing it by the average number of words per sentence these intermediate scores or values that are computed are stored in an intersection matrix we then compute the final scores and store them in a key value dictionary where every key is the sentence and every value is the sum of intersection values with the sentence that is the final score from every paragraph we extract the sentences with the highest score and finally we sort the selected sentences in the order for parents in the original text to preserve context and meaning and like that you've generated a summary for the original text so we're going to implement this using Java let's take a look at this one class at a time in the sentence class the entire text is divided into a number of paragraphs and each paragraph is divided into a number of sentences we give two numbers to every sentence the first is the paragraph number which indicates which paragraph the sentence is apart the second is number which indicates the sentence number with respect to the entire text string length is the number of characters in the sentence we really don't need this here but I'll leave it in for now and I'll delete it from the code when I upload it to github every sentence has a score that indicates its importance this is initialized to 0 the number of words is computed by manually word tokenizing a sentence and the sentences value is the actual string now let's take a look at the paragraph class every paragraph has a number associated with it and an ArrayList of sentences a quick side note an ArrayList is a type of collection that we use to implement sentences and paragraphs we could have also done this by using simple arrays but collections better model the real world basically it makes more sense in case you are wondering why I use collections here and also later in the video next we have the sentence comparator comparators in general are used to compare objects for example we know that 5 is greater than 3 as they are both integer types but how do we actually compare two objects in this case to sentence objects with a number of properties this is where a comparator comes in to use a comparator we need to create a class that implements the comparator interface for the generic type that we wish to compare in this case that generic type is sentence now we override the compare function passing in the two sentence objects to be compared in this case one sentence is greater than the other if it's score is greater so if I were to take an example if object one has a score of 3 and object 2 has a score of 6 then the sentence object 2 is greater than the sentence object 1 likewise we define another comparator called sentence comparator for summary which does exactly the same thing however the greater object is not the one with the higher score it is the one with the higher number we will be using this in the summary tool class while generating the summary to ensure that the selected sentences are in the order that they appear in the original text the summary tool class takes care of all the operations from extracting sentences to generating the final summary the constructor and the init functions will instantiate and initialize all variables required extract sentence from context method is a primitive hand-built sentence tokenizer that extracts sentences based on the position of periods in the entire text so when a period is encountered the text seen until that point is considered a sentence and added to the sentence list furthermore paragraphs are also segmented a new paragraph is encountered if there are two consecutive line feed characters present between two bodies of text group sentences into paragraphs method creates a new paragraph object and stores the ArrayList of sentences present in them know of common words finds the number of common words between the two sentences the comparison is not case-sensitive create intersection matrix creates a square matrix of size depending on the number of sentences in the entire context or the source text every sentence is compared with every other and an intermediate or partial score is generated using the normalization this matrix will be symmetric about its diagonal so we can reduce the time of execution by nearly half the diagonal elements will be 1 because every sentence is 100% similar to itself the create dictionary method takes the partial scores computed in the intersection matrix and further computes the total scores for every sentence this is done by adding all the scores of a row and assigning the value to the corresponding sentence so this creates a dictionary where the key is the sentence and the value is the sentence score the create summary method will ultimately extract the final sentences for the summary some things however need to be taken care of certain paragraphs are longer than others it wouldn't make sense to just select one sentence for paragraph so the number of sentences we select depends on the number of sentences in the paragraph we select one sentence for every five sentences in a paragraph so let's say that an article has two paragraphs with six sentences in the first and 17 in the second paragraph from the first paragraph we will select only two sentences while from the second we will select four in the code we iterate over every paragraph and determine the number of sentences to select from each we then take the sentences and sort them in descending order of score so the sentences of the paragraph will be arranged based on importance we select the required number of sentences from the beginning of the list to be a part of the final summary once all the important sentences have been selected it is important to preserve the order in which they appeared this is done by sorting the sentences based on the sentence number property which we implemented using the sentence comparator for summary class now the print sentences print intersection matrix and print dictionary methods our developer methods they display the corresponding structures these are not required to generate the summary but I will leave them here so that you can visualize the intermediate steps for generating the summary the print summary method prints the final summary on the console Prince - Prince the original word count the summary word count and the amount of compression which is the ratio of the two and now all of these methods are initiated from the main function in the main class so how exactly is my summarizer better than the one originally this text summarizer selects the number of sentences from a paragraph depending on the length like I mentioned before this is very important over the original text summarization implementation that only selects one sentence per paragraph regardless of the length so if the author decided to crunch everything into one paragraph then only one sentence would have been chosen in the current implementation however we set it to accept several sentences for larger paragraphs this delivers cogent summaries for general essays reviews articles and publications in the init method of the summary tool class we defined the input file to be samples / Amazon / Nexus 6p we are going to put all of these class files into a bin folder so let's create that first now go to your terminal and enter your projects working directory and then execute the command Java C option D bin and the name of the main class Java the option D allows us to specify the path to store the generated class files so right now all of our class files will be stored in the bin folder now we execute the main class file by specifying the class path that is the bin folder in this case so we've displayed the selected sentences one after another and even the stats at the bottom I'll be uploading a bunch of sample essays Amazon reviewed scientific articles and blog posts along with the code for this project on github the link will be down in the description below this video so what do you think of this tech summarizer did you like it did you hate it leave a comment down below and let us know your thoughts be sure to subscribe on your way out for more amazing videos and I'll see you in the next one [Music]",
            "videoid": "1PXGcUA3m18",
            "viewCount": "4441"
        },
        "1cNcB7nuFwk": {
            "caption_exist": "T",
            "channel_id": "UCmKl_5pnnZP8M4xuIMOOSoQ",
            "channel_title": "Scott Squash",
            "concepts": [],
            "description": "https://play.google.com/store/apps/details?id=com.software995.squash\n\nLengthy texts are difficult to read on smart phones. Content is often formatted for a large screen. Smartphone users must perform a combination of tedious zoom and scroll operations to navigate the text. Advertisements interrupt the text flow. Squash is today's solution. It makes it possible for the smartphone user to quickly digest key concepts in content that was designed to be viewed on a larger screen. Squash presents a synopsis that is short, easy to read, and formatted specifically for phones and tablets. It can even read the summary aloud.\n\nSquash performs text analysis using a remarkably fast combination of multivariate statistics and natural language processing. Squash can often generate a summary faster than it takes a user to load a webpage with its accompanying graphics and advertising. It has been tested with thousands of articles from CNN, BBC News, NPR News, the Huffington Post, Yahoo!, and the New York Times.\n\nSquash includes shake-to-summarize functionality which allows the user to transition from four to six to eight sentence summaries with the flick of a wrist. Squash generates four, six and eight sentence summaries of any text available to an Android phone.",
            "dislikeCount": "1",
            "duration": "PT3M15S",
            "likeCount": "9",
            "published_time": "2013-06-13T00:11:54.000Z",
            "tags": [
                "text summarization",
                "news summary",
                "www.squash995.com",
                "Squash995",
                "Squash Text Summarization",
                "summarizer"
            ],
            "thumbnail": "https://i.ytimg.com/vi/1cNcB7nuFwk/hqdefault.jpg",
            "title": "Squash Text Summarization Demonstration",
            "transcript": "  my name is Scott and I wrote squash tech summarization software for the Android phone I'm gonna give you a quick demonstration of it but first although I think it's pretty obvious I'm going to tell you why I wrote it it's very difficult to read news stories and wrong messages on a mobile phone this one is actually relatively well laid out but I'm sure you've seen in the past you'll load up a web page and you can barely read it you've got a zoom in to and scroll through it etc well I wrote squash so that you could take those stories and with the press of a button generate a summary of them which was which would be much easier to read I'm going to demonstrate it right now so we've got a news article on the phone and I'm going to invoke squash it's very simple simply share the page with squash as follows and squash will take the URL summarize it and generate a summary for you it can even read the summary aloud look at this I think that's pretty cool now you can also summarize text in the clipboard so for instance I'm going to select some text right now and I'm going to share the selected text with squash and there is your your summary generated from text stored in the clipboard now when you have text in the clipboard all you need to do to generate a summary of it is to just run squash and it will take that's that text in the clipboard and create your summary it's got a bunch of useful little features so when you have a summary and you want to share it you can share it with Facebook or or email or what that's what you can do that's what I've done right there and that's in a nutshell squash if you have any questions about it you're welcome to email me at support it's off four nine nine five again my name is Scott and I wrote this application I think you're gonna like it and I just want to mention one more thing I recently published this app after reading about Google and Yahoo's acquisitions of summly and wavy for somewhere upwards of thirty million dollars each and that's that's great for the people who wrote those applications but unfortunately Google and Yahoo took their summarization software off the market so at the moment there really isn't anything for the consumer and that's why I wrote squash and I make it available free for download thanks for taking the time to listen to me and I hope you enjoy my software",
            "videoid": "1cNcB7nuFwk",
            "viewCount": "2302"
        },
        "8CwbhSbbB1U": {
            "caption_exist": "T",
            "channel_id": "UCd9-Px5a7abJlTRRKhgJANA",
            "channel_title": "iREL Lab IIITH",
            "concepts": [
                [
                    "state",
                    3
                ],
                [
                    "objective function",
                    3
                ],
                [
                    "key",
                    1
                ],
                [
                    "language",
                    1
                ]
            ],
            "description": "Project Name:- Deep learning Approach for\u200b Extractive Summarization\u200b\n\nTraditional   approaches   to   extractive   summarization   rely   heavily   on   human-engineered features.We   have   used   a   data-driven   approach   based   on   neural   networks   and continuous   sentence   features.The   frame-work   for   single-document   summarization composed   of   a   hierarchical   document   encoder   and   an   attention-based   extractor.This architecture   allows   us   to   develop   different   classes   of   summarization   models   which   can extract   sentences   or   words.   We   train   our   models   on   large   scale   corpora   containing hundreds\u200b \u200b of\u200b \u200b thousands\u200b \u200b of\u200b \u200b document-summary\u200b \u200b pairs.\n\nwebpage:- https://piy1708.github.io/",
            "dislikeCount": "0",
            "duration": "PT5M31S",
            "likeCount": "3",
            "published_time": "2017-11-15T08:45:46.000Z",
            "tags": [
                "CS4731",
                "IRE-2017"
            ],
            "thumbnail": "https://i.ytimg.com/vi/8CwbhSbbB1U/hqdefault.jpg",
            "title": "Deep learning Approach for\u200b extractive Summarization\u200b",
            "transcript": "  hi everyone this is a major project for the course information retrieval and this we have implemented a document civilization model that will take a document as an input and will generate some read for the document for this we have used extracted approach an extractive approach we will extract important sentences out of the document and will concatenate them to generate assembly previously a human gene add features were used to represent a sentence but now we are using some conversation neural network for learning different kind of features and representing the sentences so the dataset we have used is tailing me in the news dataset we have divided them into three parts 90% of the data is used for training 5,000 for validation and 5% for testing a data file of you it looks like this so because we are using supervised learning so we have two levels assigned responding to each of one means it's going to be included zero means it's not going to integrate and two is that we are not sure about it and these entities are basically used for generating the summaries in English language so let's see the mathematical formulation in the mathematical formulation as we can see that we are trying to maximize this objective function by training the model so we will try to maximize the summation of the probabilities for each of the sentence given that we have a representation of the document so it's just conditional probability now how they feel and Quora document will receive the next slides so basically the new civilization model had two key component the first minister haider document in decoder reader and the second is the hierarchical content extractor so the highlighter document coded the our first encoding the works which we have three trained then we are encoding the sentences and then finally and holding the document in the extractor part we are predicting the probability for each of the sentences J's are which is required in the objective function so and this content extractor is a tension based that is while we are learning or we are predicting we are also violent coding we are also giving attention to the important sentences of the story then let's see the required document reader components so the convolutional sentence encoder is useful encoding require sentence we have used kernel size of 1 to 7 to learn different features like fiber on trying on Vietnam and then we will use maths falling over time and then finally perform summation to find represent us in tents then coding dissidents these recurrent neural network help in stem cells which prevents the problem of gradient vanishing gradient then finally we have a sentence extractor which will extract the sentence based on the program rating so this is our objective function which we talked about in the mathematical formulation and it needs attention mechanism because it is taking into account the previous sentence also and the previous decoding state as well so the decoding state at time T will be determined by the path of probability which was assigned to a sentence at time t minus 1 and decoding dickering state of the model t minus 1 and finally we can use the decoding and then coding from the encoding model to find a predictor probability for a sentence at time T so basically the document representation is a numerical form this is given by these hidden units finally the results so a model was trained in after getting a pop of the training dataset and we got a training loss of around 72% then finally if the testing time we have generated school for each of the civilians which signified the probability which will be assigned to each other symptoms of your being of getting selected or not selected in the solving and so the conclusion so basically we again use this or we can exchange some right this model for generating some reason English by using word extractor thank you",
            "videoid": "8CwbhSbbB1U",
            "viewCount": "316"
        },
        "AKwfVAKaigI": {
            "caption_exist": "T",
            "channel_id": "UCWN3xxRkmTPmbKwht9FuE5A",
            "channel_title": "Siraj Raval",
            "concepts": [
                [
                    "natural language processing",
                    1
                ]
            ],
            "description": "This video will get you up and running with your first AI Reader using Google's newly released pre-trained text parser, Parsey McParseface.\n\nThe code for this video is here:\nhttps://github.com/llSourcell/AI_Reader\n\nI created a Slack channel for us, sign up here:\nhttps://wizards.herokuapp.com/\n\nHere's the original blog post about Parsey:\nhttps://research.googleblog.com/2016/05/announcing-syntaxnet-worlds-most.html\n\nThis is Google's repo for Parsey:\nhttps://github.com/tensorflow/models/tree/master/syntaxnet\n\nIf you're interested in NLP, check out Michael Collins course. This guy is such a G (he co-authored Parsey), I took this class at Columbia and it was one of the few where I actually attended every session. (it's free and open source!):\nhttps://www.coursera.org/course/nlangp\n\nLink to API.AI in case you want to go that route:\nhttps://api.ai/\n\nThe political debate fact checker was an idea I had but never got around to building. It takes the transcript from a political debate, extracts the intent of a claim, queries it against google, perhaps scrapes some search result data and then assigns it a truthfulness rating out of 100. If it falls below a certain threshold, that person must be lying! How cool would that be? \n\nI love you guys! Thanks for watching my videos, I do it for you. I left my awesome job at Twilio and I'm doing this full time now. \n\nI recently created a Patreon page. If you like my videos, feel free to help support my effort here!:\nhttps://www.patreon.com/user?ty=h&u=3191693\n\nMuch more to come so please subscribe, like, and comment.\nFollow me:\nTwitter: https://twitter.com/sirajraval\nFacebook: https://www.facebook.com/sirajology Instagram: https://www.instagram.com/sirajraval/ Instagram: https://www.instagram.com/sirajraval/",
            "dislikeCount": "22",
            "duration": "PT5M29S",
            "likeCount": "637",
            "published_time": "2016-06-12T20:41:19.000Z",
            "tags": [
                "natural language processing",
                "nlp",
                "parsey",
                "parsey mcparseface",
                "parsing",
                "dependency tree",
                "artificial intelligence",
                "machine learning",
                "ai",
                "text analysis",
                "google"
            ],
            "thumbnail": "https://i.ytimg.com/vi/AKwfVAKaigI/hqdefault.jpg",
            "title": "Build an AI Reader - Machine Learning for Hackers #7",
            "transcript": "  what light in yonder window breaks and Julia is the son who's the father no no no she's a bright ball of gas though someone's son is Julia is the star in the sky hello world welcome to Serie geology in this episode we're going to build an AI reader that is an AI that can analyze human language this type of task is considered natural language processing or NLP NOP is the art of solving engineering problems that need to analyze or generate natural language text we see it all around us Google needs to do it to understand exactly what your search query means so they can give you back relevant results Twitter uses it to extract the top trending topics Microsoft uses for in-car speech recognition NOP is basically extremely dope because it deals with language Kurzweil once said that language is the key to AI a computer able to communicate indistinguishably from a human would be true AI there are 6500 known languages in the world and each of them have their own rules for syntax and grammar some rules are easy like I before E except after C and some one based on intuition since there is no consistent use case so how do we write code to analyze language before the 80s NLP was mostly just a bunch of hand coded rules like if you see the word dance translate it to kuduro or if a word ends an ING label it as present tense well this worked it was really tedious and there are a million corner cases it's just not a sustainable path to language understanding a way forward was and is machine learning so instead of hand coding the rules and a aya learns the rules by analyzing a large corpus or piece of text this is proven to be very useful and applying deep learning to NLP is currently the bleeding edge so when thinking about what tool to demo in this video I was really torn between an NLP API called API to AI and Google's newly released English parser parse emic parse fix file API that AI takes in an input query and returns on an analysis of the text Parsi is Google's newly released English parser both have similar functionality but I'm gonna go with Parsi because a it's currently the most accurate parser in the world be if you built it into your app that's one less networking call you have to make which means you can parse tax offline and see building this parsing logic from the source allows you to have more granular control over the details of how you want text to be analyzed Parsi was built using syntax net an NLP neural net framework for Google's tensorflow machine learning library so we could use syntax net to build our own parser or we could use a pre trained parser Parsi let's do that once you parsed your text there are a whole host of things you can do with it let's try it out with our own example we're going to build a simple Python app that uses parsing mc-- parse phase to analyze a command by a user and then repeat it back to them both where it is different we'll begin by importing our dependencies then we'll set up our program to receive and store the user input the input text is the corpus will be analyzing we can get an array of all the part of speech tags in the input tab using the tagger function so what is part of speech tagging it's assigning grammatical labels to each word in a given corpus you know all those words we learned back in elementary school so take the phrase I saw her face and now I'm a believer if we tagged each word in that phrase individually without looking at the sentence as a whole we might tag saw as a certain bird which means this would be a quote from Leatherface but if we look at this word in the context of the sentence we realize that it's a different verb Google trained Parsi by interpreting sentences from left to right for each word and the sentence and the words around it extracted a set of features like the prefix and suffix put them into data blocks concatenated them all together and fed them to a feed-forward neural net with lots of hiddenly which would then predict the probability distribution over set a possible POS tags and going in order from left to right was useful since they could use a previous words tag as a feature in the next word so what is a parse tree well part of speech tagging isn't enough there's another part the meaning behind some piece of text isn't just the type of word that's being used but also how that word relates to the rest of the sentence take the example phrase he fed her cat foo there are three possibilities of what this phrase could mean number one he fed a woman's cat some food that's the obvious one to us intuitive humans but there's also number two he found a woman some food that was intended for a cat or number three he somehow encouraged some cat food to eat something the meaning of the sentence depends on the context of each work the team used something called the head modifier construction to sort word dependencies this generated directed arcs between words like that and Cat Cat being a direct object the word debt the sentence starts out unprocessed with an initial stack of words the unprocessed segment is called the buffer at the parser it encounters words as it moves from left to right it pushes words onto the stack then it can do one of two it can either pop two words off the stack attach the second to the first which will create a dependency arc pointing to the left and push the first word back on the stack or create an arc pointing to the right and push the second word back on the stack this will keep repeating until the entire sentence is processed the system decides which way to point the are depending on the context ie previous POS tagging once that's done it uses the sequence of decision to learn a classifier that will predict appendices in the novel corpus it applies to softmax function to each of the decisions which normalizes or adjust them to a common scale and does is globally by summing up all the softmax scores in log space so the neural net is able to get probabilities for each possible decision and a heuristic called beam search helps decide on the best one when predicting them once we have our parse tree and parts of speech variables let's store the root word and the dependent object into their own variable we'll call a synonym API to retrieve a synonym for the dependent object then construct a novel sentence that repeats the command that users entered back to them in different wording looks like it works pretty well the scope of what you can do with this is so that you can use this text analysis to create a text summarizer work recognize the intent of a query or understand if a review is positive or negative or my personal favorite create a political debate back checker links with more info below please subscribe for more ml videos for now I've got to go fix a buffer overflow so thanks for watching",
            "videoid": "AKwfVAKaigI",
            "viewCount": "36016"
        },
        "AgvfJddkzvE": {
            "caption_exist": "T",
            "channel_id": "UC5zx8Owijmv-bbhAK6Z9apg",
            "channel_title": "Data Science Tutorials - All in One",
            "concepts": [
                [
                    "matrix",
                    16
                ],
                [
                    "graph",
                    12
                ],
                [
                    "node",
                    3
                ],
                [
                    "state",
                    3
                ],
                [
                    "walk",
                    3
                ],
                [
                    "text summarization",
                    2
                ],
                [
                    "linear",
                    2
                ],
                [
                    "vertex",
                    1
                ],
                [
                    "square matrix",
                    1
                ]
            ],
            "description": ".\nCopyright Disclaimer Under Section 107 of the Copyright Act 1976, allowance is made for \"FAIR USE\" for purposes such as criticism, comment, news reporting, teaching, scholarship, and research. Fair use is a use permitted by copyright statute that might otherwise be infringing. Non-profit, educational or personal use tips the balance in favor of fair use.\n.",
            "dislikeCount": "0",
            "duration": "PT10M11S",
            "likeCount": "3",
            "published_time": "2016-04-06T01:59:11.000Z",
            "tags": [
                "Natural Language Processing",
                "Language Processing",
                "University of Michigan",
                "Michigan",
                "NLP",
                "Coursera",
                "Dragomir R. Radev",
                "Computational Linguistics",
                "Linguistics",
                "Information Retrieval",
                "Computer Science",
                "Video Lecture",
                "Video Tutorial",
                "NLP Video Course",
                "Video Course",
                "Natural Language Processing Video Course",
                "Course",
                "Data Science",
                "Natural Language Processing Video Lecture",
                "Summarization Techniques",
                "Summarization",
                "NLP Summarization"
            ],
            "thumbnail": "https://i.ytimg.com/vi/AgvfJddkzvE/hqdefault.jpg",
            "title": "Summarization Techniques (3/3) | NLP | University of Michigan",
            "transcript": "  okay we're now going to continue with some additional techniques for text summarization I'm going to go very briefly over a few more papers well the first one is by John Conroy Diane only from 2001 it uses a hidden Markov models for text summarization and it is that we want to take into account the local dependencies between sentences the idea is that you don't want to include sentences in the summary randomly no independent of one another way often if you include the sentence you have to decide whether the sentence before after it should also be included so the features that I used are things like position number of terms similarity to the document terms so that which is very similar to the centroid idea in the mid paper so the hmm alternates between some again non summary states so how the probability of staying in a summary state or of leaving a summary state going to no summary state and so on all the four possible combinations so here's an example from the conroy and O'Leary paper you have green and blue sentences that tell you whether you want to include the centers in the summary or next paper is by Mouse Osborne 2002 he was the first one to take into account the fact that the features used in previous papers were actually dependent and techniques like naive Bayesian and so on should not be necessarily the best ones so in his case he used the MaxEnt or log linear model to take into account the dependencies between the different features and he got better performance than the base so the features that he used were sentence lengths sentence positioned whether the sentence is inside introduction of the document where it's inside a conclusion and so on now the next paper is by a con and another of 2004 it was published in Journal of artificial intelligence research or Jerr and this was the first paper on method code based on a random walks for multi document summarization and that technique also works for single document summaries so the idea is something called lexical centrality lexical centrality me that if a sentence is likely to be visited during a random walk process on a similarity graph corresponding to all the sentences in the set of documents then that sentence is worthy of including in the summary so the steps are the following you can present a text as graph with Santos is connected to each other if they have a lot of words in common and then you just use a standard graph centrality metric for example between a centrality I can vector centrality to determine the top sentences one of the components of leg shank is graph clustering so before you want to pick the most central sentences you want also to segment the graph into units that correspond to different themes so here's an example we have a collection of 10 or 11 sentences from different documents that correspond to the same event the first one D 1 s 1 just means that it's sent as one from document 1 the second one is sent as one from document 2 and so on we have 11 of those in total and we can now build a similarity matrix that corresponds to all the different pairs of sentences in that input now it's obvious that the diagonal entries are all ones however we are also very interested in the high values that are not on the diagonal so for example this is 0.45 value here between sentences 1 &amp; 2 so send us 1 and send us to God be very strongly connected in the graph so now let's see how we can compute the cosine centrality of this graph using a cosine cutoff of 0.3 so what we have here is 11 nodes each of which corresponds to one of the sentences in the input and only those sentence pairs that have a similarity above 0.3 are connected now as you can see this graph is still fairly disconnected and there's not much useful information that can be gained from the structure if we lower the cutoff for cosine similarity to 0.2 we're going to see much better structure in fact it was very obvious at this point that sentence d4 s1 is very highly connected to the rest of the graph where a sadness is like D 2 s 2 and D 3 s 1 are not as highly connected if we keep lowering the - hold we're going to get a situation where almost everything is connected to everything so we don't want to go that far in in there kind of a lot of paper they found that threshold of about 0.15 gives you the best information value for the graph so this approximately half of the connections are actually present and half are not present so in a graph like this what you want is four sentences to vote for the most central sentence by essentially passing messages along the edges of the graph so if D for s 1 is the most central sentence we want to produce that as part of the somewhere so here I'm going to discuss a little bit more advanced material you can skip this part if you don't feel comfortable with the linear algebra used in it so here's how Lex rank works Lex rank is the lexical centrality method used in there kind of a lot of paper so it's based on a square connectivity matrix of the where each node corresponds to a sentence it can be the directed or undirected now an eigenvalue for square matrix a is a scalar lambda such that there exists a vector X which is not a null vector such that the product a X of the matrix with that vector is equal to the product of the scalar lambda with that vector so that's some sort of a implicit direction of the matrix the normalized eigen vector associated with the largest lambda is called the principle a convective alpha and a matrix is called the stochastic matrix when the sum of entries in each row sum to 1 and none of them is negative so they all from some probability distribution and there is a theorem that says that all stochastic matrices have a principal eigenvector so the connectivity matrix in this kind of setup is similar to the one that is used in PageRank for a document ranking system behind Google and it's also not to be a reducible so one can use an integrative power method to compute the principal investor for pretty much arbitrarily large matrices so that eigenvector corresponds to the stationary value of the Markov stochastic process described by the connectivity matrix essentially random walk over the nose of the matrix in proportion to the weights of the edges and the stationary value of the markov matrix is computed by that power method the power method is something very straight forward P is the vector of values that correspond to the centrality of the nodes each transposed is a transpose of the connectivity matrix so if we have the eigenvector formula p equals e transpose p we can also write this as AI minus ETS post P equals zero where I is the matrix that has ones on the diagonal and zeros everywhere else and then in PageRank D is also an added twist to deal with dead-end pages so if you end up in a node that doesn't have any outgoing edges then it's possible with the probability 1 minus epsilon to start randomly from a different page so the value of a node P of V the vertex is equal to 1 minus epsilon divided by n so this is the probability of the teleportation a random jump plus epsilon the sum of the normalized values of the centrality for the adjacent nodes where PR are the nodes that are connected to V and eigenvector centrality it is computed in the following way the paths in the random walk I just waited by the centrality of the know that the path connects so in general the lexing method was found to be very successful for an evaluation of summarization based on duck where duck is the nice new summarization corpus and this was an official evaluation used for many years thousands so the next paper that I want to mention very briefly is by going in Leo 2001 this is the first paper that uses latent semantic analysis LSA something which we have talked about in the past in this class it works on both single and multi document summarization cases and doesn't use any explicit semantics in linguistics for example were denied so each document is represented as a word by sentence matrix where each row corresponds to word and each column corresponds to a sentence the weights in the matrix are based on the tf-idf values of the words so LSA as we remember from a previous lecture is based on the singular value decomposition so we want to present a matrix a as the product of you Sigma and V transpose where the roles of V transpose I depend on topics that correspond to the documents and then we want to pick the sentences that contains the independent topics so that's in summary how the Gong a new method works so we're going to continue with evaluation of summarization in the next",
            "videoid": "AgvfJddkzvE",
            "viewCount": "651"
        },
        "BJ0MnawUpaU": {
            "caption_exist": "T",
            "channel_id": "UC8zMHlsBK_fAzvmqbXR8mvA",
            "channel_title": "Mike Bernico",
            "concepts": [
                [
                    "matrix",
                    19
                ],
                [
                    "stop words",
                    6
                ],
                [
                    "list",
                    5
                ],
                [
                    "sparse matrix",
                    3
                ],
                [
                    "mean",
                    2
                ],
                [
                    "shape",
                    1
                ],
                [
                    "language",
                    1
                ],
                [
                    "array",
                    1
                ]
            ],
            "description": "",
            "dislikeCount": "6",
            "duration": "PT14M34S",
            "likeCount": "297",
            "published_time": "2015-02-01T19:26:54.000Z",
            "tags": [
                "latent semantic analysis",
                "Text Mining (Industry)",
                "Semantic Analysis",
                "data science"
            ],
            "thumbnail": "https://i.ytimg.com/vi/BJ0MnawUpaU/hqdefault.jpg",
            "title": "Text Analytics - Latent Semantic Analysis",
            "transcript": "  hey everyone so I want to show you today a technique called latent semantic analysis and what I'm going to do is I'm going to show you how we can mine concepts from a collection of documents my goal here is to give you a reason why to do tf-idf so we talked about tf-idf we're probably going to do more tf-idf later on in class but we haven't covered much machine learning yet we haven't covered a lot of the models that we're going to build so I want to show you just a quick easy way to make tf-idf useful alright so I'm going to be doing some some analysis on the forum posts from week zero so if you remember in week zero I asked everyone to go post on our forum what is your definition of data science after having watched Josh wills and Hilary Mason's videos okay so you did that I downloaded that data into an XML file and I'm going to use this program to parse it and so what I'm going to do is I'm going to open all of the entire XML file just into a great big blob I'm going to use this thing in in Python called beautiful soup and beautiful soup is really good for for parsing XML and for HTML and what I'm going to do is I've gone in and I found that all of the posts that are in the raw XML are wrapped in text tags so then what I'm going to do is I'm going to use beautifulsoup to find all the text tags and what I'm going to get is I'm going to get a list of documents and I'm going to do a little bit of clean-up on them and so now I have a corpus of documents that where each document is your posts from week 0 okay before I get into a of the tf-idf vectorizing I want to introduce you to a concept that you haven't probably heard yet and it's stop words and so stop words are words that I don't want to convert into features because they're just not especially useful so words like a and and and the I don't even need to consider these words so I'm just going to use I'm just going to throw them out and also using stop words gives me the opportunity to filter any junk that's in the data and in this case there's a lot of HTML junk that's left over from the download and I'm going to clean that out too so my initial set of stop words is going to come from stop words at words English and this is part of NLT K which is the Python natural language toolkit and I'm going to add these words to it as well and these are all a collection of HTML junk that I didn't want my analysis and I found these as remanence body you know just kind of going through the data alright so now that we have a list of documents a corpus of documents I'm going to convert want to convert them with tf-idf vectorizing here's an example of the very first element in my list so the very first document and this is one of the posts you can see there's some of that HTML junk still left over all right instead of writing my own tf-idf vectorizer like I did last time I'm going to use the tf-idf vector vectorizer that comes with scikit-learn it's much easier I'm right here using stop set which I defined above as my stop words words to throw out use IDF equals true you don't have to use IDF's but it's definitely going to be a good idea here and gram range is pretty interesting because what this lets us do is look at not only tokenized by single words but we could also look at it by grams which is occurrences of two words and trigrams occurrences of three gram of three words and we're going to look at all of those and there could be two words that occur together that could be important or three words they occur together it could be important so we're going to look at all of those and so X in whenever you see an uppercase X like this you can assume that this means a matrix as opposed to if it were a lowercase X it would probably mean a vector or just a single variable that's just by convention that's how machine learning people tend to work so X is going to be our input set it is going to be a matrix and we're going to call fit transform on the set of documents the corpus and then what we can see is X 0 which is the transformed first document in the corpus transformed to tf-idf is going to be a row vector sparse matrix of 3393 elements with 89 stored elements okay what the heck does that mean I just sprung a bunch of stuff on you all right if you remember back from our last example we saw that when we use tf-idf what happened is all of the unimportant words or the words that weren't occurred that didn't occur across them that occurred commonly across everything they just kind of washed out to zero well also we saw that you know there there could be many documents there are many words in it that occurred in one document in the corpus that don't occur in document 0 and so they wouldn't have a score associated with them going to exist so a sparse matrix is a really handy way to save some memory by only storing the elements that do exist and have a score so we can see in our print document 0 here in its sparse matrix form and what you can see is that even though there are 3393 elements in the matrix on the array in this case really we only store we only score 89 of them and here they are so this is going to be 0 by 647 so this could interpreted it as a 647 'the word and this would be its tf-idf score not to worry though I know that probably seems confusing but we're it's going to all be handled for us by scikit-learn and by python okay so now we have a corpus of tf-idf transformed documents great here comes the LSA part latent semantic analysis what we're going to do is we're going to take X which is our matrix our input matrix where m is the number of documents and n is the number of terms so imagine a document by term matrix and I'm going to take that that matrix and I'm going to decompose it into three matrices called us and T and when we do this decomposition we have to pick a value K and that's how many concepts we're going to keep in this case I chose 27 because I wanted to keep as many as there were documents but that's an arbitrary number okay and so what happens is X when we decompose this matrix X what we get is three matrices us and V transpose that when multiplied together approximately equal X if you've never seen matrix decomposition that's you really don't need to know anything more than I just said what we do is we solve for three matrices the more or less multiply to be X good enough okay um there's some important things that happen and I won't get too deep into the math on why they happen but what happens is you will be an M by K matrix where the rows will V be the documents and the columns will be mathematical concepts that's kind of weird to think about but what we're going to learn are concepts that were contained within the documents s is going to be a diagonal matrix and the elements will be the amount of variation captured from each concept and then V is going to be an M by K matrix watch the transpose where the rows will be the terms and the columns will be the concept so now this is going to be a term by concept matrix and this is the one that we're actually going to be the most interested in because we're going to want to understand which terms comprise a concept or which terms can be used to describe a concept because concepts are a mathematical construct and they don't really have word labels okay so just as a reminder the shape of X is 27 documents by 3393 terms and it's the reason there's so many is because it not only counts the single terms but also the by grams and trigrams all right so instantiate an instance of truncated SVD that's truncated singular value decomposition which is the engine I'm going to use to decompose my matrix and by calling LSA dot fit on my matrix I'm going to decompose it you can see this happened and this thing called LSA dot components underscore is what gets created and it represents V which is the term by concept matrix and so if I look at zero what I'll see are these are all of the terms that go with concept zero all right well so that doesn't really help me it's just a bunch of numbers how do we know how do we know what terms those are okay well so the first important thing to know is these are the importances or the how much how important that term is to that concept and the position zero one two three four and so on relates to the position of the term so I can get a list of all of the terms in that same order by calling vectorizer which is my tf-idf vectorizer dot get feature names and that will give me a list of the terms and then what I can do is for each of those components I can go through them and I can take the terms and stick them to the importance is the component I'm to the concept important and then I can sort them and I'm going to only print the first ten term so the first 10 terms that make them up and then here we go alright so when I run that loop what you'll see is concept zero alright concept zero has these is the most important words data procedures that doesn't tell us much but look at this large amounts of data large amounts amounts of data so this is really that concept LSA has identified here that concept of big data large amounts of data it's something that almost all of you wrote about and the concept was identified here is across all of your documents something that's important all right concept number one there's some data data science so those are words that we use a lot but answers and decisions questions findings so finding the answers making decisions you know answering questions that's really what we're talking about in concept one concept three is an interesting one here too if you notice I skipped concept two because it looked kind of like I had quite a bit of junk in it things that I didn't filter out quite as well as I should have excuse me for that so let's move on to concept three concept three you can see we make better decisions use good better make better use data right so we're using data science to make better that's pretty awesome we found we identified that so what do we have so far we have using making better decisions using big data and finding answers to questions let me write those down just right below so using big data answering questions alright I think that that's good next one so analyzing information that in creative thinking in science I like that those are all good creative thinking analyzing information those are two pretty different things but they're kind of conceptually the same to the computer and then this next one is about data science and data scientists and it also captured the idea that someone had used about data science compared to archeology in history and I like that too and so I can go on and on but what you can see here is that the computer has started to piece together some concepts they kind of come up with an an average conceptual idea of what data science is based on your posts and you know I don't know what you think about that but I think that's pretty cool if you think that's cool too let me know post about it in the forums and also that you're going to have some homework that looks a lot like this and I'll have that up shortly thanks a lot bye",
            "videoid": "BJ0MnawUpaU",
            "viewCount": "39393"
        },
        "GX4RTIIuxy8": {
            "caption_exist": "T",
            "channel_id": "UC95bRAbJdLyXGbMfNemr9lQ",
            "channel_title": "Data Science, ML & AI Tutorials by Dr. Niraj Kumar",
            "concepts": [
                [
                    "algorithm",
                    15
                ],
                [
                    "graph",
                    12
                ],
                [
                    "node",
                    11
                ],
                [
                    "degree",
                    8
                ],
                [
                    "pagerank algorithm",
                    7
                ],
                [
                    "system",
                    6
                ],
                [
                    "link",
                    5
                ],
                [
                    "page rank algorithm",
                    5
                ],
                [
                    "common words",
                    2
                ],
                [
                    "form",
                    1
                ],
                [
                    "reprocessing",
                    1
                ],
                [
                    "air",
                    1
                ],
                [
                    "undirected graph",
                    1
                ],
                [
                    "list",
                    1
                ],
                [
                    "jungle",
                    1
                ],
                [
                    "stable",
                    1
                ]
            ],
            "description": "This video tutorial explains, graph based document summarization system (developed by using pagerank algorithm). A java implementation of the system is also demonstrated. The supporting code for the entire demonstration is available at:\n\nhttps://sites.google.com/site/nirajatweb/home/technical_and_coding_stuff/textrank-and-lexrank-based-single-document-summarization",
            "dislikeCount": "2",
            "duration": "PT14M44S",
            "likeCount": "23",
            "published_time": "2016-04-10T13:52:57.000Z",
            "tags": [
                "Single Document Summarization",
                "Graph Based Summarization",
                "Pagerank based Text Summarization",
                "Pagerank",
                "Sentence Graph",
                "Information Retrieval",
                "Text Mining",
                "NLP"
            ],
            "thumbnail": "https://i.ytimg.com/vi/GX4RTIIuxy8/hqdefault.jpg",
            "title": "Document Summarization PART-1: Pagerank based document summarization)",
            "transcript": "  in this session I will explain you how to develop your own single document summarization system to identify a summary of to three sentences for any given news articles for this I will use a simple graph this algorithm known as PageRank so before going to discuss the entire summarization system first of all I will explain how PageRank algorithm works and how we can use PageRank algorithm to summarize the news article now suppose this is a simple undirected graph now here the PageRank L is rank of any node suppose s 1 can be defined as PageRank of all a decent node divided by its degree so it will be like PageRank score of node s 2 divided by its degree its degree is 2 plus page rank score of node s 4 divided by its degree which is 3 similarly page rank score of node s 4 can be calculated as pagerank a score of node s1 actually s4 is connected with is 3 s 1 and s 5 so it will be like PageRank a score of node s 1 divided by its degree plus score of node s 3 divided by its degree plus this rank is 4 of nodes is 5 divided by its degree and so on now we can formulate the entire algorithm in this form where D is known as damping factor actually the PageRank algorithm hold 30 volts and imaginary surfer who is randomly clicking on the links from here to here or from here to here and will eventually stop clicking to define this step means the probability that any step that the person will continue is a damping factor and is defined as D and here n is the total number of nodes in the graph in this case the total number of nodes is 6 sorry and this represents PageRank of any node U is equal to damping factor divided by n plus 1 minus D into summation of PageRank a score of P divided by degree of the air V is a member of adjacent u means if we consider this node as you then all the nodes which will be we will means all the nodes like s 2 and s 4 will be member of B so this is a simple formula to calculate the page rank is 4 now how to apply this simple PageRank algorithm to calculate the text to rank the texts text so that we can calculate the summary of that augment now for this first of all we will copy the entire text however you can take simple HTML based parser or you can copy the text now we have copied the entire text and it is given here now how to use the text rank algorithm or page rank algorithm to rank all these sentences so for this I will go like that now this is the first sentence of the text that we had copy like here after that we apply a simple pre-processing stable this pea processing step will contains Burnley noun and adjective we will remove all other terms like there be a you etc actually it is believed that the combinations of noun and adjective gives better results now our sentence pre-process sentence will be a sequence of noun an adjective now to calculate the noun and adjective we a simple Stanford POS tagger this is implemented here in document pre-processing step we have used first of all we apply simple cleaning in the cleaning stage we use a lemon tie Jason steps actually lemon hydration step just removes simple impurities a sorry simple apply simple cleaning which is not like a stemming and after that now the next part is how to prepare the graph we take each sentence as a node of graph now suppose we represent this sentence as s1 then word is represented as w1 see it is represented as w2 Olympic is represented as WP and suppose games may be represented as W 30 and Rho W 14 and Rio represented as W 15 and the summer may be represented as W 60 now for its sentences of the document may be represented like this now we will prepare the graph like there may be a link between s 1 and s 2 if we have some common words for example here w1 and w2 are common W 3 and W 3 is common so there will be a link between sentence s 1 and sentence s 2 with link weight 2 similarly there will be a link between s 1 and s 4 if it helps it has if we can find some common words like W 1 and W 1 similarly W 2 and W 2 so it's a link weight maybe will be 2 and by using this we can prepare the entire graph after preparing the graph we can apply the PageRank algorithm and the PageRank algorithm will give you the rank list of nodes suppose here it gives like s for highest rank then s3 second highest rank then s1 as third highest rank s2 has a fourth highest rank and then s5 and then s6 now we can pick top 2 or top 3 sentences at a summary of the document now we will go through the entire code description so this is the entire code so first of the actually the code contains Stanford lament ideation system implemented is the sentences and after that and after the 11th edition it generates a p2p process to text so first of all we'll go through the main program and I will run this program so program is complete now we can see the output so output is the pre process text so this is the be processed text so this is the actual actual text and this is the after pre processing you can see the text like this contains only noun an similarly the similarly you can find the second sentence and after process pre-processing you can find just down and adjectives and this is a final summary this is the some relation score so it says that ninth sentence will have a a strength and after that second highest score will be for sentence first so you can say that by using these two sentences you can present all the informations majority of the information contained in the page or if you have more space then you can take top three sentences add a summary now I will go through some basic things like page rank algorithm how I have implemented the page rank algorithm so this is the general part of page rank algorithm to implement the page rank algorithm we have used Java Universal network graph library this is jungle Iridum so used Java Universal network graph library you can find the complete code at my website technical and coding stuff to know the page rank how it is implemented you can see with through an example you can use this example and directly run the system but to run the system first of all you have to confirm that you call all the important abilities first important libraries may be used in document reprocessing which contains some open NLP tools or sentence separation and then Stanford core NLP tools for POS tagging and then Java reversal network graph liability algorithms jar file that you can get so this is the age from where you can download java universal network graph library to run the PageRank algorithm and after that this is a completely working system for your summarization hope it will work",
            "videoid": "GX4RTIIuxy8",
            "viewCount": "4243"
        },
        "NDPyjZkblJc": {
            "caption_exist": "T",
            "channel_id": "UCDe6Dv8nq-FHFf5sNdJ8LHQ",
            "channel_title": "Shashank Nainwal",
            "concepts": [
                [
                    "language",
                    7
                ],
                [
                    "stop words",
                    7
                ],
                [
                    "natural language processing",
                    4
                ],
                [
                    "common words",
                    4
                ],
                [
                    "functions",
                    4
                ],
                [
                    "string",
                    2
                ],
                [
                    "algorithm",
                    2
                ],
                [
                    "heap",
                    1
                ],
                [
                    "pattern",
                    1
                ],
                [
                    "list",
                    1
                ],
                [
                    "key",
                    1
                ]
            ],
            "description": "",
            "dislikeCount": "8",
            "duration": "PT12M1S",
            "likeCount": "39",
            "published_time": "2017-05-07T05:27:48.000Z",
            "tags": [],
            "thumbnail": "https://i.ytimg.com/vi/NDPyjZkblJc/hqdefault.jpg",
            "title": "24   A Serious NLP Application   Text Auto Summarization using Python",
            "transcript": "  when tackle a pretty challenging problem in this class how do you order summarize a piece of text or a newspaper articles which is downloaded transparent we implemented using natural language processing in python the source code for implementing this is attached as an external resource to this lecture and you can follow our log as I'm explaining the code in this lecture let's tackle a difficult problem now the objective is to take in the URL of a newspaper article from the Washington Post and automatically summarize it in three sentences how will we do this we'll use something called natural language processing this will allow us to understand what are the patterns in a piece of text let us review our tweet or any article in a newspaper you might have seen this before in different websites where some reviews and articles have one sentence to describe them what we are going to do is X summarization this is a pretty challenging problem so what we will do is we'll make use of the amazing libraries that Python provides let's go through the steps involved in doing this the first step is to download the contents of the URL so whichever article we're interested in get its URL and then download the contents of this URL using some standard libraries and Python the second is to extract the article from all the other HTML that is on the page so normally when you download a webpage all that X should be contained within some divs and tags get rid of all of this and get the X from the webpage then figure out the three most important sentences in the article now obviously this is the key we'll talk about how to do this in a minute extracting the article from all the other html's us on the webpage can be a little tricky the usual approach to do this is to try something called like regular expressions regular expressions allow you to parts text and figure out patterns and yet text which corresponds to a certain pattern but this is not very reliable way of doing things luckily Python has this amazing text scraping library which is called beautiful soup you can use this library to figure out all the text that is between the HTML and the div tags on the webpage and extract it out okay now that you have all the text for the article which is on this web page how do you figure out which are the three most important sentences in that article we'll use a natural language processing algorithm that will implement using pythons NLP library this is called NLP K or natural language toolkit NLT K comes with a lot of built-in functions which are really powerful it has functions to help you organize articles into sentences organized means breaking up it has functions which will help you tokenized sentences into words and it also comes with ways to remove stopped words it comes as built-in sets of stop words stop words are just words that are really common in the English language like the is an which you don't want to include when you are doing X summarization NLT K also comes with corpora or large bodies of text that you can download and play with ok let's get to the real details how do we figure out the most important sentence in an article here is one approach first we'll find the most common words in the article then find the sentence in which those most common words occur the most often boom that's the most important sentence of course there are a few wrinkles when you find the most common words in the article we need to eliminate words like though is an etc which are really common words and don't add much in terms of information to the article summary these words are called stop words now ordinarily you would have to come up with a list of stop words that you would not want to include in your calculations but NL DK does this for you it comes in with built-in sets of stop words that you could use to figure out which words to eliminate from your calculations all right let's go through the algorithms that we need to do in detail so as we said the first step is to download the article from the URL this is pretty straightforward and you can do this using standard libraries in Python next we'll get rid of everything other than the article itself so all the crud that's in the article the HTML the divs the tags we need to get rid of all of that the way to do this is to use the Python library beautifulsoup next we come to a whole bunch of natural language processing this will do using NL DK which is another Python library we basically need to start by splitting the article into words this is called tokenizing and NL DK has built-in functions to allow you to tokenize articles into sentences and sentences into words the next step is to eliminate stock words words like the is an which occurs really commonly in the English language now take the remaining words and find out how often each remaining word occurs this is the frequency of the word in that article now the more common a word appears the more important it is this is just how we have decided to decide the importance you could use other features as well like how often the word appears in the headline and so on then for each sentence find the score of how important the words in that sentence are so you take each sentence in the words in that sentence and see how important they are drank all the sentences by that score let's take a look at some of the code that you will need to do this as you know the first step is to download the article from a URL and parse it to get the text of the article here is the code that will help you to do this this is a function that takes in the URL of an article in The Washington Post and it sends returns the article - all of the crud it removes all the HTML Javascript the tags etc which are written in the text of a webpage notice this line of code here which downloads the page from that URL this is done using a standard library in Python called URL Lib through the next step is to use beautifulsoup beautifulsoup will help you remove all the HTML they've sent arcs which are present or extract the articles from those tags this is the code that will RIM help you remove the HTML if sandbags this part of the code takes in a token this is a token in between which all the text of the article is kept this is specific to the structure of the webpage that you're passing it then finds all the text between back token and it musha's it together then it takes all the P tags within that text so it finds the paragraphs within that text and musha's it all together so we get one string with the text of the article and without any tags like article or P once it's done with this it basically returns the text of the article as one string and the title of the article so it returns a tuple which is the text and now this code only works for URLs where we know the structure in particular this code works for the Washington Post Washington Post uses a structure which has the article contained inside the tags article okay next we need to do all the steps that we described in the NLP portion of our algorithm we need to compute the frequencies of words in an article first we will contain all the behavior that we want inside a class this class is called the frequency summarises make a note of this because this class is what we use going ahead in other exercises as well the constructor of this class basically has a set of stop words that we've initialized this is in case the user doesn't pass in any stop words this is a default set of stop words that will be used which is what is provided by n LD k then there are cut-offs to remove words that are appearing too often or too little the rest of the code is pretty simple it just goes through or iterates through each word in each sentence of the text of the articles and computes the frequencies for those words if the frequency is below the cutoff or above a cutoff it just removes that word now that we have the frequencies of all the words in the article the next step is to summarize the article here's the code that will help us do that we first start by breaking up the article into sentences this is done by using a send underscore tokenized function which is available in alice NL ticket then we break up each sentence into words this is again done using word and the score tokenize which is a function in NLP k now we need to compute the rank of a sentence how important that sentence is for that article we'll do this by taking each sentence taking the words in that sentence and summing up the frequencies of those words in the entire article so the more important or a more often a word appears in the article if that word appears in this sentence the sentence becomes more important once we have calculated the rank of each sentence we can order them we do this by using a function called n largest this is available in a Python library called heap you then return the top-ranked sentences as the article summary that's it you have summarized [Music]",
            "videoid": "NDPyjZkblJc",
            "viewCount": "4788"
        },
        "ogrJaOIuBx4": {
            "caption_exist": "T",
            "channel_id": "UCWN3xxRkmTPmbKwht9FuE5A",
            "channel_title": "Siraj Raval",
            "concepts": [
                [
                    "matrix",
                    7
                ],
                [
                    "language",
                    3
                ],
                [
                    "training data",
                    3
                ],
                [
                    "link",
                    2
                ],
                [
                    "natural language processing",
                    1
                ],
                [
                    "text summarization",
                    1
                ],
                [
                    "algorithm",
                    1
                ],
                [
                    "list",
                    1
                ],
                [
                    "subset",
                    1
                ],
                [
                    "head",
                    1
                ]
            ],
            "description": "I'll show you how you can turn an article into a one-sentence summary in Python with the Keras machine learning library. We'll go over word embeddings, encoder-decoder architecture, and the role of attention in learning theory.\n\nCode for this video (Challenge included):\nhttps://github.com/llSourcell/How_to_make_a_text_summarizer\n\nJie's Winning Code:\nhttps://github.com/jiexunsee/rudimentary-ai-composer\n\nMore Learning resources:\nhttps://www.quora.com/Has-Deep-Learning-been-applied-to-automatic-text-summarization-successfully\nhttps://research.googleblog.com/2016/08/text-summarization-with-tensorflow.html\nhttps://en.wikipedia.org/wiki/Automatic_summarization\nhttp://deeplearning.net/tutorial/rnnslu.html\nhttp://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/\n\nPlease subscribe! And like. And comment. That's what keeps me going.\n\nJoin us in the Wizards Slack channel:\nhttp://wizards.herokuapp.com/\n\nAnd please support me on Patreon:\nhttps://www.patreon.com/user?u=3191693\nFollow me:\nTwitter: https://twitter.com/sirajraval\nFacebook: https://www.facebook.com/sirajology Instagram: https://www.instagram.com/sirajraval/ Instagram: https://www.instagram.com/sirajraval/",
            "dislikeCount": "91",
            "duration": "PT9M6S",
            "likeCount": "2052",
            "published_time": "2017-03-17T21:38:15.000Z",
            "tags": [
                "deep learning",
                "text summarization",
                "summarizer",
                "automatic",
                "machine learning",
                "coursera",
                "udacity",
                "andrew ng",
                "NLP",
                "natural language processing",
                "word embeddings",
                "word vectors",
                "word2vec",
                "glove",
                "global vectors",
                "python",
                "java",
                "ruby",
                "C++",
                "programming",
                "coding",
                "data science",
                "AI",
                "ML",
                "artificial intelligence",
                "neural network",
                "neural net",
                "tensorflow",
                "google",
                "LSTM",
                "recurrent",
                "RNN"
            ],
            "thumbnail": "https://i.ytimg.com/vi/ogrJaOIuBx4/hqdefault.jpg",
            "title": "How to Make a Text Summarizer - Intro to Deep Learning #10",
            "transcript": "  hello world it's Suraj and we're going to make an app that reason article of text and creates a one sentence summary out of it using the power of natural language processing language is in many ways the seat of intelligence it's the original communication protocol that we invented to describe all the incredibly complex processes happening in our neocortex do you ever feel like you're getting flooded with an increasing amount of articles and links and videos to choose from as this data grows the importance of semantic density does as well how can you say the most important thing in the shortest amount of time having a generated summary lets you decide whether you want to deep dive further or not and the better it gets the more will be able to apply it to more complex language like that in a scientific paper or even an entire book the future of NLP is a very bright one interestingly enough one of the earliest used cases for machine summarization was by the Canadian government in the early 90s for a weather system they invented called fog instead of sifting through all the meteorological data they had access to manually they'll and fog read it and generate a weather forecast from it on a recurring basis it had a set textual template and it would fill in the values for the current weather given the data something like this it was just an experiment but they found it sometimes people actually prefer the computer-generated forecast to the human ones partly because the generated ones use more consistent terminology a similar approach has been applied in fields with lots of data that needs you man readable summaries like finance and in medicine summarizing a patient's medical data has proven to be a great decision support tool for doctors most summarization tools in the past were extracted they selected an existing subset of words or numbers from some data to create a summary but you and I do something a little more complex than that when we summarize our brain builds an internal semantic representation of what we've just read and from that we can generate a summary this is instead an abstracted method and we can do this with deep learning what can't we do with it so let's build a text summarizer that a headline from a short article using care of we're going to use this collection of news articles as our training data will convert it to pickle format which essentially means converting it into a raw ice-cream pickling is a way of converting a Python object into a character stream so we can easily reconstruct that object in another Python script modularity for the win we're saving the data as a tuple with the heading description and keywords the heading and description are the list of headings and their respective articles in order and the keywords are akin to tags but we won't be using those in this example we're going to first tokenize or split up the text into individual words because that's the level we're going to deal with this data in our headline will be generated one word at a time we want some way of representing these words numerically and he'll coined the term for this called word embeddings back in 2003 but they were first made popular by a team of researchers at Google when they released word Tyvek inspired by boys to men just kidding word - Bek is a two layer neural net trained on a big label text corpus it's a pre trained model you can download it takes a word as its input and produces a vector as its output one vector per word creating word vectors lets us analyze words mathematically so these high dimensional vectors represent words and each dimension encodes a different property like gender or title the magnitude along each axis represents the relevance of that property to a word so we could say king + man - woman equals Queen we can also find the similarity between words which equates to distance what de Becque offers a predictive approach to creating word vectors but another approach is count based and a popular algorithm for that is glove short for global vectors it first construct a large co-occurrence matrix of words by context for each word ie row it will tell how frequently it sees it in some context which is the column since the number of contexts can be large it factorizes the matrix to get a lower dimensional matrix which represents words by features so each row has a feature representation for each word and they also train it on a large text corpus both perform similar we weld like glove trains a little faster so we'll go with that will download the pre trained glove for vectors from this link and save them to disk then we'll use them to initialize an embedding matrix with our tokenized vocabulary from our training data we'll initialize it with random numbers and copy all the glove weights of words show up in our training vocabulary and for every word outside this embedding matrix will find the closest word inside the matrix I'm measuring the cosine distance of glove vectors now we've got this matrix award embeddings that we could do so many things with so how are we going to use these word embeddings to create a summery headline for a novel article we feed it let's back up for a second then geo squad first introduce a neural architecture called sequence to sequence in 2014 that later inspired the Google brain team to use it for text summarization successfully it's called sequence of sequence because we are taking an input sequence and outputting not a single value a sequence as well we're going to encode then we decode when I feed it a book is set to rise and when I decode that I mesmerize so we use two recurrent networks one for each sequence the first is the encoder Network it takes an input sequence and creates an encoded representation of it the second is the decoder Network we feed it as its input that same encoded representation and it will generate an output sequence by decoding it there are different ways we could approach this architecture one approach would be to let our encoder network learn these embedding from scratch by feeding it our training data but we're taking a less computationally expensive approach because we already have learned embeddings from gloves when we build our encoder lsdm network will set those pre trained embeddings as our first layers weights the embedding layer is meant to turn input integers into fixed size vectors anyway we've just given it a huge head start by doing this and when we train this model it will just fine tune or improve the accuracy of our embeddings as a supervised classification problem where the input data is our set of go cap words and the labels are their associated headline words will minimize the cross entropy loss using RMF now for our decoder our decoder will generate headlines it will have the same LS pm architecture as our encoder and will initialize its weights using our same pre-trained glove embeddings it will take as input the vector representation generated after feeding in the last word of the input text so it will first generate its own representation using its embedding layer and the next step is to convert this representation into a word but there is actually one more step we need a way to decide what part of the input we need to remember like names and numbers we talked about the importance of memory that's why we use lsdm cells but another important aspect of learning theory is attention basically what is the most relevant data to memorize our decoder will generate a word as its output and that same word will be fed in as input when generating the next word until we have a headline we use an attention mechanism when outputting each word in the decoder for each output word it computes a weight over each of the input words that determines how much attention should be paid to that input word all the weights sum up to one and are used to compute a weighted average of the last hidden layers generated after processing each of the inputted words will take that weighted average and input it into the softmax layer along with the last hidden layer from the current step of the decoder so let's see what our model generates for this article after training all right we've got this headline generated beautifully and let's do it once more for a different article couldn't have said it better myself so to break it down we can use retrained word vectors using a model like love easily to avoid having to create them ourselves to generate an output sequence of words given an input sequence of words we use a neural encoder decoder architecture and by adding an attention mechanism to our decoder it can help it decide what is the most relevant token to focus on when generating new texts the winner of the coding challenge in the last video is Jieun C he wrote an AI composer in 100 lines of code last week's challenge was non-trivial and he managed to get a working demo up so definitely check out his repo wizard of the week the coding challenge for this video is to a sequence the sequence model with Terra's to summarize a piece of text oats your github link in the comments and I'll announce the winner next video please subscribe for more programming videos and for now I've got to remember to pay attention so thanks for watching",
            "videoid": "ogrJaOIuBx4",
            "viewCount": "108947"
        },
        "xj5ZvWgPqKM": {
            "caption_exist": "T",
            "channel_id": "UCU9GTVEPqlSNRDHypVf3BRw",
            "channel_title": "Story by Data",
            "concepts": [
                [
                    "language",
                    10
                ],
                [
                    "natural",
                    7
                ],
                [
                    "natural language processing",
                    4
                ],
                [
                    "key",
                    2
                ],
                [
                    "work",
                    1
                ],
                [
                    "algorithm",
                    1
                ],
                [
                    "power",
                    1
                ]
            ],
            "description": "In this video I want to highlight a few of the awesome things that we can do with Natural Language Processing or NLP. NLP basically means getting a computer to understand text and help you with analysis.\nSome of the major tasks that are a part of NLP include:\n\u00b7        Automatic summarization\n\u00b7        Coreference resolution\n\u00b7        Discourse analysis\n\u00b7        Machine translation\n\u00b7        Morphological segmentation\n\u00b7        Named entity recognition (NER)\n\u00b7        Natural language generation\n\u00b7        Natural language understanding\n\u00b7        Optical character recognition (OCR)\n\u00b7        Part-of-speech tagging\n\u00b7        Parsing\n\u00b7        Question answering\n\u00b7        Relationship extraction\n\u00b7        Sentence breaking (also known as sentence boundary disambiguation)\n\u00b7        Sentiment analysis\n\u00b7        Speech recognition\n\u00b7        Speech segmentation\n\u00b7        Topic segmentation and recognition\n\u00b7        Word segmentation\n\u00b7        Word sense disambiguation\n\u00b7        Lemmatization\n\u00b7        Native-language identification\n\u00b7        Stemming\n\u00b7        Text simplification\n\u00b7        Text-to-speech\n\u00b7        Text-proofing\n\u00b7        Natural language search\n\u00b7        Query expansion\n\u00b7        Automated essay scoring\n\u00b7        Truecasing\nLet\u2019s discuss some of the cool things NLP helps us with in life\n1.      Spam Filters \u2013 nobody wants to receive spam emails, NLP is here to help fight span and reduce the number of spam emails you receive. No it is not yet perfect and I\u2019m sure we still all still receive some spam emails but imagine how many you\u2019d get without NLP!\n2.      Bridging Language Barriers \u2013 when you come across a phrase or even an entire website in another language, NLP is there to help you translate it into something you can understand.\n3.      Investment Decisions \u2013 NLP has the power to help you make decisions for financial investing. It can read large amounts of text (such as news articles, press releases, etc) and can pull in the key data that will help make buy/hold/sell decisions. For example, it can let you know if there is an acquisition that is planned or has happened \u2013 which has large implications on the value of your investment\n4.      Insights \u2013 humans simply can\u2019t read everything that is available to us. NLP helps us summarize the data we have and pull out meaningful information. An example of this is a computer reading through thousands of customer reviews to identify issues or conduct sentiment analysis. I\u2019ve personally used NLP for getting insights from data. At work, we conducted an in depth interview which included several open ended response type questions.  As a result we received thousands of paragraphs of data to analyze. It is very time consuming to read through every single answer so I created an algorithm that will categorize the responses into one of 6 categories using key terms for each category. This is a great time saver and turned out to be very accurate.  \nPlease subscribe to the YouTube channel to be notified of future content! Thanks!\n \n \nhttps://en.wikipedia.org/wiki/Natural_language_processing\nhttps://www.lifewire.com/applications-of-natural-language-processing-technology-2495544",
            "dislikeCount": "2",
            "duration": "PT3M53S",
            "likeCount": "33",
            "published_time": "2017-02-02T20:28:51.000Z",
            "tags": [
                "Tableau tableau coach tutorial videos tableau tutorial business intelligence tool tableau software",
                "data science",
                "data scientist",
                "journey to data science",
                "NLP",
                "natural language processing",
                "spam filter",
                "investment decision",
                "data into insights",
                "language translation"
            ],
            "thumbnail": "https://i.ytimg.com/vi/xj5ZvWgPqKM/hqdefault.jpg",
            "title": "Amazing Things NLP Can Do!",
            "transcript": "  hello everyone today I want to talk about the natural language processing or NLP and some of the amazing things that you can do with NLP natural language processing basically means getting a computer to understand text and help you with your analysis so let's talk about some of the major tasks that are a part of NLP but I have some of them listed here basically we have automatic summarization core reference resolution discourse analysis machine translation morphological segmentation named entity recognition or any are natural language generation natural language understanding optical character recognition or OCR we have part of speech tagging textual parsing question answering relation extraction send in the breaking sentiment analysis which is pretty popular now speech recognition speech segmentation takata contagion and recognition we can do also word segmentation word sense disambiguation something called a lemon cessation or native language identification you can do stemming text simplification text-to-speech texting natural language search as well as query expansion and automated essay scoring and true casing so let's discuss some of the cool things that natural language processing can help us with in life the first thing I want to talk about is spam filters now nobody wants to receive spam emails and NLP is here to help fight spam and reduce the numbers and emails that you receive so it's not perfect yet and I'm sure we still all receive some spam emails but can you imagine how many you deck without natural language processing next I want to talk about is bridging language barriers when you come across a faes phrase or in an even entire website in another language NLP you can basically hope you translate it into something that you can investment decisions so nobody has the power to help you make decisions for financial investing you can read large amounts of text such as news articles press releases etc you can pull in the key exam in key data that you you'll need in order to make your buy hold or sell decisions for example um let you know if there's an acquisition that's planned or has happened which has large implications on the value of your investments they met into insights this is my favorite so humans simply can't read everything that's available to us and we have a lot of data available to us so enoki hopes that summarize this data that we have and help us helps us to call meaningful information out so an example of this is a computer reading through thousands of customer reviews to identify any issues or conducting sentiment analysis so I personally have an example of when I use NLP in my line of work basically we conducted a very in-depth survey we concluded about two to three thousand survey respondents and we had several questions that were open-ended which resulted in us receiving unstructured text in response so we have let's say for the for this example we had 2000 paragraphs of unstructured text one thing that I was able to do was create an algorithm that can parse or categorize this text into about six categories using keywords for each category this ended up saving a lot of time and it actually turned out to be really accurate so thank you for watching the amazing things that NLP can do and please subscribe to the YouTube channel to be notified of all future content thank you",
            "videoid": "xj5ZvWgPqKM",
            "viewCount": "3966"
        },
        "zi16nl82AMA": {
            "caption_exist": "T",
            "channel_id": "UCfzlCWGWYyIQ0aLC5w48gBQ",
            "channel_title": "sentdex",
            "concepts": [
                [
                    "list",
                    18
                ],
                [
                    "algorithm",
                    6
                ],
                [
                    "shuffle",
                    3
                ],
                [
                    "mean",
                    2
                ],
                [
                    "language",
                    2
                ],
                [
                    "common words",
                    2
                ],
                [
                    "natural language processing",
                    1
                ],
                [
                    "training data",
                    1
                ],
                [
                    "degree",
                    1
                ],
                [
                    "bag",
                    1
                ]
            ],
            "description": "Now that we understand some of the basics of of natural language processing with the Python NLTK module, we're ready to try out text classification. This is where we attempt to identify a body of text with some sort of label. \n\nTo start, we're going to use some sort of binary label. Examples of this could be identifying text as spam or not, or, like what we'll be doing, positive sentiment or negative sentiment. \n\nPlaylist link: https://www.youtube.com/watch?v=FLZvOKSCkxY&list=PLQVvvaa0QuDf2JswnfiGkliBInZnIC4HL&index=1\n\nsample code: http://pythonprogramming.net\nhttp://hkinsley.com\nhttps://twitter.com/sentdex\nhttp://sentdex.com\nhttp://seaofbtc.com",
            "dislikeCount": "8",
            "duration": "PT11M41S",
            "likeCount": "368",
            "published_time": "2015-05-12T13:18:21.000Z",
            "tags": [
                "Natural Language Toolkit (Software)",
                "Python (Programming Language)",
                "Natural Language Processing (Software Genre)",
                "Document Classification (Software Genre)",
                "text classification",
                "sentiment"
            ],
            "thumbnail": "https://i.ytimg.com/vi/zi16nl82AMA/hqdefault.jpg",
            "title": "Text Classification - Natural Language Processing With Python and NLTK p.11",
            "transcript": "  what is going on everybody welcome to the 11th NLT k with python for natural language processing tutorial video in this video we're going to actually start creating our own algorithm or text classifier as what it's called now we're going to be doing a text classifier for sentiment analysis but you can also use text classifiers for all kinds of stuff maybe you're trying to classify the text as stocks writing or politics writing or whatever you want economics or anything or another form of text classifier might be just discerning whether or not something is spam or a legitimate email that kind of thing so our text classifier is going to classify something as either a positive connotation or a negative connotation basically or meaning or sentiment as a form of opinion mining basically so let's go ahead and get started and show how we might actually do this because this methodology our methodology here can be applied to any categories as long as they're tagged and there are two categories so we just have two choices there's not a degree of choices it's just one or the other so this is spam or it's not spam right when you have a spam filter you've got spam or not spam you don't have like a bunch of little folders that are like maybe spam highly likely spam definitely spam no usually it's just your inbox and spam okay same thing here we'll do sentiment analysis positive or negative now of course with sentiment analysis that's not necessarily the case you might have something that's slightly positive highly positive extremely positive and the same for negatives but anyway this is just an example but you can feel free to get your own list and your own tagged list even or create your own list and as long as it is a document with some words in it and it's labeled as either one of two labels you can use anything you want it could be spam it can be defiant you know whether or not something as a text message versus a you know official document or something like that it could be anything so anyway let's go ahead and get started work of course going to be using NLT kay what's going to import random and we're going to use that to shuffle up the data set that we have because right now it's very highly ordered it's all for the first thousand all positive for the second thousand that's no good and then finally from NLT KDOT corpus we're going to import the movie reviews so this is what I was just talking about it's a thousand positive and a thousand negative movie reviews so we can train against them and they're already labeled you know so that's the whole idea of training data now when you read the NLT K documentation you're going to see the following one-liner this is kind of a confusing one-liner if you ask me it's kind of a wasted one-liner because it's basically the same amount of lines if you didn't make it a silly one-liner but people are just in love with their one-liners so I will show it to you because this is how most people do it anyways so let's go so documents is going to equal and it's going to be a list and it's also going to be it it's going to be like a list of tuples basically in the tuple itself will be the words and we're going to call words as features so if you follow along any sort of machine learning tutorial including some of my own machine learning tutorials you'll know that you've got features and those features are what makes up you know the elements of something and we use those features to train based on their categories and or tags or whatever you want to call them so anyway we're going to have a list of tuples and the first L or the zeroth element in that tuple will be the words they're basically the presence or the non absence there it is of a word and then the second part of that tuple will be the category so this will be anyway the list of movie underscore reviews words for a file ID and this is why I hate one liners because file ID makes no sense at this point but we'll get there and category so again it's it's a list of tuples so you can even see this is a list of a tuple and then basically this is where or for the category in movie reviews categories so this is basically for category and positive or negative and then again for file ID in moon moving reviews dot file IDs for the specific category category okay so this is a one-liner we could put all this on one line we're just choosing not to excite would be absurd and the one-liner works basically if you wrote this yourself you would have you know documents documents equals an empty list then you would do this is the first part so it's asking you know for category movie reviews that's the first part then for file ID whoops let's get just this for file ID in the movie reviews : without the l and then we would have the tuple that the tuple equals that and we would do something like you know documents dot append we would append this tuple of course so anyway this is actually easier to read I don't know why ruin insists on one liners sometimes they make a whole lot of sense they really do make everything nice and compact but this one doesn't so why but documents equals that good to go next what we want to do is we're going to do a random shuffle of those documents because we're going to train test and when you train a test you test on separate data that you did not train against because if you train and test on the same data that's extreme bias so we want to shuffle the documents so now let's go ahead and just print documents and oops the first if element in the documents just to see where we are to make sure not doing anything too crazy at this point but it should be documents one will be a list of words and yes there it is and then whether or not it's positive or negative so this is the list of all the words in this document starting with a Denzel Washington is among the many available and then finally moving to the end this is all the features and then the rating here is positive so this will be part of our bag of words so to speak now what we're going to end up having to do is take all of these words though literally take every word in every review and compile them and then basically we do is we'll take that list of words and we'll find the most popular words used and then we take of those most popular words which one appear in positive text and which ones negative text and then we simply just search for those words and whichever one it has more negative words or more positive words that's how we classify it so hence the word naive for Bayes algorithm a document one and so now what we're going to do is now that we have all the documents loaded we don't need to print documents one anymore that's a waste so I just wanted to show it to you and now we're going to say all underscore words is going to be an empty list and then we're going to say for W in movie underscore reviews words what do we want to do w first of all is going to equal W lower we just want to make sure everything's normalized so we're not gonna care about casing in words so we're just going to convert everything to a lowercase and then we're just going to say all and in fact we can just do this all words all words append W lower so now this adds all the words to this list and then later on I sort of misspoke the documents is basically what we'll do to create training and testing sets so we're actually not going to add all the words from documents this this element here I suppose basically we have the reviews and then all of the words so this is just just words from all the movie reviews so this is actually how we're going to compile this massive list of all words then later on we use the features of documents which are words to compare so anyway so moving on now what we can do is we can actually find out really quickly who are the most common things so first of all all words is a list right now so let's convert that to an NLT K frequency distribution so we can do all underscore words um equals NLT k3 quints rec dist of all words like this and then we can do this print all underscore words dot most common and let's do the 15 most common words and then while we're doing that waiting for that well do we can also find out like how many words are there so this is still kind of coming up now you just pull oh there we go so these are the top 15 most common words so as you probably recall with ml TK you have a punctuation that is classified as being possibly words so you can change that if you want we're not going to bother worrying too much about that but if you wanted to you can and then put a few videos later we're going to talk about how to improve the algorithm if we wanted to but anyway you've got commas these period a and of like these are basically all words that there's nothing in this list that actually matters to positive or negative at all so just keep that in mind now all words eventually we're going to kind of shrink all words but all words is actually a huge amount of words I mean we I think we have like you know 50,000 or maybe even more so don't worry that the top 15 are kind of useless words that's just because that's the English language but just showing you that how to use this frequency distribution mostly but you can also do something like this print and then all underscore words and then like stupid okay so let's see how many times is the word stupid pop up in our entire corpus of movie reviews so we'll kind of wait for this to show up I just pull it up here and this will mark the end okay so here we have two hundred fifty three times the word stupid appears in these reviews so keep in mind that we've got two thousand movie reviews total two hundred and fifty three times the word stupid appears so I mean obviously someone might write a movie review and use the word stupid many times that's totally possible but you can see that quite a large number almost 1/4 possibly movie reviews have the word stupid in them which is understandable because we do have you a thousand negative reviews anyway are not a quarter my bad and eight because we have two thousand reviews anyways um that's it for now we'll continue building on this in the next video now that we have all words and then we have all the words per document here and their category now we can actually start to train so we can train based on the words in each document and the category has we can use the naivebayes algorithm to say okay well these words are generally positive these words are generally negative so we can do that and then we'll end up testing it and see how good this algorithm actually is it's actually pretty basic algorithm anyways uh that's it for now if you guys have any questions or comments up to this point please feel free to leave them below otherwise as always thanks for watching things for all the spoiler subscriptions until next time",
            "videoid": "zi16nl82AMA",
            "viewCount": "74148"
        }
    }
}